This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
.claude/
  settings.local.json
.github/
  workflows/
    ci.yml
backend/
  alembic/
    versions/
      cd3912ae6d70_initial_migration_job_and_file_tables.py
    env.py
    README
    script.py.mako
  app/
    azure_adapter.py
    currency.py
    db.py
    main.py
    models.py
  langgraph_nodes/
    base.py
    check_currency.py
    convert.py
    excel.py
    extract.py
    pipeline.py
    upload.py
  tests/
    fixtures/
      README.md
    __init__.py
    test_azure_adapter.py
    test_currency.py
    test_db_models.py
    test_health.py
    test_pipeline.py
    test_placeholder.py
    test_sse_progress.py
  .gitignore
  alembic.ini
  poetry.toml
  pyproject.toml
  README.md
frontend/
  public/
    vite.svg
  src/
    assets/
      react.svg
    components/
      ProgressBar.test.tsx
      ProgressBar.tsx
      UploadArea.test.tsx
      UploadArea.tsx
    hooks/
      useSse.test.ts
      useSse.ts
    App.css
    App.test.tsx
    App.tsx
    index.css
    main.tsx
    setupTests.ts
    vite-env.d.ts
  .gitignore
  eslint.config.js
  index.html
  jest.config.js
  package.json
  postcss.config.js
  README.md
  tailwind.config.js
  tsconfig.app.json
  tsconfig.json
  tsconfig.node.json
  vite.config.ts
.gitignore
.pre-commit-config.yaml
CLAUDE.md
LICENSE
prompt_plan.md
README.md
server.py
spec.md
todo.md
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path=".claude/settings.local.json">
{
  "permissions": {
    "allow": [
      "Bash(pytest:*)",
      "Bash(python:*)",
      "Bash(ruff check:*)",
      "Bash(black:*)",
      "Bash(find:*)",
      "Bash(timeout:*)",
      "Bash(rm:*)",
      "Bash(mkdir:*)",
      "Bash(git add:*)",
      "Bash(git commit:*)",
      "Bash(touch:*)",
      "Bash(ls:*)",
      "Bash(npm test:*)",
      "Bash(npm install)",
      "Bash(npm install:*)",
      "Bash(npm run lint)",
      "Bash(poetry install:*)",
      "Bash(git checkout:*)",
      "Bash(git reset:*)",
      "Bash(poetry lock:*)",
      "Bash(pip install:*)",
      "Bash(mv:*)",
      "Bash(cp:*)",
      "Bash(poetry run pytest:*)",
      "Bash(poetry run ruff:*)",
      "Bash(poetry run black:*)",
      "Bash(poetry run isort:*)",
      "Bash(git stash push:*)",
      "Bash(git stash:*)",
      "Bash(git mv:*)",
      "Bash(true)",
      "Bash(git rm:*)",
      "Bash(poetry run:*)",
      "Bash(npm create:*)",
      "Bash(npx tailwindcss init:*)",
      "Bash(poetry show:*)",
      "Bash(poetry env info:*)",
      "Bash(poetry:*)",
      "Bash(source:*)",
      "Bash(brew install:*)",
      "mcp__ide__getDiagnostics",
      "Bash(chmod:*)",
      "Bash(git branch:*)",
      "Bash(uv run pytest:*)"
    ],
    "deny": []
  }
}
</file>

<file path="backend/alembic/versions/cd3912ae6d70_initial_migration_job_and_file_tables.py">
"""Initial migration: Job and File tables

Revision ID: cd3912ae6d70
Revises:
Create Date: 2025-07-13 18:39:51.539338

"""
from collections.abc import Sequence

import sqlalchemy as sa

from alembic import op

# revision identifiers, used by Alembic.
revision: str = "cd3912ae6d70"
down_revision: str | Sequence[str] | None = None
branch_labels: str | Sequence[str] | None = None
depends_on: str | Sequence[str] | None = None


def upgrade() -> None:
    """Upgrade schema."""
    # ### commands auto generated by Alembic - please adjust! ###
    op.create_table(
        "jobs",
        sa.Column("job_id", sa.String(), nullable=False),
        sa.Column("status", sa.String(), nullable=False),
        sa.Column("processed", sa.Integer(), nullable=True),
        sa.Column("total", sa.Integer(), nullable=False),
        sa.Column("created_at", sa.DateTime(), nullable=True),
        sa.Column("updated_at", sa.DateTime(), nullable=True),
        sa.PrimaryKeyConstraint("job_id"),
    )
    op.create_table(
        "files",
        sa.Column("id", sa.Integer(), autoincrement=True, nullable=False),
        sa.Column("job_id", sa.String(), nullable=False),
        sa.Column("filename", sa.String(), nullable=False),
        sa.Column("status", sa.String(), nullable=False),
        sa.Column("original_currency", sa.String(), nullable=True),
        sa.Column("target_currency", sa.String(), nullable=True),
        sa.Column("error_message", sa.String(), nullable=True),
        sa.ForeignKeyConstraint(
            ["job_id"],
            ["jobs.job_id"],
        ),
        sa.PrimaryKeyConstraint("id"),
    )
    # ### end Alembic commands ###


def downgrade() -> None:
    """Downgrade schema."""
    # ### commands auto generated by Alembic - please adjust! ###
    op.drop_table("files")
    op.drop_table("jobs")
    # ### end Alembic commands ###
</file>

<file path="backend/alembic/env.py">
import os
from logging.config import fileConfig

from sqlalchemy import engine_from_config, pool

from alembic import context

# Import our models
from app.models import Base

# this is the Alembic Config object, which provides
# access to the values within the .ini file in use.
config = context.config

# Interpret the config file for Python logging.
# This line sets up loggers basically.
if config.config_file_name is not None:
    fileConfig(config.config_file_name)

# add your model's MetaData object here
# for 'autogenerate' support
target_metadata = Base.metadata

# Set the database URL from environment if not in config
database_url = os.getenv("DATABASE_URL", "sqlite:///./invoice.db")
config.set_main_option("sqlalchemy.url", database_url)

# other values from the config, defined by the needs of env.py,
# can be acquired:
# my_important_option = config.get_main_option("my_important_option")
# ... etc.


def run_migrations_offline() -> None:
    """Run migrations in 'offline' mode.

    This configures the context with just a URL
    and not an Engine, though an Engine is acceptable
    here as well.  By skipping the Engine creation
    we don't even need a DBAPI to be available.

    Calls to context.execute() here emit the given string to the
    script output.

    """
    url = config.get_main_option("sqlalchemy.url")
    context.configure(
        url=url,
        target_metadata=target_metadata,
        literal_binds=True,
        dialect_opts={"paramstyle": "named"},
    )

    with context.begin_transaction():
        context.run_migrations()


def run_migrations_online() -> None:
    """Run migrations in 'online' mode.

    In this scenario we need to create an Engine
    and associate a connection with the context.

    """
    connectable = engine_from_config(
        config.get_section(config.config_ini_section, {}),
        prefix="sqlalchemy.",
        poolclass=pool.NullPool,
    )

    with connectable.connect() as connection:
        context.configure(connection=connection, target_metadata=target_metadata)

        with context.begin_transaction():
            context.run_migrations()


if context.is_offline_mode():
    run_migrations_offline()
else:
    run_migrations_online()
</file>

<file path="backend/alembic/README">
Generic single-database configuration.
</file>

<file path="backend/alembic/script.py.mako">
"""${message}

Revision ID: ${up_revision}
Revises: ${down_revision | comma,n}
Create Date: ${create_date}

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa
${imports if imports else ""}

# revision identifiers, used by Alembic.
revision: str = ${repr(up_revision)}
down_revision: Union[str, Sequence[str], None] = ${repr(down_revision)}
branch_labels: Union[str, Sequence[str], None] = ${repr(branch_labels)}
depends_on: Union[str, Sequence[str], None] = ${repr(depends_on)}


def upgrade() -> None:
    """Upgrade schema."""
    ${upgrades if upgrades else "pass"}


def downgrade() -> None:
    """Downgrade schema."""
    ${downgrades if downgrades else "pass"}
</file>

<file path="backend/app/azure_adapter.py">
"""Azure Document Intelligence adapter for invoice extraction."""

import os
from dataclasses import dataclass
from pathlib import Path

from azure.ai.documentintelligence.aio import DocumentIntelligenceClient
from azure.core.credentials import AzureKeyCredential
from dotenv import load_dotenv

load_dotenv()


# Robust internal dataclasses (preserving original structure)
@dataclass
class DefaultContent:
    """Text content with optional confidence."""

    content: str


@dataclass
class ValueCurrency:
    """Monetary value with currency code."""

    amount: float
    currency_code: str


@dataclass
class InvoiceTotal:
    """Invoice total with structured and text content."""

    value_currency: ValueCurrency | None
    content: str


@dataclass
class InvoiceData:
    """Complete invoice data structure matching Azure response."""

    InvoiceDate: DefaultContent | None
    InvoiceId: DefaultContent | None
    InvoiceTotal: InvoiceTotal | None
    VendorName: DefaultContent | None
    VendorAddressRecipient: DefaultContent | None


# Simple output format for web API
@dataclass
class SimpleInvoiceData:
    """Simplified invoice data structure for API responses."""

    date: str | None = None
    total: float | None = None
    currency: str | None = None
    vendor: str | None = None
    filename: str | None = None


async def extract_invoice(path: str) -> InvoiceData | None:
    """Extract invoice data using Azure Document Intelligence.

    Args:
        path: Path to the invoice file (PDF, JPEG, PNG)

    Returns:
        InvoiceData: Extracted invoice data or None if extraction fails
    """
    # Get credentials from environment
    endpoint = os.getenv("AZURE_DOCUMENT_INTELLIGENCE_ENDPOINT")
    api_key = os.getenv("AZURE_DOCUMENT_INTELLIGENCE_API_KEY")

    if not endpoint or not api_key:
        print("Error: Missing Azure Document Intelligence credentials in .env file")
        print("Required variables:")
        print("- AZURE_DOCUMENT_INTELLIGENCE_ENDPOINT")
        print("- AZURE_DOCUMENT_INTELLIGENCE_API_KEY")
        return None

    # Check if file exists
    file_path = Path(path)
    if not file_path.exists():
        print(f"Error: File '{path}' not found")
        return None

    try:
        # Initialize async client
        async with DocumentIntelligenceClient(endpoint=endpoint, credential=AzureKeyCredential(api_key)) as client:
            # Read file
            with open(path, "rb") as file:
                file_data = file.read()

            print(f"Processing {path} with Azure Document Intelligence...")

            # Analyze document using prebuilt invoice model
            poller = await client.begin_analyze_document("prebuilt-invoice", file_data, content_type="application/pdf")

            result = await poller.result()

            # Extract data from Azure response
            invoice_data = _extract_from_azure_response(result)

            return invoice_data

    except Exception as e:
        print(f"Error processing document: {e}")
        return None


def _extract_from_azure_response(azure_result) -> InvoiceData | None:
    """Extract structured data from Azure Document Intelligence response."""
    if not azure_result.documents:
        return None

    invoice = azure_result.documents[0]
    fields = invoice.fields

    # Extract content from Azure field objects with proper structure
    invoice_date = None
    if "InvoiceDate" in fields and fields["InvoiceDate"]:
        content = getattr(fields["InvoiceDate"], "content", "")
        if content:
            invoice_date = DefaultContent(content=content)

    invoice_id = None
    if "InvoiceId" in fields and fields["InvoiceId"]:
        content = getattr(fields["InvoiceId"], "content", "")
        if content:
            invoice_id = DefaultContent(content=content)

    invoice_total = None
    if "InvoiceTotal" in fields and fields["InvoiceTotal"]:
        total_field = fields["InvoiceTotal"]
        content = getattr(total_field, "content", "")

        value_currency = None
        if hasattr(total_field, "value_currency") and total_field.value_currency:
            amount = getattr(total_field.value_currency, "amount", 0.0)
            currency_code = getattr(total_field.value_currency, "currency_code", "")
            if amount or currency_code:
                value_currency = ValueCurrency(amount=amount, currency_code=currency_code)

        if content or value_currency:
            invoice_total = InvoiceTotal(value_currency=value_currency, content=content)

    vendor_name = None
    if "VendorName" in fields and fields["VendorName"]:
        content = getattr(fields["VendorName"], "content", "")
        if content:
            vendor_name = DefaultContent(content=content)

    vendor_address = None
    if "VendorAddressRecipient" in fields and fields["VendorAddressRecipient"]:
        content = getattr(fields["VendorAddressRecipient"], "content", "")
        if content:
            vendor_address = DefaultContent(content=content)

    return InvoiceData(
        InvoiceDate=invoice_date,
        InvoiceId=invoice_id,
        InvoiceTotal=invoice_total,
        VendorName=vendor_name,
        VendorAddressRecipient=vendor_address,
    )


# Conversion helpers between formats
def to_simple_format(invoice: InvoiceData, filename: str) -> SimpleInvoiceData:
    """Convert full InvoiceData to simplified format for API responses.

    Args:
        invoice: Full structured invoice data
        filename: Original filename

    Returns:
        SimpleInvoiceData: Simplified format for JSON serialization
    """
    # Extract date
    date = None
    if invoice.InvoiceDate and invoice.InvoiceDate.content:
        date = invoice.InvoiceDate.content

    # Extract total and currency
    total = None
    currency = None
    if invoice.InvoiceTotal and invoice.InvoiceTotal.value_currency:
        total = invoice.InvoiceTotal.value_currency.amount
        currency = invoice.InvoiceTotal.value_currency.currency_code

    # Extract vendor (prefer VendorName, fallback to VendorAddressRecipient)
    vendor = None
    if invoice.VendorName and invoice.VendorName.content:
        vendor = invoice.VendorName.content
    elif invoice.VendorAddressRecipient and invoice.VendorAddressRecipient.content:
        vendor = invoice.VendorAddressRecipient.content

    return SimpleInvoiceData(date=date, total=total, currency=currency, vendor=vendor, filename=filename)


def from_azure_response(azure_result) -> InvoiceData | None:
    """Create InvoiceData from Azure Document Intelligence response.

    This is an alias for _extract_from_azure_response for backward compatibility.
    """
    return _extract_from_azure_response(azure_result)


# Convenience functions for different use cases
async def extract_invoice_simple(path: str) -> SimpleInvoiceData | None:
    """Extract invoice data in simplified format for web API.

    Args:
        path: Path to the invoice file

    Returns:
        SimpleInvoiceData: Simplified format or None if extraction fails
    """
    full_data = await extract_invoice(path)
    if full_data:
        filename = Path(path).name
        return to_simple_format(full_data, filename)
    return None


# Synchronous wrapper for backward compatibility
def extract_invoice_sync(path: str) -> InvoiceData | None:
    """Synchronous wrapper for invoice extraction."""
    import asyncio

    try:
        loop = asyncio.get_event_loop()
    except RuntimeError:
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)

    return loop.run_until_complete(extract_invoice(path))
</file>

<file path="backend/app/currency.py">
"""Currency conversion using Frankfurter API with circuit breaker."""

from decimal import Decimal

import httpx
from dateutil import parser as date_parser

# Module-level circuit breaker state
_failure_count = 0
_max_failures = 2  # Block on 3rd attempt (after 2 failures)


class FrankfurterDown(Exception):
    """Exception raised when Frankfurter API is down after max failures."""

    pass


def _normalize_date(date_str: str) -> str:
    """Normalize a date string to YYYY-MM-DD format."""
    try:
        # Parse the date string using dateutil which handles many formats
        parsed_date = date_parser.parse(date_str)
        # Return in YYYY-MM-DD format
        return parsed_date.strftime("%Y-%m-%d")
    except (ValueError, TypeError) as e:
        raise ValueError(f"Invalid date format: {date_str}") from e


async def get_rate(date: str, from_: str, to_: str) -> Decimal:
    """Get exchange rate from Frankfurter API with circuit breaker.

    Args:
        date: Date in any common format (YYYY-MM-DD, MM/DD/YYYY, DD-MM-YYYY, etc.)
        from_: Source currency code (e.g., "USD")
        to_: Target currency code (e.g., "ILS")

    Returns:
        Decimal: Exchange rate from source to target currency

    Raises:
        ValueError: If the date format is invalid
        FrankfurterDown: If API is down after 3 consecutive failures
        Exception: If the API request fails
    """
    global _failure_count

    # Check circuit breaker
    if _failure_count >= _max_failures:
        raise FrankfurterDown(f"Frankfurter API is down after {_max_failures + 1} consecutive failures")

    # Normalize the date to YYYY-MM-DD format
    norm_date = _normalize_date(date)

    # Normalize currency codes to uppercase
    from_currency = from_.upper()
    to_currency = to_.upper()

    # Make API request with httpx
    url = f"https://api.frankfurter.app/{norm_date}?from={from_currency}&to={to_currency}"

    try:
        async with httpx.AsyncClient(timeout=2.0) as client:
            response = await client.get(url)

        if response.status_code != 200:
            _failure_count += 1
            raise Exception(
                f"[get_rate] Failed to fetch exchange rate for {norm_date} ({from_currency} to {to_currency}). "
                f"Code {response.status_code} {response.reason_phrase}\n{response.text}"
            )

        data = response.json()

        # Extract the exchange rate from the response
        if to_currency not in data.get("rates", {}):
            _failure_count += 1
            raise Exception(f"[get_rate] Currency {to_currency} not found in response for {norm_date}")

        rate = data["rates"][to_currency]

        # Reset failure count only on complete success
        _failure_count = 0

        return Decimal(str(rate))

    except httpx.TimeoutException as e:
        _failure_count += 1
        raise Exception(f"[get_rate] Request timeout for date {norm_date}") from e
    except httpx.RequestError as e:
        _failure_count += 1
        raise Exception(f"[get_rate] Network error for date {norm_date}: {e}") from e
    except (ValueError, KeyError) as e:
        _failure_count += 1
        raise Exception(f"[get_rate] Invalid JSON response for date {norm_date}") from e


def reset_circuit_breaker() -> None:
    """Reset the circuit breaker failure count (for testing)."""
    global _failure_count
    _failure_count = 0


def get_failure_count() -> int:
    """Get current failure count (for testing)."""
    return _failure_count
</file>

<file path="backend/app/db.py">
"""Database session management."""

import os

from sqlalchemy import create_engine
from sqlalchemy.orm import Session, sessionmaker

from .models import Base

# Get database URL from environment, default to SQLite
DATABASE_URL = os.getenv("DATABASE_URL", "sqlite:///./invoice.db")

# Create engine
engine = create_engine(
    DATABASE_URL, connect_args={"check_same_thread": False} if DATABASE_URL.startswith("sqlite") else {}
)

# Create session factory
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)


def get_session() -> Session:
    """Get a database session."""
    session = SessionLocal()
    try:
        return session
    except Exception:
        session.close()
        raise


def create_tables():
    """Create all tables."""
    Base.metadata.create_all(bind=engine)
</file>

<file path="backend/app/models.py">
"""SQLAlchemy models for invoice processing."""

from datetime import UTC, datetime

from sqlalchemy import DateTime, ForeignKey, Integer, String
from sqlalchemy.orm import DeclarativeBase, Mapped, mapped_column, relationship


class Base(DeclarativeBase):
    """Base class for all database models."""

    pass


class Job(Base):
    """Job model for tracking batch invoice processing jobs."""

    __tablename__ = "jobs"

    job_id: Mapped[str] = mapped_column(String, primary_key=True)
    status: Mapped[str] = mapped_column(String, nullable=False)
    processed: Mapped[int] = mapped_column(Integer, default=0)
    total: Mapped[int] = mapped_column(Integer, nullable=False)
    created_at: Mapped[datetime] = mapped_column(DateTime, default=lambda: datetime.now(UTC))
    updated_at: Mapped[datetime] = mapped_column(
        DateTime, default=lambda: datetime.now(UTC), onupdate=lambda: datetime.now(UTC)
    )

    # Relationship to files
    files: Mapped[list["File"]] = relationship("File", back_populates="job")


class File(Base):
    """File model for tracking individual invoice files."""

    __tablename__ = "files"

    id: Mapped[int] = mapped_column(Integer, primary_key=True, autoincrement=True)
    job_id: Mapped[str] = mapped_column(String, ForeignKey("jobs.job_id"), nullable=False)
    filename: Mapped[str] = mapped_column(String, nullable=False)
    status: Mapped[str] = mapped_column(String, nullable=False)
    original_currency: Mapped[str | None] = mapped_column(String)
    target_currency: Mapped[str | None] = mapped_column(String)
    error_message: Mapped[str | None] = mapped_column(String)

    # Relationship to job
    job: Mapped["Job"] = relationship("Job", back_populates="files")
</file>

<file path="backend/langgraph_nodes/base.py">
"""Base node for LangGraph pipeline."""


async def run(input: dict) -> dict:
    """Base node that passes input through unchanged.

    Args:
        input: Input dictionary containing pipeline state

    Returns:
        dict: Same input dictionary unchanged
    """
    return input
</file>

<file path="backend/langgraph_nodes/check_currency.py">
"""Check currency node for currency validation."""


async def run(input: dict) -> dict:
    """Check currency node that validates currency information.

    Args:
        input: Input dictionary containing pipeline state

    Returns:
        dict: Same input dictionary unchanged
    """
    return input
</file>

<file path="backend/langgraph_nodes/convert.py">
"""Convert node for currency conversion."""


async def run(input: dict) -> dict:
    """Convert node that performs currency conversion.

    Args:
        input: Input dictionary containing pipeline state

    Returns:
        dict: Same input dictionary unchanged
    """
    return input
</file>

<file path="backend/langgraph_nodes/excel.py">
"""Excel node for report generation."""


async def run(input: dict) -> dict:
    """Excel node that generates Excel reports.

    Args:
        input: Input dictionary containing pipeline state

    Returns:
        dict: Same input dictionary unchanged
    """
    return input
</file>

<file path="backend/langgraph_nodes/extract.py">
"""Extract node for invoice data extraction."""


async def run(input: dict) -> dict:
    """Extract node that extracts data from invoices.

    Args:
        input: Input dictionary containing pipeline state

    Returns:
        dict: Same input dictionary unchanged
    """
    return input
</file>

<file path="backend/langgraph_nodes/pipeline.py">
"""LangGraph pipeline for invoice processing workflow."""

from langgraph.graph import Graph

from . import check_currency, convert, excel, extract, upload


def create_pipeline() -> Graph:
    """Create the invoice processing pipeline graph.

    Returns:
        Graph: LangGraph instance with nodes linked in processing order
    """
    # Create a new graph
    graph = Graph()

    # Add nodes to the graph
    graph.add_node("upload", upload.run)
    graph.add_node("extract", extract.run)
    graph.add_node("check_currency", check_currency.run)
    graph.add_node("convert", convert.run)
    graph.add_node("excel", excel.run)

    # Link nodes in the processing order
    graph.add_edge("upload", "extract")
    graph.add_edge("extract", "check_currency")
    graph.add_edge("check_currency", "convert")
    graph.add_edge("convert", "excel")

    # Set entry point
    graph.set_entry_point("upload")
    graph.set_finish_point("excel")

    return graph


def get_compiled_pipeline():
    """Get a compiled pipeline ready for execution.

    Returns:
        Compiled LangGraph pipeline
    """
    pipeline = create_pipeline()
    return pipeline.compile()
</file>

<file path="backend/langgraph_nodes/upload.py">
"""Upload node for file processing."""


async def run(input: dict) -> dict:
    """Upload node that processes file uploads.

    Args:
        input: Input dictionary containing pipeline state

    Returns:
        dict: Same input dictionary unchanged
    """
    return input
</file>

<file path="backend/tests/fixtures/README.md">
# Test Fixtures

This directory contains test fixtures for Azure Document Intelligence testing.

## Adding Sample Invoices

To add a sample invoice for testing:

1. Place the PDF file in this directory
2. Update the test cases to reference the new file
3. Run tests to generate VCR cassettes

**Note:** Do not commit actual invoice files with sensitive information.
Use anonymized or dummy invoices only.
</file>

<file path="backend/tests/__init__.py">
"""Test package for invoice converter."""
</file>

<file path="backend/tests/test_azure_adapter.py">
"""Tests for Azure Document Intelligence adapter."""

import os
from unittest.mock import AsyncMock, Mock, patch

import pytest

from app.azure_adapter import (
    DefaultContent,
    InvoiceData,
    InvoiceTotal,
    SimpleInvoiceData,
    ValueCurrency,
    _extract_from_azure_response,
    extract_invoice,
    extract_invoice_simple,
    from_azure_response,
    to_simple_format,
)


class TestDataClasses:
    """Test cases for the data classes."""

    def test_invoice_data_creation(self):
        """Test InvoiceData creation with robust structure."""
        invoice_data = InvoiceData(
            InvoiceDate=DefaultContent("2025-01-15"),
            InvoiceId=DefaultContent("INV-12345"),
            InvoiceTotal=InvoiceTotal(
                value_currency=ValueCurrency(amount=914.50, currency_code="USD"), content="$914.50"
            ),
            VendorName=DefaultContent("Test Company Inc."),
            VendorAddressRecipient=DefaultContent("123 Main St"),
        )

        assert invoice_data.InvoiceDate.content == "2025-01-15"
        assert invoice_data.InvoiceId.content == "INV-12345"
        assert invoice_data.InvoiceTotal.value_currency.amount == 914.50
        assert invoice_data.InvoiceTotal.value_currency.currency_code == "USD"
        assert invoice_data.VendorName.content == "Test Company Inc."

    def test_simple_invoice_data_creation(self):
        """Test SimpleInvoiceData creation."""
        simple_data = SimpleInvoiceData(
            date="2025-01-15", total=914.50, currency="USD", vendor="Test Company Inc.", filename="test_invoice.pdf"
        )

        assert simple_data.date == "2025-01-15"
        assert simple_data.total == 914.50
        assert simple_data.currency == "USD"
        assert simple_data.vendor == "Test Company Inc."
        assert simple_data.filename == "test_invoice.pdf"

    def test_invoice_data_with_none_values(self):
        """Test InvoiceData with None values."""
        invoice_data = InvoiceData(
            InvoiceDate=None, InvoiceId=None, InvoiceTotal=None, VendorName=None, VendorAddressRecipient=None
        )

        assert invoice_data.InvoiceDate is None
        assert invoice_data.InvoiceId is None
        assert invoice_data.InvoiceTotal is None
        assert invoice_data.VendorName is None
        assert invoice_data.VendorAddressRecipient is None


class TestExtractFromAzureResponse:
    """Test cases for Azure response extraction."""

    def test_extract_success(self):
        """Test successful extraction from Azure response."""
        # Mock Azure response structure
        mock_date_field = Mock()
        mock_date_field.content = "2025-01-15"

        mock_total_field = Mock()
        mock_value_currency = Mock()
        mock_value_currency.amount = 914.50
        mock_value_currency.currency_code = "USD"
        mock_total_field.value_currency = mock_value_currency
        mock_total_field.content = "$914.50"

        mock_vendor_field = Mock()
        mock_vendor_field.content = "Test Company Inc."

        mock_fields = {
            "InvoiceDate": mock_date_field,
            "InvoiceTotal": mock_total_field,
            "VendorName": mock_vendor_field,
        }

        mock_document = Mock()
        mock_document.fields = mock_fields

        mock_result = Mock()
        mock_result.documents = [mock_document]

        # Test the function
        result = _extract_from_azure_response(mock_result)

        assert result is not None
        assert result.InvoiceDate.content == "2025-01-15"
        assert result.InvoiceTotal.value_currency.amount == 914.50
        assert result.InvoiceTotal.value_currency.currency_code == "USD"
        assert result.InvoiceTotal.content == "$914.50"
        assert result.VendorName.content == "Test Company Inc."

    def test_extract_no_documents(self):
        """Test handling when no documents in response."""
        mock_result = Mock()
        mock_result.documents = []

        result = _extract_from_azure_response(mock_result)
        assert result is None

    def test_extract_missing_fields(self):
        """Test handling of missing fields."""
        mock_fields = {}  # Empty fields

        mock_document = Mock()
        mock_document.fields = mock_fields

        mock_result = Mock()
        mock_result.documents = [mock_document]

        result = _extract_from_azure_response(mock_result)

        assert result is not None
        assert result.InvoiceDate is None
        assert result.InvoiceId is None
        assert result.InvoiceTotal is None
        assert result.VendorName is None
        assert result.VendorAddressRecipient is None

    def test_extract_vendor_fallback(self):
        """Test vendor extraction fallback to VendorAddressRecipient."""
        mock_address_field = Mock()
        mock_address_field.content = "ABC Corp, 123 Main St"

        mock_fields = {
            "VendorAddressRecipient": mock_address_field,
        }

        mock_document = Mock()
        mock_document.fields = mock_fields

        mock_result = Mock()
        mock_result.documents = [mock_document]

        result = _extract_from_azure_response(mock_result)

        assert result is not None
        assert result.VendorAddressRecipient.content == "ABC Corp, 123 Main St"
        assert result.VendorName is None


class TestConversionHelpers:
    """Test cases for format conversion helpers."""

    def test_to_simple_format_complete(self):
        """Test conversion from full to simple format with all fields."""
        full_data = InvoiceData(
            InvoiceDate=DefaultContent("2025-01-15"),
            InvoiceId=DefaultContent("INV-12345"),
            InvoiceTotal=InvoiceTotal(
                value_currency=ValueCurrency(amount=914.50, currency_code="USD"), content="$914.50"
            ),
            VendorName=DefaultContent("Test Company Inc."),
            VendorAddressRecipient=DefaultContent("123 Main St"),
        )

        result = to_simple_format(full_data, "test_invoice.pdf")

        assert result.date == "2025-01-15"
        assert result.total == 914.50
        assert result.currency == "USD"
        assert result.vendor == "Test Company Inc."  # Should prefer VendorName
        assert result.filename == "test_invoice.pdf"

    def test_to_simple_format_with_fallbacks(self):
        """Test conversion with vendor fallback to address."""
        full_data = InvoiceData(
            InvoiceDate=None,
            InvoiceId=None,
            InvoiceTotal=None,
            VendorName=None,
            VendorAddressRecipient=DefaultContent("ABC Corp, 123 Main St"),
        )

        result = to_simple_format(full_data, "test_invoice.pdf")

        assert result.date is None
        assert result.total is None
        assert result.currency is None
        assert result.vendor == "ABC Corp, 123 Main St"  # Should use address as fallback
        assert result.filename == "test_invoice.pdf"

    def test_to_simple_format_empty(self):
        """Test conversion with empty data."""
        full_data = InvoiceData(
            InvoiceDate=None, InvoiceId=None, InvoiceTotal=None, VendorName=None, VendorAddressRecipient=None
        )

        result = to_simple_format(full_data, "test_invoice.pdf")

        assert result.date is None
        assert result.total is None
        assert result.currency is None
        assert result.vendor is None
        assert result.filename == "test_invoice.pdf"

    def test_from_azure_response_alias(self):
        """Test from_azure_response is working as alias."""
        mock_result = Mock()
        mock_result.documents = []

        result1 = from_azure_response(mock_result)
        result2 = _extract_from_azure_response(mock_result)

        assert result1 == result2  # Both should return None


class TestExtractInvoice:
    """Test cases for the extract_invoice function."""

    @pytest.mark.asyncio
    async def test_missing_credentials(self):
        """Test handling of missing credentials."""
        with patch.dict(os.environ, {}, clear=True):
            result = await extract_invoice("test.pdf")
            assert result is None

    @pytest.mark.asyncio
    async def test_file_not_found(self):
        """Test handling when file doesn't exist."""
        with patch.dict(
            os.environ,
            {
                "AZURE_DOCUMENT_INTELLIGENCE_ENDPOINT": "https://test.cognitiveservices.azure.com/",
                "AZURE_DOCUMENT_INTELLIGENCE_API_KEY": "test-api-key",
            },
        ):
            result = await extract_invoice("nonexistent.pdf")
            assert result is None

    @pytest.mark.asyncio
    @patch("app.azure_adapter.DocumentIntelligenceClient")
    @patch("builtins.open")
    @patch("app.azure_adapter.Path")
    async def test_extract_success(self, mock_path, mock_open, mock_client_class):
        """Test successful invoice extraction."""
        with patch.dict(
            os.environ,
            {
                "AZURE_DOCUMENT_INTELLIGENCE_ENDPOINT": "https://test.cognitiveservices.azure.com/",
                "AZURE_DOCUMENT_INTELLIGENCE_API_KEY": "test-api-key",
            },
        ):
            # Setup file mocks
            mock_path_instance = Mock()
            mock_path_instance.exists.return_value = True
            mock_path_instance.name = "test_invoice.pdf"
            mock_path.return_value = mock_path_instance

            mock_file = Mock()
            mock_file.read.return_value = b"mock pdf data"
            mock_open.return_value.__enter__.return_value = mock_file

            # Setup Azure client mock
            mock_client = AsyncMock()
            mock_client_class.return_value.__aenter__.return_value = mock_client

            # Mock the begin_analyze_document call
            mock_poller = AsyncMock()
            mock_client.begin_analyze_document.return_value = mock_poller

            # Mock successful Azure response
            mock_date_field = Mock()
            mock_date_field.content = "2025-01-15"

            mock_total_field = Mock()
            mock_value_currency = Mock()
            mock_value_currency.amount = 914.50
            mock_value_currency.currency_code = "USD"
            mock_total_field.value_currency = mock_value_currency
            mock_total_field.content = "$914.50"

            mock_vendor_field = Mock()
            mock_vendor_field.content = "Test Company Inc."

            mock_fields = {
                "InvoiceDate": mock_date_field,
                "InvoiceTotal": mock_total_field,
                "VendorName": mock_vendor_field,
            }

            mock_document = Mock()
            mock_document.fields = mock_fields

            mock_result = Mock()
            mock_result.documents = [mock_document]
            mock_poller.result.return_value = mock_result

            # Test the function
            result = await extract_invoice("test_invoice.pdf")

            assert result is not None
            assert result.InvoiceDate.content == "2025-01-15"
            assert result.InvoiceTotal.value_currency.amount == 914.50
            assert result.InvoiceTotal.value_currency.currency_code == "USD"
            assert result.VendorName.content == "Test Company Inc."

            # Verify API was called correctly
            mock_client.begin_analyze_document.assert_called_once_with(
                "prebuilt-invoice", b"mock pdf data", content_type="application/pdf"
            )

    @pytest.mark.asyncio
    @patch("app.azure_adapter.DocumentIntelligenceClient")
    @patch("builtins.open")
    @patch("app.azure_adapter.Path")
    async def test_azure_exception(self, mock_path, mock_open, mock_client_class):
        """Test handling of Azure client exceptions."""
        with patch.dict(
            os.environ,
            {
                "AZURE_DOCUMENT_INTELLIGENCE_ENDPOINT": "https://test.cognitiveservices.azure.com/",
                "AZURE_DOCUMENT_INTELLIGENCE_API_KEY": "test-api-key",
            },
        ):
            # Setup file mocks
            mock_path_instance = Mock()
            mock_path_instance.exists.return_value = True
            mock_path_instance.name = "test_invoice.pdf"
            mock_path.return_value = mock_path_instance

            mock_file = Mock()
            mock_file.read.return_value = b"mock pdf data"
            mock_open.return_value.__enter__.return_value = mock_file

            # Setup Azure client mock to raise exception
            mock_client = AsyncMock()
            mock_client.begin_analyze_document.side_effect = Exception("Azure API error")
            mock_client_class.return_value.__aenter__.return_value = mock_client

            result = await extract_invoice("test_invoice.pdf")
            assert result is None

    @pytest.mark.asyncio
    @patch("app.azure_adapter.extract_invoice")
    async def test_extract_invoice_simple_success(self, mock_extract):
        """Test successful simple extraction."""
        # Mock full extraction result
        full_data = InvoiceData(
            InvoiceDate=DefaultContent("2025-01-15"),
            InvoiceId=DefaultContent("INV-12345"),
            InvoiceTotal=InvoiceTotal(
                value_currency=ValueCurrency(amount=914.50, currency_code="USD"), content="$914.50"
            ),
            VendorName=DefaultContent("Test Company Inc."),
            VendorAddressRecipient=None,
        )
        mock_extract.return_value = full_data

        # Test simple extraction
        result = await extract_invoice_simple("test_invoice.pdf")

        assert result is not None
        assert result.date == "2025-01-15"
        assert result.total == 914.50
        assert result.currency == "USD"
        assert result.vendor == "Test Company Inc."
        assert result.filename == "test_invoice.pdf"

    @pytest.mark.asyncio
    @patch("app.azure_adapter.extract_invoice")
    async def test_extract_invoice_simple_failure(self, mock_extract):
        """Test simple extraction when full extraction fails."""
        mock_extract.return_value = None

        result = await extract_invoice_simple("test_invoice.pdf")
        assert result is None


# VCR cassette tests would go here when we have real Azure API calls to record
# @pytest.mark.vcr()
# async def test_extract_invoice_real_api():
#     """Test with real Azure API using VCR cassette."""
#     # This would be used with a real PDF file and Azure credentials
#     # The first run would record the HTTP interactions
#     # Subsequent runs would replay them
#     pass
</file>

<file path="backend/tests/test_currency.py">
"""Tests for the currency module."""

from decimal import Decimal

import httpx
import pytest
import respx

from app.currency import FrankfurterDown, _normalize_date, get_failure_count, get_rate, reset_circuit_breaker


class TestGetRate:
    """Test cases for the get_rate function."""

    def setup_method(self):
        """Reset circuit breaker before each test."""
        reset_circuit_breaker()

    @pytest.mark.asyncio
    async def test_happy_path(self):
        """Test successful exchange rate retrieval with mocked response 1.2."""
        expected_response = {"amount": 1.0, "base": "USD", "date": "2025-07-10", "rates": {"EUR": 1.2}}

        with respx.mock:
            respx.get("https://api.frankfurter.app/2025-07-10?from=USD&to=EUR").mock(
                return_value=httpx.Response(200, json=expected_response)
            )

            result = await get_rate("2025-07-10", "USD", "EUR")

            assert result == Decimal("1.2")
            assert get_failure_count() == 0

    @pytest.mark.asyncio
    async def test_currency_normalization(self):
        """Test that currency codes are normalized to uppercase."""
        expected_response = {"amount": 1.0, "base": "USD", "date": "2025-07-10", "rates": {"EUR": 1.5}}

        with respx.mock:
            respx.get("https://api.frankfurter.app/2025-07-10?from=USD&to=EUR").mock(
                return_value=httpx.Response(200, json=expected_response)
            )

            result = await get_rate("2025-07-10", "usd", "eur")

            assert result == Decimal("1.5")

    @pytest.mark.asyncio
    async def test_date_formats(self):
        """Test various date format inputs."""
        expected_response = {"amount": 1.0, "base": "USD", "date": "2025-07-10", "rates": {"EUR": 1.2}}

        date_formats = ["2025-07-10", "07/10/2025", "10 Jul 2025"]

        for date_format in date_formats:
            with respx.mock:
                respx.get("https://api.frankfurter.app/2025-07-10?from=USD&to=EUR").mock(
                    return_value=httpx.Response(200, json=expected_response)
                )

                result = await get_rate(date_format, "USD", "EUR")
                assert result == Decimal("1.2")

    @pytest.mark.asyncio
    async def test_failure_increments_counter(self):
        """Test that API failures increment the failure counter."""
        with respx.mock:
            respx.get("https://api.frankfurter.app/2025-07-10?from=USD&to=EUR").mock(
                return_value=httpx.Response(500, text="Internal Server Error")
            )

            with pytest.raises(Exception, match="Failed to fetch exchange rate"):
                await get_rate("2025-07-10", "USD", "EUR")

            assert get_failure_count() == 1

    @pytest.mark.asyncio
    async def test_timeout_increments_counter(self):
        """Test that timeouts increment the failure counter."""
        with respx.mock:
            respx.get("https://api.frankfurter.app/2025-07-10?from=USD&to=EUR").mock(
                side_effect=httpx.TimeoutException("Timeout")
            )

            with pytest.raises(Exception, match="Request timeout"):
                await get_rate("2025-07-10", "USD", "EUR")

            assert get_failure_count() == 1

    @pytest.mark.asyncio
    async def test_network_error_increments_counter(self):
        """Test that network errors increment the failure counter."""
        with respx.mock:
            respx.get("https://api.frankfurter.app/2025-07-10?from=USD&to=EUR").mock(
                side_effect=httpx.ConnectError("Connection failed")
            )

            with pytest.raises(Exception, match="Network error"):
                await get_rate("2025-07-10", "USD", "EUR")

            assert get_failure_count() == 1

    @pytest.mark.asyncio
    async def test_third_failure_raises_frankfurter_down(self):
        """Test that the third consecutive failure raises FrankfurterDown."""
        # First two failures
        for i in range(2):
            with respx.mock:
                respx.get("https://api.frankfurter.app/2025-07-10?from=USD&to=EUR").mock(
                    return_value=httpx.Response(500, text="Internal Server Error")
                )

                with pytest.raises(Exception, match="Failed to fetch exchange rate"):
                    await get_rate("2025-07-10", "USD", "EUR")

                assert get_failure_count() == i + 1

        # Third failure should raise FrankfurterDown without making a request
        with pytest.raises(FrankfurterDown, match="Frankfurter API is down after 3 consecutive failures"):
            await get_rate("2025-07-10", "USD", "EUR")

        assert get_failure_count() == 2  # Should stay at 2 since circuit breaker prevented the 3rd request

    @pytest.mark.asyncio
    async def test_success_resets_failure_count(self):
        """Test that successful requests reset the failure counter."""
        # First failure
        with respx.mock:
            respx.get("https://api.frankfurter.app/2025-07-10?from=USD&to=EUR").mock(
                return_value=httpx.Response(500, text="Internal Server Error")
            )

            with pytest.raises(Exception, match="Failed to fetch exchange rate"):
                await get_rate("2025-07-10", "USD", "EUR")

            assert get_failure_count() == 1

        # Successful request should reset counter
        expected_response = {"amount": 1.0, "base": "USD", "date": "2025-07-10", "rates": {"EUR": 1.2}}

        with respx.mock:
            respx.get("https://api.frankfurter.app/2025-07-10?from=USD&to=EUR").mock(
                return_value=httpx.Response(200, json=expected_response)
            )

            result = await get_rate("2025-07-10", "USD", "EUR")

            assert result == Decimal("1.2")
            assert get_failure_count() == 0

    @pytest.mark.asyncio
    async def test_invalid_json_response(self):
        """Test handling of invalid JSON responses."""
        with respx.mock:
            respx.get("https://api.frankfurter.app/2025-07-10?from=USD&to=EUR").mock(
                return_value=httpx.Response(200, text="Not JSON")
            )

            with pytest.raises(Exception, match="Invalid JSON response"):
                await get_rate("2025-07-10", "USD", "EUR")

            assert get_failure_count() == 1

    @pytest.mark.asyncio
    async def test_missing_currency_in_response(self):
        """Test handling when target currency is missing from response."""
        incomplete_response = {"amount": 1.0, "base": "USD", "date": "2025-07-10", "rates": {}}

        with respx.mock:
            respx.get("https://api.frankfurter.app/2025-07-10?from=USD&to=EUR").mock(
                return_value=httpx.Response(200, json=incomplete_response)
            )

            with pytest.raises(Exception, match="Currency EUR not found in response"):
                await get_rate("2025-07-10", "USD", "EUR")

            assert get_failure_count() == 1

    @pytest.mark.asyncio
    async def test_invalid_date_format(self):
        """Test that invalid dates raise ValueError."""
        with pytest.raises(ValueError, match="Invalid date format"):
            # This should fail in date normalization before making any request
            await get_rate("invalid-date-32-13-2025", "USD", "EUR")


class TestDateNormalization:
    """Test cases for date normalization helper function."""

    def test_normalize_date_formats(self):
        """Test various date format normalization."""
        assert _normalize_date("2025-07-10") == "2025-07-10"
        assert _normalize_date("07/10/2025") == "2025-07-10"
        assert _normalize_date("July 10, 2025") == "2025-07-10"
        assert _normalize_date("10 Jul 2025") == "2025-07-10"

    def test_normalize_date_invalid(self):
        """Test that invalid dates raise ValueError."""
        with pytest.raises(ValueError, match="Invalid date format"):
            _normalize_date("invalid-date")

        with pytest.raises(ValueError, match="Invalid date format"):
            _normalize_date("32-13-2025")


class TestCircuitBreakerHelpers:
    """Test cases for circuit breaker helper functions."""

    def test_reset_circuit_breaker(self):
        """Test circuit breaker reset functionality."""
        # Simulate some failures
        reset_circuit_breaker()

        # Manually increment (this would normally happen through failed requests)
        import app.currency

        app.currency._failure_count = 2

        assert get_failure_count() == 2

        reset_circuit_breaker()
        assert get_failure_count() == 0

    def test_get_failure_count(self):
        """Test failure count getter."""
        reset_circuit_breaker()
        assert get_failure_count() == 0

        # Manually set count to test getter
        import app.currency

        app.currency._failure_count = 5
        assert get_failure_count() == 5
</file>

<file path="backend/tests/test_db_models.py">
"""Tests for database models."""

import os
import tempfile
from datetime import datetime

import pytest
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker

from app.models import Base, File, Job


@pytest.fixture
def temp_db_session():
    """Create a temporary database session for testing."""
    # Create a temporary file for the SQLite database
    db_fd, db_path = tempfile.mkstemp(suffix=".db")
    os.close(db_fd)

    try:
        # Create engine and session
        engine = create_engine(f"sqlite:///{db_path}", connect_args={"check_same_thread": False})
        Base.metadata.create_all(bind=engine)

        SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)
        session = SessionLocal()

        yield session

        session.close()
    finally:
        # Clean up the temporary database file
        if os.path.exists(db_path):
            os.unlink(db_path)


def test_job_creation(temp_db_session):
    """Test creating and querying a Job."""
    # Create a job
    job = Job(job_id="test-job-123", status="pending", processed=0, total=5)

    temp_db_session.add(job)
    temp_db_session.commit()

    # Query back the job
    retrieved_job = temp_db_session.query(Job).filter(Job.job_id == "test-job-123").first()

    assert retrieved_job is not None
    assert retrieved_job.job_id == "test-job-123"
    assert retrieved_job.status == "pending"
    assert retrieved_job.processed == 0
    assert retrieved_job.total == 5
    assert isinstance(retrieved_job.created_at, datetime)
    assert isinstance(retrieved_job.updated_at, datetime)


def test_file_creation(temp_db_session):
    """Test creating and querying a File."""
    # Create a job first
    job = Job(job_id="test-job-456", status="processing", processed=1, total=3)
    temp_db_session.add(job)
    temp_db_session.commit()

    # Create a file associated with the job
    file = File(
        job_id="test-job-456",
        filename="invoice1.pdf",
        status="completed",
        original_currency="USD",
        target_currency="EUR",
    )

    temp_db_session.add(file)
    temp_db_session.commit()

    # Query back the file
    retrieved_file = temp_db_session.query(File).filter(File.filename == "invoice1.pdf").first()

    assert retrieved_file is not None
    assert retrieved_file.job_id == "test-job-456"
    assert retrieved_file.filename == "invoice1.pdf"
    assert retrieved_file.status == "completed"
    assert retrieved_file.original_currency == "USD"
    assert retrieved_file.target_currency == "EUR"
    assert retrieved_file.error_message is None


def test_job_file_relationship(temp_db_session):
    """Test the relationship between Job and File models."""
    # Create a job
    job = Job(job_id="test-job-789", status="completed", processed=2, total=2)
    temp_db_session.add(job)
    temp_db_session.commit()

    # Create files associated with the job
    file1 = File(
        job_id="test-job-789",
        filename="invoice1.pdf",
        status="completed",
        original_currency="USD",
        target_currency="EUR",
    )

    file2 = File(
        job_id="test-job-789",
        filename="invoice2.pdf",
        status="failed",
        original_currency="GBP",
        target_currency="EUR",
        error_message="Invalid PDF format",
    )

    temp_db_session.add_all([file1, file2])
    temp_db_session.commit()

    # Query job and check relationship
    retrieved_job = temp_db_session.query(Job).filter(Job.job_id == "test-job-789").first()
    assert len(retrieved_job.files) == 2

    # Check file relationship back to job
    retrieved_file = temp_db_session.query(File).filter(File.filename == "invoice1.pdf").first()
    assert retrieved_file.job.job_id == "test-job-789"
    assert retrieved_file.job.status == "completed"
</file>

<file path="backend/tests/test_health.py">
"""Tests for the health endpoint."""

from fastapi.testclient import TestClient

from app.main import app

client = TestClient(app)


def test_health_endpoint():
    """Test that health endpoint returns 200 and correct JSON."""
    response = client.get("/health")
    assert response.status_code == 200
    assert response.json() == {"status": "ok"}
</file>

<file path="backend/tests/test_pipeline.py">
"""Tests for LangGraph pipeline."""

import pytest

from langgraph_nodes.pipeline import get_compiled_pipeline


@pytest.mark.asyncio
async def test_pipeline_noop():
    """Test that pipeline passes data through unchanged."""
    # Sample input data
    sample_input = {
        "job_id": "test-job-123",
        "files": [
            {"filename": "invoice1.pdf", "status": "uploaded"},
            {"filename": "invoice2.pdf", "status": "uploaded"},
        ],
        "target_currency": "USD",
        "metadata": {"user_id": "user123", "timestamp": "2025-01-01T00:00:00Z"},
    }

    # Get compiled pipeline
    pipeline = get_compiled_pipeline()

    # Execute pipeline
    result = await pipeline.ainvoke(sample_input)

    # Assert output is identical to input
    assert result == sample_input
</file>

<file path="backend/tests/test_placeholder.py">
"""Placeholder test to verify pytest configuration."""


def test_placeholder():
    """Basic test to ensure pytest is working."""
    assert 1 == 1
</file>

<file path="backend/tests/test_sse_progress.py">
"""Tests for SSE progress endpoint."""

import json

import pytest
from httpx import ASGITransport, AsyncClient

from app.main import app


@pytest.mark.asyncio
async def test_progress_endpoint_returns_sse_events():
    """Test that progress endpoint returns correct SSE events."""
    job_id = "test-job-123"

    async with AsyncClient(transport=ASGITransport(app=app), base_url="http://test") as client:
        async with client.stream("GET", f"/progress/{job_id}") as response:
            assert response.status_code == 200
            assert response.headers["content-type"] == "text/event-stream; charset=utf-8"
            assert response.headers["cache-control"] == "no-cache"

            events = []
            buffer = ""

            async for chunk in response.aiter_text():
                buffer += chunk

                # Process complete SSE events (separated by \n\n)
                while "\n\n" in buffer:
                    event_text, buffer = buffer.split("\n\n", 1)

                    if event_text.startswith("data: "):
                        data_line = event_text[6:].strip()  # Remove "data: " prefix
                        if data_line:
                            event_data = json.loads(data_line)
                            events.append(event_data)

                            # Stop after we get the completion event
                            if event_data.get("status") == "completed":
                                break

            # Should have 10 processing events + 1 completion event = 11 total
            assert len(events) == 11

            # Check processing events
            for i in range(10):
                event = events[i]
                assert event["job_id"] == job_id
                assert event["status"] == "processing"
                assert event["percentage"] == i * 10

            # Check completion event
            completion_event = events[10]
            assert completion_event["job_id"] == job_id
            assert completion_event["status"] == "completed"
            assert completion_event["percentage"] == 100
</file>

<file path="backend/.gitignore">
# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
share/python-wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST

# PyInstaller
*.manifest
*.spec

# Installer logs
pip-log.txt
pip-delete-this-directory.txt

# Unit test / coverage reports
htmlcov/
.tox/
.nox/
.coverage
.coverage.*
.cache
nosetests.xml
coverage.xml
*.cover
*.py,cover
.hypothesis/
.pytest_cache/
cover/

# Translations
*.mo
*.pot

# Django stuff:
*.log
local_settings.py
db.sqlite3
db.sqlite3-journal

# Flask stuff:
instance/
.webassets-cache

# Scrapy stuff:
.scrapy

# Sphinx documentation
docs/_build/

# PyBuilder
.pybuilder/
target/

# Jupyter Notebook
.ipynb_checkpoints

# IPython
profile_default/
ipython_config.py

# pyenv
.python-version

# pipenv
Pipfile.lock

# pdm
.pdm.toml

# PEP 582
__pypackages__/

# Celery stuff
celerybeat-schedule
celerybeat.pid

# SageMath parsed files
*.sage.py

# Environments
.env
.venv
env/
venv/
ENV/
env.bak/
venv.bak/

# Spyder project settings
.spyderproject
.spyproject

# Rope project settings
.ropeproject

# mkdocs documentation
/site

# mypy
.mypy_cache/
.dmypy.json
dmypy.json

# Pyre type checker
.pyre/

# pytype static type analyzer
.pytype/

# Cython debug symbols
cython_debug/

# Ruff cache
.ruff_cache/
</file>

<file path="backend/alembic.ini">
# A generic, single database configuration.

[alembic]
# path to migration scripts.
# this is typically a path given in POSIX (e.g. forward slashes)
# format, relative to the token %(here)s which refers to the location of this
# ini file
script_location = %(here)s/alembic

# template used to generate migration file names; The default value is %%(rev)s_%%(slug)s
# Uncomment the line below if you want the files to be prepended with date and time
# see https://alembic.sqlalchemy.org/en/latest/tutorial.html#editing-the-ini-file
# for all available tokens
# file_template = %%(year)d_%%(month).2d_%%(day).2d_%%(hour).2d%%(minute).2d-%%(rev)s_%%(slug)s

# sys.path path, will be prepended to sys.path if present.
# defaults to the current working directory.  for multiple paths, the path separator
# is defined by "path_separator" below.
prepend_sys_path = .


# timezone to use when rendering the date within the migration file
# as well as the filename.
# If specified, requires the python>=3.9 or backports.zoneinfo library and tzdata library.
# Any required deps can installed by adding `alembic[tz]` to the pip requirements
# string value is passed to ZoneInfo()
# leave blank for localtime
# timezone =

# max length of characters to apply to the "slug" field
# truncate_slug_length = 40

# set to 'true' to run the environment during
# the 'revision' command, regardless of autogenerate
# revision_environment = false

# set to 'true' to allow .pyc and .pyo files without
# a source .py file to be detected as revisions in the
# versions/ directory
# sourceless = false

# version location specification; This defaults
# to <script_location>/versions.  When using multiple version
# directories, initial revisions must be specified with --version-path.
# The path separator used here should be the separator specified by "path_separator"
# below.
# version_locations = %(here)s/bar:%(here)s/bat:%(here)s/alembic/versions

# path_separator; This indicates what character is used to split lists of file
# paths, including version_locations and prepend_sys_path within configparser
# files such as alembic.ini.
# The default rendered in new alembic.ini files is "os", which uses os.pathsep
# to provide os-dependent path splitting.
#
# Note that in order to support legacy alembic.ini files, this default does NOT
# take place if path_separator is not present in alembic.ini.  If this
# option is omitted entirely, fallback logic is as follows:
#
# 1. Parsing of the version_locations option falls back to using the legacy
#    "version_path_separator" key, which if absent then falls back to the legacy
#    behavior of splitting on spaces and/or commas.
# 2. Parsing of the prepend_sys_path option falls back to the legacy
#    behavior of splitting on spaces, commas, or colons.
#
# Valid values for path_separator are:
#
# path_separator = :
# path_separator = ;
# path_separator = space
# path_separator = newline
#
# Use os.pathsep. Default configuration used for new projects.
path_separator = os

# set to 'true' to search source files recursively
# in each "version_locations" directory
# new in Alembic version 1.10
# recursive_version_locations = false

# the output encoding used when revision files
# are written from script.py.mako
# output_encoding = utf-8

# database URL.  This is consumed by the user-maintained env.py script only.
# other means of configuring database URLs may be customized within the env.py
# file.
# sqlalchemy.url = driver://user:pass@localhost/dbname


[post_write_hooks]
# post_write_hooks defines scripts or Python functions that are run
# on newly generated revision scripts.  See the documentation for further
# detail and examples

# format using "black" - use the console_scripts runner, against the "black" entrypoint
# hooks = black
# black.type = console_scripts
# black.entrypoint = black
# black.options = -l 79 REVISION_SCRIPT_FILENAME

# lint with attempts to fix using "ruff" - use the module runner, against the "ruff" module
# hooks = ruff
# ruff.type = module
# ruff.module = ruff
# ruff.options = check --fix REVISION_SCRIPT_FILENAME

# Alternatively, use the exec runner to execute a binary found on your PATH
# hooks = ruff
# ruff.type = exec
# ruff.executable = ruff
# ruff.options = check --fix REVISION_SCRIPT_FILENAME

# Logging configuration.  This is also consumed by the user-maintained
# env.py script only.
[loggers]
keys = root,sqlalchemy,alembic

[handlers]
keys = console

[formatters]
keys = generic

[logger_root]
level = WARNING
handlers = console
qualname =

[logger_sqlalchemy]
level = WARNING
handlers =
qualname = sqlalchemy.engine

[logger_alembic]
level = INFO
handlers =
qualname = alembic

[handler_console]
class = StreamHandler
args = (sys.stderr,)
level = NOTSET
formatter = generic

[formatter_generic]
format = %(levelname)-5.5s [%(name)s] %(message)s
datefmt = %H:%M:%S
</file>

<file path="backend/poetry.toml">
[virtualenvs]
in-project = true
</file>

<file path="backend/README.md">
# Invoice Converter Backend

FastAPI backend for the Invoice Converter application.

## Features

- Invoice data extraction using Azure Document Intelligence
- Currency conversion via Frankfurter API
- LangGraph pipeline orchestration
- Real-time progress tracking with Server-Sent Events
- Excel report generation

## Development

```bash
# Install dependencies
poetry install

# Run tests
poetry run pytest

# Start development server
poetry run uvicorn app.main:app --reload
```
</file>

<file path="frontend/public/vite.svg">
<svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" class="iconify iconify--logos" width="31.88" height="32" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 257"><defs><linearGradient id="IconifyId1813088fe1fbc01fb466" x1="-.828%" x2="57.636%" y1="7.652%" y2="78.411%"><stop offset="0%" stop-color="#41D1FF"></stop><stop offset="100%" stop-color="#BD34FE"></stop></linearGradient><linearGradient id="IconifyId1813088fe1fbc01fb467" x1="43.376%" x2="50.316%" y1="2.242%" y2="89.03%"><stop offset="0%" stop-color="#FFEA83"></stop><stop offset="8.333%" stop-color="#FFDD35"></stop><stop offset="100%" stop-color="#FFA800"></stop></linearGradient></defs><path fill="url(#IconifyId1813088fe1fbc01fb466)" d="M255.153 37.938L134.897 252.976c-2.483 4.44-8.862 4.466-11.382.048L.875 37.958c-2.746-4.814 1.371-10.646 6.827-9.67l120.385 21.517a6.537 6.537 0 0 0 2.322-.004l117.867-21.483c5.438-.991 9.574 4.796 6.877 9.62Z"></path><path fill="url(#IconifyId1813088fe1fbc01fb467)" d="M185.432.063L96.44 17.501a3.268 3.268 0 0 0-2.634 3.014l-5.474 92.456a3.268 3.268 0 0 0 3.997 3.378l24.777-5.718c2.318-.535 4.413 1.507 3.936 3.838l-7.361 36.047c-.495 2.426 1.782 4.5 4.151 3.78l15.304-4.649c2.372-.72 4.652 1.36 4.15 3.788l-11.698 56.621c-.732 3.542 3.979 5.473 5.943 2.437l1.313-2.028l72.516-144.72c1.215-2.423-.88-5.186-3.54-4.672l-25.505 4.922c-2.396.462-4.435-1.77-3.759-4.114l16.646-57.705c.677-2.35-1.37-4.583-3.769-4.113Z"></path></svg>
</file>

<file path="frontend/src/assets/react.svg">
<svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" class="iconify iconify--logos" width="35.93" height="32" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 228"><path fill="#00D8FF" d="M210.483 73.824a171.49 171.49 0 0 0-8.24-2.597c.465-1.9.893-3.777 1.273-5.621c6.238-30.281 2.16-54.676-11.769-62.708c-13.355-7.7-35.196.329-57.254 19.526a171.23 171.23 0 0 0-6.375 5.848a155.866 155.866 0 0 0-4.241-3.917C100.759 3.829 77.587-4.822 63.673 3.233C50.33 10.957 46.379 33.89 51.995 62.588a170.974 170.974 0 0 0 1.892 8.48c-3.28.932-6.445 1.924-9.474 2.98C17.309 83.498 0 98.307 0 113.668c0 15.865 18.582 31.778 46.812 41.427a145.52 145.52 0 0 0 6.921 2.165a167.467 167.467 0 0 0-2.01 9.138c-5.354 28.2-1.173 50.591 12.134 58.266c13.744 7.926 36.812-.22 59.273-19.855a145.567 145.567 0 0 0 5.342-4.923a168.064 168.064 0 0 0 6.92 6.314c21.758 18.722 43.246 26.282 56.54 18.586c13.731-7.949 18.194-32.003 12.4-61.268a145.016 145.016 0 0 0-1.535-6.842c1.62-.48 3.21-.974 4.76-1.488c29.348-9.723 48.443-25.443 48.443-41.52c0-15.417-17.868-30.326-45.517-39.844Zm-6.365 70.984c-1.4.463-2.836.91-4.3 1.345c-3.24-10.257-7.612-21.163-12.963-32.432c5.106-11 9.31-21.767 12.459-31.957c2.619.758 5.16 1.557 7.61 2.4c23.69 8.156 38.14 20.213 38.14 29.504c0 9.896-15.606 22.743-40.946 31.14Zm-10.514 20.834c2.562 12.94 2.927 24.64 1.23 33.787c-1.524 8.219-4.59 13.698-8.382 15.893c-8.067 4.67-25.32-1.4-43.927-17.412a156.726 156.726 0 0 1-6.437-5.87c7.214-7.889 14.423-17.06 21.459-27.246c12.376-1.098 24.068-2.894 34.671-5.345a134.17 134.17 0 0 1 1.386 6.193ZM87.276 214.515c-7.882 2.783-14.16 2.863-17.955.675c-8.075-4.657-11.432-22.636-6.853-46.752a156.923 156.923 0 0 1 1.869-8.499c10.486 2.32 22.093 3.988 34.498 4.994c7.084 9.967 14.501 19.128 21.976 27.15a134.668 134.668 0 0 1-4.877 4.492c-9.933 8.682-19.886 14.842-28.658 17.94ZM50.35 144.747c-12.483-4.267-22.792-9.812-29.858-15.863c-6.35-5.437-9.555-10.836-9.555-15.216c0-9.322 13.897-21.212 37.076-29.293c2.813-.98 5.757-1.905 8.812-2.773c3.204 10.42 7.406 21.315 12.477 32.332c-5.137 11.18-9.399 22.249-12.634 32.792a134.718 134.718 0 0 1-6.318-1.979Zm12.378-84.26c-4.811-24.587-1.616-43.134 6.425-47.789c8.564-4.958 27.502 2.111 47.463 19.835a144.318 144.318 0 0 1 3.841 3.545c-7.438 7.987-14.787 17.08-21.808 26.988c-12.04 1.116-23.565 2.908-34.161 5.309a160.342 160.342 0 0 1-1.76-7.887Zm110.427 27.268a347.8 347.8 0 0 0-7.785-12.803c8.168 1.033 15.994 2.404 23.343 4.08c-2.206 7.072-4.956 14.465-8.193 22.045a381.151 381.151 0 0 0-7.365-13.322Zm-45.032-43.861c5.044 5.465 10.096 11.566 15.065 18.186a322.04 322.04 0 0 0-30.257-.006c4.974-6.559 10.069-12.652 15.192-18.18ZM82.802 87.83a323.167 323.167 0 0 0-7.227 13.238c-3.184-7.553-5.909-14.98-8.134-22.152c7.304-1.634 15.093-2.97 23.209-3.984a321.524 321.524 0 0 0-7.848 12.897Zm8.081 65.352c-8.385-.936-16.291-2.203-23.593-3.793c2.26-7.3 5.045-14.885 8.298-22.6a321.187 321.187 0 0 0 7.257 13.246c2.594 4.48 5.28 8.868 8.038 13.147Zm37.542 31.03c-5.184-5.592-10.354-11.779-15.403-18.433c4.902.192 9.899.29 14.978.29c5.218 0 10.376-.117 15.453-.343c-4.985 6.774-10.018 12.97-15.028 18.486Zm52.198-57.817c3.422 7.8 6.306 15.345 8.596 22.52c-7.422 1.694-15.436 3.058-23.88 4.071a382.417 382.417 0 0 0 7.859-13.026a347.403 347.403 0 0 0 7.425-13.565Zm-16.898 8.101a358.557 358.557 0 0 1-12.281 19.815a329.4 329.4 0 0 1-23.444.823c-7.967 0-15.716-.248-23.178-.732a310.202 310.202 0 0 1-12.513-19.846h.001a307.41 307.41 0 0 1-10.923-20.627a310.278 310.278 0 0 1 10.89-20.637l-.001.001a307.318 307.318 0 0 1 12.413-19.761c7.613-.576 15.42-.876 23.31-.876H128c7.926 0 15.743.303 23.354.883a329.357 329.357 0 0 1 12.335 19.695a358.489 358.489 0 0 1 11.036 20.54a329.472 329.472 0 0 1-11 20.722Zm22.56-122.124c8.572 4.944 11.906 24.881 6.52 51.026c-.344 1.668-.73 3.367-1.15 5.09c-10.622-2.452-22.155-4.275-34.23-5.408c-7.034-10.017-14.323-19.124-21.64-27.008a160.789 160.789 0 0 1 5.888-5.4c18.9-16.447 36.564-22.941 44.612-18.3ZM128 90.808c12.625 0 22.86 10.235 22.86 22.86s-10.235 22.86-22.86 22.86s-22.86-10.235-22.86-22.86s10.235-22.86 22.86-22.86Z"></path></svg>
</file>

<file path="frontend/src/components/ProgressBar.test.tsx">
import { render } from '@testing-library/react';
import { ProgressBar } from './ProgressBar';

describe('ProgressBar', () => {
  it('should display correct width based on percentage', () => {
    const { container, rerender } = render(<ProgressBar percentage={25} />);
    
    const progressFill = container.querySelector('.bg-blue-600');
    expect(progressFill).toHaveStyle('width: 25%');

    // Test width increases
    rerender(<ProgressBar percentage={75} />);
    expect(progressFill).toHaveStyle('width: 75%');
  });

  it('should clamp percentage values', () => {
    const { container, rerender } = render(<ProgressBar percentage={-10} />);
    
    const progressFill = container.querySelector('.bg-blue-600');
    expect(progressFill).toHaveStyle('width: 0%');

    rerender(<ProgressBar percentage={150} />);
    expect(progressFill).toHaveStyle('width: 100%');
  });
});
</file>

<file path="frontend/src/components/ProgressBar.tsx">
interface ProgressBarProps {
  percentage: number;
}

export const ProgressBar = ({ percentage }: ProgressBarProps) => {
  const clampedPercentage = Math.min(Math.max(percentage, 0), 100);

  return (
    <div className="w-full bg-gray-200 rounded-full h-2.5">
      <div
        className="bg-blue-600 h-2.5 rounded-full transition-all duration-300 ease-out"
        style={{ width: `${clampedPercentage}%` }}
      />
    </div>
  );
};
</file>

<file path="frontend/src/components/UploadArea.test.tsx">
import { render, screen, fireEvent, act } from '@testing-library/react';
import UploadArea from './UploadArea';

test('displays selected files in DOM after selection', async () => {
  render(<UploadArea />);

  // Create fake files
  const file1 = new File(['file1 content'], 'invoice1.pdf', { type: 'application/pdf' });
  const file2 = new File(['file2 content'], 'receipt2.jpg', { type: 'image/jpeg' });

  // Get the file input (it's hidden by react-dropzone)
  const fileInput = document.querySelector('input[type="file"]') as HTMLInputElement;

  // Simulate file selection wrapped in act
  await act(async () => {
    Object.defineProperty(fileInput, 'files', {
      value: [file1, file2],
      writable: false,
    });

    fireEvent.change(fileInput);
  });

  // Assert that files appear in the DOM
  expect(screen.getByText('invoice1.pdf')).toBeInTheDocument();
  expect(screen.getByText('receipt2.jpg')).toBeInTheDocument();
  expect(screen.getByText('Selected Files (2)')).toBeInTheDocument();

  // Check that file elements have the correct test id
  const selectedFiles = screen.getAllByTestId('selected-file');
  expect(selectedFiles).toHaveLength(2);
});
</file>

<file path="frontend/src/hooks/useSse.test.ts">
import { renderHook, act } from '@testing-library/react';
import { useSse } from './useSse';

// Mock EventSource
interface MockEventSourceType {
  url: string;
  onmessage: ((event: MessageEvent) => void) | null;
  onerror: ((event: Event) => void) | null;
  readyState: number;
  close: jest.Mock;
  simulateMessage: (data: string) => void;
  simulateError: () => void;
}

let mockEventSource: MockEventSourceType;

const createMockEventSource = () => ({
  url: '',
  onmessage: null as ((event: MessageEvent) => void) | null,
  onerror: null as ((event: Event) => void) | null,
  readyState: 1,
  close: jest.fn(),
  simulateMessage: function(data: string) {
    if (this.onmessage) {
      const event = new MessageEvent('message', { data });
      this.onmessage(event);
    }
  },
  simulateError: function() {
    if (this.onerror) {
      const event = new Event('error');
      this.onerror(event);
    }
  }
});

// Mock the global EventSource
const originalEventSource = global.EventSource;
beforeEach(() => {
  mockEventSource = createMockEventSource();
  global.EventSource = jest.fn().mockImplementation((url) => {
    mockEventSource.url = url;
    return mockEventSource;
  }) as unknown as typeof EventSource;
});

afterEach(() => {
  global.EventSource = originalEventSource;
  jest.clearAllMocks();
});

describe('useSse', () => {
  it('should handle two events and update data progressively', () => {
    const { result } = renderHook(() => useSse('/test-url'));

    // Initially should have no data
    expect(result.current.data).toBeNull();
    expect(result.current.error).toBeNull();

    // Simulate first event with 25% progress
    act(() => {
      mockEventSource.simulateMessage(JSON.stringify({ percentage: 25 }));
    });

    expect(result.current.data).toEqual({ percentage: 25 });
    expect(result.current.error).toBeNull();

    // Simulate second event with 75% progress
    act(() => {
      mockEventSource.simulateMessage(JSON.stringify({ percentage: 75 }));
    });

    expect(result.current.data).toEqual({ percentage: 75 });
    expect(result.current.error).toBeNull();
  });

  it('should handle JSON parse errors', () => {
    const { result } = renderHook(() => useSse('/test-url'));

    act(() => {
      mockEventSource.simulateMessage('invalid json');
    });

    expect(result.current.data).toBeNull();
    expect(result.current.error).toBe('Failed to parse JSON data');
  });

  it('should handle connection errors', () => {
    const { result } = renderHook(() => useSse('/test-url'));

    act(() => {
      mockEventSource.simulateError();
    });

    expect(result.current.error).toBe('EventSource connection error');
  });
});
</file>

<file path="frontend/src/hooks/useSse.ts">
import { useState, useEffect, useRef } from 'react';

interface UseSseReturn<T = unknown> {
  data: T | null;
  error: string | null;
}

export const useSse = <T = unknown>(url: string): UseSseReturn<T> => {
  const [data, setData] = useState<T | null>(null);
  const [error, setError] = useState<string | null>(null);
  const eventSourceRef = useRef<EventSource | null>(null);

  useEffect(() => {
    if (!url) return;

    const eventSource = new EventSource(url);
    eventSourceRef.current = eventSource;

    eventSource.onmessage = (event) => {
      try {
        const parsedData = JSON.parse(event.data) as T;
        setData(parsedData);
        setError(null);
      } catch {
        setError('Failed to parse JSON data');
      }
    };

    eventSource.onerror = () => {
      setError('EventSource connection error');
    };

    return () => {
      eventSource.close();
    };
  }, [url]);

  return { data, error };
};
</file>

<file path="frontend/src/App.css">
#root {
  max-width: 1280px;
  margin: 0 auto;
  padding: 2rem;
  text-align: center;
}

.logo {
  height: 6em;
  padding: 1.5em;
  will-change: filter;
  transition: filter 300ms;
}
.logo:hover {
  filter: drop-shadow(0 0 2em #646cffaa);
}
.logo.react:hover {
  filter: drop-shadow(0 0 2em #61dafbaa);
}

@keyframes logo-spin {
  from {
    transform: rotate(0deg);
  }
  to {
    transform: rotate(360deg);
  }
}

@media (prefers-reduced-motion: no-preference) {
  a:nth-of-type(2) .logo {
    animation: logo-spin infinite 20s linear;
  }
}

.card {
  padding: 2em;
}

.read-the-docs {
  color: #888;
}
</file>

<file path="frontend/src/App.test.tsx">
import { render, screen } from '@testing-library/react';
import App from './App';

test('renders headline', () => {
  render(<App />);
  const headlineElement = screen.getByRole('heading', { name: /invoice converter/i });
  expect(headlineElement).toBeInTheDocument();
});
</file>

<file path="frontend/src/index.css">
@tailwind base;
@tailwind components;
@tailwind utilities;
</file>

<file path="frontend/postcss.config.js">
export default {
  plugins: {
    tailwindcss: {},
    autoprefixer: {},
  },
}
</file>

<file path="frontend/README.md">
# React + TypeScript + Vite

This template provides a minimal setup to get React working in Vite with HMR and some ESLint rules.

Currently, two official plugins are available:

- [@vitejs/plugin-react](https://github.com/vitejs/vite-plugin-react/blob/main/packages/plugin-react) uses [Babel](https://babeljs.io/) for Fast Refresh
- [@vitejs/plugin-react-swc](https://github.com/vitejs/vite-plugin-react/blob/main/packages/plugin-react-swc) uses [SWC](https://swc.rs/) for Fast Refresh

## Expanding the ESLint configuration

If you are developing a production application, we recommend updating the configuration to enable type-aware lint rules:

```js
export default tseslint.config([
  globalIgnores(['dist']),
  {
    files: ['**/*.{ts,tsx}'],
    extends: [
      // Other configs...

      // Remove tseslint.configs.recommended and replace with this
      ...tseslint.configs.recommendedTypeChecked,
      // Alternatively, use this for stricter rules
      ...tseslint.configs.strictTypeChecked,
      // Optionally, add this for stylistic rules
      ...tseslint.configs.stylisticTypeChecked,

      // Other configs...
    ],
    languageOptions: {
      parserOptions: {
        project: ['./tsconfig.node.json', './tsconfig.app.json'],
        tsconfigRootDir: import.meta.dirname,
      },
      // other options...
    },
  },
])
```

You can also install [eslint-plugin-react-x](https://github.com/Rel1cx/eslint-react/tree/main/packages/plugins/eslint-plugin-react-x) and [eslint-plugin-react-dom](https://github.com/Rel1cx/eslint-react/tree/main/packages/plugins/eslint-plugin-react-dom) for React-specific lint rules:

```js
// eslint.config.js
import reactX from 'eslint-plugin-react-x'
import reactDom from 'eslint-plugin-react-dom'

export default tseslint.config([
  globalIgnores(['dist']),
  {
    files: ['**/*.{ts,tsx}'],
    extends: [
      // Other configs...
      // Enable lint rules for React
      reactX.configs['recommended-typescript'],
      // Enable lint rules for React DOM
      reactDom.configs.recommended,
    ],
    languageOptions: {
      parserOptions: {
        project: ['./tsconfig.node.json', './tsconfig.app.json'],
        tsconfigRootDir: import.meta.dirname,
      },
      // other options...
    },
  },
])
```
</file>

<file path="frontend/tailwind.config.js">
/** @type {import('tailwindcss').Config} */
export default {
  content: [
    "./index.html",
    "./src/**/*.{js,ts,jsx,tsx}",
  ],
  theme: {
    extend: {},
  },
  plugins: [],
}
</file>

<file path="frontend/tsconfig.app.json">
{
  "compilerOptions": {
    "tsBuildInfoFile": "./node_modules/.tmp/tsconfig.app.tsbuildinfo",
    "target": "ES2022",
    "useDefineForClassFields": true,
    "lib": ["ES2022", "DOM", "DOM.Iterable"],
    "module": "ESNext",
    "skipLibCheck": true,

    /* Bundler mode */
    "moduleResolution": "bundler",
    "allowImportingTsExtensions": true,
    "verbatimModuleSyntax": true,
    "moduleDetection": "force",
    "noEmit": true,
    "jsx": "react-jsx",

    /* Linting */
    "strict": true,
    "noUnusedLocals": true,
    "noUnusedParameters": true,
    "erasableSyntaxOnly": true,
    "noFallthroughCasesInSwitch": true,
    "noUncheckedSideEffectImports": true
  },
  "include": ["src"]
}
</file>

<file path="todo.md">
# Invoice Converter Master TODO Checklist

A comprehensive, stepbystep list of everything needed to deliver **v0.1.0**. Tick each box as you finish the work **on the feature branch *before* you open your PR**.

> **Legend**
>  = not started = in progress (open PR) = merged into `main`

---

## P0Repo&CIBaseline

* [ ] **C0.1`init-repo`**  skeleton, `.gitignore`, MIT license, empty `backend/`&`frontend/` dirs
* [ ] **C0.2`tooling-ci`**  Ruff+Black, Jest, precommit, placeholder tests, GitHubCI

## P1BackendScaffold

* [ ] **C1.1`backend-scaffold`**  FastAPI app + `/health`, pytest client test
* [ ] **C1.2`db-basics`**

  * [ ] SQLAlchemy models (`Job`, `File`)
  * [ ] `db.py` session helper
  * [ ] Alembic init + first migration
  * [ ] Model insert/query unit test
  * [ ] CI: run `alembic upgrade head` before tests

## P2FrontendScaffold

* [ ] **C2.1`frontend-scaffold`**  Vite+ReactTS, Tailwind, smoke test
* [ ] **C2.2`upload-stub`**  `<UploadArea>` with Dropzone & Jest test

## P3JobLifecycle &SSE

* [ ] **C3.1`sse-endpoint`**  dummy SSE endpoint & async test
* [ ] **C3.2`sse-frontend-hook`**  `useSse`, `<ProgressBar>`, mocked EventSource test

## P4LangGraph Skeleton

* [ ] **C4.1`langgraph-skeleton`**  six empty nodes + passthrough pipeline, unit test

## P5Azure Extractor

* [ ] **C5.1`azure-adapter`**  DI wrapper, `InvoiceData` dataclass, VCR tests

## P6Currency Flow

* [ ] **C6.1`currency-adapter`**  Frankfurter client, retry guard, unit tests
* [ ] **C6.2`converter-node`**  HALF\_UP rounding logic, tests

## P7Excel Generator

* [ ] **C7.1`excel-generator`**  openpyxl workbook, suffix helper, byteshash test

## P8PipelineIntegration

* [ ] **C8.1`pipeline-integration`**

  * [ ] Wire real nodes in order
  * [ ] `/process-invoices` saves uploads & spawns task
  * [ ] `execute_pipeline` queues progress for SSE
  * [ ] `/download/{job_id}` serves XLSX
  * [ ] Endtoend integration test (3 invoices)

## P9FrontendUX

* [ ] **C9.1`ui-currency-select`**  ISO dropdown, Jest interaction test
* [ ] **C9.2`ui-complete-flow`**  POST upload, live progress, autodownload, optional E2E script

## P10Packaging &Deploy

* [ ] **C10.1`docker-prod`**  multistage Dockerfile, compose file, README excerpt

## P11Hardening&Docs

* [ ] **C11.1`edge-tests`**  backend edge cases, frontend limits, 80% coverage gate
* [ ] **C11.2`docs-polish`**  Architecture doc, expanded README, Contributing guide

## Release

* [ ] **Tag`v0.1.0`**  bump version, CHANGELOG, `git tag v0.1.0`

---

### NicetoHave (Postv0.1)

* [ ] Cypress/Playwright E2E pipeline in CI
* [ ] Caching Frankfurter responses in SQLite table
* [ ] Darkmode switch in UI
* [ ] i18n support (reacti18next) for UI strings
</file>

<file path="backend/app/main.py">
"""Invoice Converter API main application."""

import asyncio
import json
from collections.abc import AsyncGenerator

from fastapi import FastAPI
from fastapi.responses import StreamingResponse

app = FastAPI(title="Invoice Converter API")


async def event_generator(job_id: str) -> AsyncGenerator[str, None]:
    """Generate SSE events for job progress."""
    for i in range(10):
        percentage = i * 10
        event_data = {"job_id": job_id, "status": "processing", "percentage": percentage}
        yield f"data: {json.dumps(event_data)}\n\n"
        await asyncio.sleep(1)

    # Send completion event
    completion_data = {"job_id": job_id, "status": "completed", "percentage": 100}
    yield f"data: {json.dumps(completion_data)}\n\n"


@app.get("/progress/{job_id}")
async def get_progress(job_id: str):
    """SSE endpoint for job progress updates."""
    return StreamingResponse(
        event_generator(job_id),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
        },
    )


@app.get("/health")
def health_check():
    """Health check endpoint."""
    return {"status": "ok"}
</file>

<file path="frontend/src/components/UploadArea.tsx">
import { useCallback, useState } from 'react';
import { useDropzone } from 'react-dropzone';
import { useSse } from '../hooks/useSse';
import { ProgressBar } from './ProgressBar';

const MAX_FILE_SIZE = 1024 * 1024; // 1MB
const MAX_FILES = 100;
const ACCEPTED_FILE_TYPES = {
  'application/pdf': ['.pdf'],
  'image/jpeg': ['.jpg', '.jpeg'],
  'image/png': ['.png']
};

interface ProgressData {
  percentage: number;
}

export default function UploadArea() {
  const [selectedFiles, setSelectedFiles] = useState<File[]>([]);
  const { data: progressData } = useSse<ProgressData>(selectedFiles.length > 0 ? '/progress/demo' : '');

  const onDrop = useCallback((acceptedFiles: File[]) => {
    const validFiles = acceptedFiles.filter(file => {
      return file.size <= MAX_FILE_SIZE;
    });

    const totalFiles = selectedFiles.length + validFiles.length;
    const filesToAdd = totalFiles > MAX_FILES 
      ? validFiles.slice(0, MAX_FILES - selectedFiles.length)
      : validFiles;

    setSelectedFiles(prev => [...prev, ...filesToAdd]);
  }, [selectedFiles]);

  const { getRootProps, getInputProps, isDragActive } = useDropzone({
    onDrop,
    accept: ACCEPTED_FILE_TYPES,
    maxSize: MAX_FILE_SIZE,
    maxFiles: MAX_FILES
  });

  const removeFile = (index: number) => {
    setSelectedFiles(prev => prev.filter((_, i) => i !== index));
  };

  return (
    <div className="space-y-4">
      <div
        {...getRootProps()}
        className={`border-2 border-dashed rounded-lg p-8 text-center cursor-pointer transition-colors ${
          isDragActive 
            ? 'border-blue-400 bg-blue-50' 
            : 'border-gray-300 hover:border-gray-400'
        }`}
      >
        <input {...getInputProps()} />
        <div className="text-gray-600">
          {isDragActive ? (
            <p>Drop the files here...</p>
          ) : (
            <div>
              <p className="mb-2">Drag & drop files here, or click to select</p>
              <p className="text-sm text-gray-500">
                PDF, JPG, PNG files only. Max 1MB per file, 100 files total.
              </p>
            </div>
          )}
        </div>
      </div>

      {selectedFiles.length > 0 && (
        <div className="mt-4">
          <h3 className="text-lg font-medium mb-2">Selected Files ({selectedFiles.length})</h3>
          {progressData?.percentage !== undefined && (
            <div className="mb-4">
              <div className="flex justify-between items-center mb-1">
                <span className="text-sm font-medium text-gray-700">Processing Progress</span>
                <span className="text-sm text-gray-600">{progressData.percentage}%</span>
              </div>
              <ProgressBar percentage={progressData.percentage} />
            </div>
          )}
          <div className="space-y-2">
            {selectedFiles.map((file, index) => (
              <div
                key={`${file.name}-${index}`}
                className="flex items-center justify-between p-2 bg-gray-50 rounded border"
                data-testid="selected-file"
              >
                <div className="flex items-center space-x-2">
                  <span className="text-sm font-medium">{file.name}</span>
                  <span className="text-xs text-gray-500">
                    ({(file.size / 1024).toFixed(1)} KB)
                  </span>
                </div>
                <button
                  onClick={() => removeFile(index)}
                  className="text-red-500 hover:text-red-700 text-sm"
                  data-testid="remove-file-button"
                >
                  Remove
                </button>
              </div>
            ))}
          </div>
        </div>
      )}
    </div>
  );
}
</file>

<file path="frontend/src/main.tsx">
import { StrictMode } from 'react'
import { createRoot } from 'react-dom/client'
import './index.css'
import App from './App.tsx'

createRoot(document.getElementById('root')!).render(
  <StrictMode>
    <App />
  </StrictMode>,
)
</file>

<file path="frontend/src/vite-env.d.ts">
/// <reference types="vite/client" />
</file>

<file path="frontend/eslint.config.js">
import js from '@eslint/js'
import globals from 'globals'
import reactHooks from 'eslint-plugin-react-hooks'
import reactRefresh from 'eslint-plugin-react-refresh'
import tseslint from 'typescript-eslint'
import { globalIgnores } from 'eslint/config'

export default tseslint.config([
  globalIgnores(['dist', 'coverage']),
  {
    files: ['**/*.{ts,tsx}'],
    extends: [
      js.configs.recommended,
      tseslint.configs.recommended,
      reactHooks.configs['recommended-latest'],
      reactRefresh.configs.vite,
    ],
    languageOptions: {
      ecmaVersion: 2020,
      globals: globals.browser,
    },
  },
])
</file>

<file path="frontend/index.html">
<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <link rel="icon" type="image/svg+xml" href="/vite.svg" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Vite + React + TS</title>
  </head>
  <body>
    <div id="root"></div>
    <script type="module" src="/src/main.tsx"></script>
  </body>
</html>
</file>

<file path="frontend/tsconfig.json">
{
  "files": [],
  "references": [
    { "path": "./tsconfig.app.json" },
    { "path": "./tsconfig.node.json" }
  ]
}
</file>

<file path="frontend/tsconfig.node.json">
{
  "compilerOptions": {
    "tsBuildInfoFile": "./node_modules/.tmp/tsconfig.node.tsbuildinfo",
    "target": "ES2023",
    "lib": ["ES2023"],
    "module": "ESNext",
    "skipLibCheck": true,

    /* Bundler mode */
    "moduleResolution": "bundler",
    "allowImportingTsExtensions": true,
    "verbatimModuleSyntax": true,
    "moduleDetection": "force",
    "noEmit": true,

    /* Linting */
    "strict": true,
    "noUnusedLocals": true,
    "noUnusedParameters": true,
    "erasableSyntaxOnly": true,
    "noFallthroughCasesInSwitch": true,
    "noUncheckedSideEffectImports": true
  },
  "include": ["vite.config.ts"]
}
</file>

<file path="frontend/vite.config.ts">
import { defineConfig } from 'vite'
import react from '@vitejs/plugin-react'

// https://vite.dev/config/
export default defineConfig({
  plugins: [react()],
})
</file>

<file path=".pre-commit-config.yaml">
repos:
  - repo: https://github.com/pre-commit/pre-commit-hooks
    rev: v4.5.0
    hooks:
      - id: trailing-whitespace
      - id: end-of-file-fixer
      - id: check-yaml
      - id: check-added-large-files

  - repo: https://github.com/psf/black
    rev: 23.11.0
    hooks:
      - id: black
        files: ^backend/
        args: [--line-length=120]

  - repo: https://github.com/pycqa/isort
    rev: 5.12.0
    hooks:
      - id: isort
        files: ^backend/
        args: [--profile=black, --line-length=120]

  - repo: https://github.com/astral-sh/ruff-pre-commit
    rev: v0.1.6
    hooks:
      - id: ruff
        files: ^backend/
        args: [check]

  - repo: https://github.com/pre-commit/mirrors-eslint
    rev: v8.53.0
    hooks:
      - id: eslint
        files: ^frontend/.*\.(ts|tsx)$
        additional_dependencies:
          - eslint@8.53.0
          - "@typescript-eslint/eslint-plugin@6.10.0"
          - "@typescript-eslint/parser@6.10.0"
          - eslint-plugin-react-hooks@4.6.0
          - eslint-plugin-react-refresh@0.4.4
</file>

<file path="LICENSE">
MIT License

SPDX-License-Identifier: MIT

Copyright (c) 2025 Invoice Converter

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
</file>

<file path="prompt_plan.md">
# Invoice Converter - Development Prompt Plan

## Table of Contents

- [Pass 1: High-Level Blueprint](#pass-1-high-level-blueprint)
- [Pass 2: Break Each Phase into Chunks](#pass-2-break-each-phase-into-chunks)
- [Pass 3: Micro-Steps Example](#pass-3-micro-steps-example)
- [Prompts for Code-Generation LLM](#prompts-for-code-generation-llm)
  - [Initialization Prompts (P-0)](#initialization-prompts-p-0)
  - [Backend Foundation (P-1)](#backend-foundation-p-1)
  - [Frontend Foundation (P-2)](#frontend-foundation-p-2)
  - [Core Infrastructure (P-3 to P-4)](#core-infrastructure-p-3-to-p-4)
  - [Business Logic (P-5 to P-7)](#business-logic-p-5-to-p-7)
  - [Integration & UX (P-8 to P-9)](#integration--ux-p-8-to-p-9)
  - [Production & Polish (P-10 to P-11)](#production--polish-p-10-to-p-11)

---

## Pass 1: High-Level Blueprint

| Phase | Goal | Main Deliverables |
|-------|------|-------------------|
| **P-0** | Repo & CI Baseline | Shared tooling & coding standards | mono-repo skeleton, pre-commit, Ruff+Black, Jest, GitHub Actions |
| **P-1** | Backend Scaffold | Small FastAPI app with health-check & SQLite wiring | `/health`, `/process-invoices` stub, Alembic migration, pytest env |
| **P-2** | Frontend Scaffold | React + Vite + Tailwind starter | Dropzone stub, currency dropdown stub, Storybook story |
| **P-3** | Job Lifecycle | Minimal job table + SSE progress stream | DB models, `/progress/{job_id}`, in-memory progress ticker |
| **P-4** | LangGraph Skeleton | Empty nodes wired in order | six placeholder nodes returning "pass-through" data |
| **P-5** | Azure Extractor | Real invoice field extraction | adapter around Azure SDK, unit tests w/ VCR-py |
| **P-6** | Currency Flow | Frankfurter integration & converter node | rate fetch, 3-failure guardrail, decimal math |
| **P-7** | Excel Generator | Styled openpyxl workbook + download endpoint | deterministic ordering & suffix logic |
| **P-8** | Full Pipeline Hook-up | End-to-end happy-path, streamed progress | LangGraph executor + SSE events |
| **P-9** | Frontend UX | Real-time progress UI & auto-download | EventSource hook, progress bar, quips JSON |
| **P-10** | Packaging | Single Docker image, compose file | multi-stage build, .env injection |
| **P-11** | Hardening | Edge-case tests, cleanup, docs | 80%+ coverage, README "Getting Started" |

## Pass 2: Break Each Phase into Chunks

| Chunk ID | Target Branch | Scope |
|----------|---------------|-------|
| **C-0.1** | `init-repo` | `.gitignore`, MIT LICENSE, empty `backend/` & `frontend/` dirs |
| **C-0.2** | `tooling-ci` | `pyproject` (Ruff, Black), `package.json` (Jest, RTL), pre-commit, simple GH Action "lint & test" |
| **C-1.1** | `backend-scaffold` | FastAPI `app/main.py` with `/health`, Poetry/uv setup |
| **C-1.2** | `db-basics` | SQLAlchemy models Job & File, Alembic migration, SQLite URL via env |
| **C-2.1** | `frontend-scaffold` | Vite + React TS, Tailwind config, `<App />` placeholder |
| **C-2.2** | `upload-stub` | `<InvoiceUploader>` dropzone accepting 1 MB, no network yet |
| **C-3.1** | `sse-endpoint` | `/progress/{job_id}` returning dummy ticking events |
| **C-3.2** | `sse-frontend-hook` | React hook `useSse` + progress bar component |
| **C-4.1** | `langgraph-skeleton` | six empty nodes, executor wiring, passes thru dummy dict |
| **C-5.1** | `azure-adapter` | thin wrapper over Azure DI, injectable interface, pytest with recorded cassette |
| **C-6.1** | `currency-adapter` | Frankfurter client, retry logic, unit tests |
| **C-6.2** | `converter-node` | rate  Decimal, half-up rounding |
| **C-7.1** | `excel-generator` | openpyxl workbook creator, style, unit test comparing XLSX bytes hash |
| **C-8.1** | `pipeline-integration` | Connect nodes, persist progress to DB, emit SSE |
| **C-9.1** | `ui-currency-select` | ISO code dropdown, default USD |
| **C-9.2** | `ui-complete-flow` | call `/process-invoices`, listen to progress, auto-download |
| **C-10.1** | `docker-prod` | multi-stage Dockerfile, compose, README snippet |
| **C-11.1** | `edge-tests` | missing date, Azure fail, 3rd-rate failure, front-end Jest |
| **C-11.2** | `docs-polish` | CONTRIBUTING, API md, architecture diagram |

## Pass 3: Micro-Steps Example

*Example shown for Chunk C-1.1; the prompts later cover all chunks*

| Step | Commit Message | Safety Net |
|------|----------------|------------|
| **S-1** | `"feat(backend): create FastAPI app with /health returning 200 OK JSON"` | `pytest test_health.py` |
| **S-2** | `"chore(tasks): add uvicorn app.main:app make target"` | none |
| **S-3** | `"test(ci): wire backend tests into GH Actions"` | action fails red on test error |

*Every other chunk is decomposed in the same fashion inside the prompt list.*

---

## Prompts for Code-Generation LLM

> **Instructions**: Paste one prompt at a time; run tests after each commit.  
> All prompts are self-contained and build on the code produced by previous prompts.

### Naming Convention

- Step numbers below match the chunk/step tables
- Branch names are suggestions; feel free to adjust

---

## Initialization Prompts (P-0)

### Prompt 1  `init-repo`

```text
You are a senior full-stack engineer.

**Task**  
Create the initial repository skeleton for "invoice-converter".

**Requirements**
1. Add `.gitignore` suitable for Python, Node, and macOS.
2. Add MIT LICENSE with SPDX header.
3. Create empty directories: `backend/` and `frontend/`.
4. Commit message: `chore(repo): initial skeleton`.

**Tests**  
No code yet  just ensure the directory tree exists.

Return ONLY the file list with contents where applicable.
```

### Prompt 2  `tooling-ci`

```text
You now have the repo from Prompt 1.

**Task**  
Introduce shared tooling and CI baseline.

*Backend (Python 3.11)*
- Add `pyproject.toml` using Poetry with Ruff & Black config (`line-length = 120`).

*Frontend (Node 20)*
- Add `package.json` with scripts:
  - `test`  Jest
  - `lint`  `eslint --ext ts,tsx src`

*Continuous Integration*
- `.github/workflows/ci.yml` running on push:
  1. Set up Python, install deps, run `pytest`.
  2. Set up Node, run `npm ci && npm test`.
  3. Run Ruff + Black check.

*Pre-commit*
- `.pre-commit-config.yaml` with `ruff`, `black`, and `isort`.

**Tests**
- Add a trivial `tests/test_placeholder.py` asserting `1 == 1`.
- Configure Jest with a sample test `frontend/src/__tests__/placeholder.test.ts` asserting `true`.

Commit message: `chore(ci): tooling, lint, pre-commit, placeholder tests`.
```

---

## Backend Foundation (P-1)

### Prompt 3  `backend-scaffold`

```text
Extend the codebase from Prompt 2.

**Task**
1. In `backend/app/`, create `main.py` that:
   - Instantiates FastAPI with `title="Invoice Converter API"`.
   - Defines `GET /health` that returns `{"status": "ok"}`.
2. Add `backend/app/__init__.py` (empty).
3. Add `backend/tests/test_health.py` that spins up `TestClient` and asserts 200 + JSON.
4. Update Poetry dependencies: `fastapi`, `uvicorn[standard]`, `pytest`, `httpx`, `pytest-asyncio`.

Commit message: `feat(backend): FastAPI app with /health and unit test`.
```

### Prompt 4  `db-basics`

```text
Build on previous code.

**Task**
1. Add SQLAlchemy models:
   - `Job(job_id: str PK, status: str, processed: int, total: int, created_at, updated_at)`
   - `File(id PK, job_id FK, filename, status, original_currency, target_currency, error_message)`
2. Create `backend/app/db.py` with `get_session()` using SQLite URL from env `DATABASE_URL` defaulting to `sqlite:///./invoice.db`.
3. Add Alembic with an initial migration generating the two tables.
4. Unit tests:
   - `test_db_models.py` inserts a job + file and queries back.
5. Update CI to run `alembic upgrade head` before tests.

Commit: `feat(db): SQLAlchemy models and Alembic migration`.
```

---

## Frontend Foundation (P-2)

### Prompt 5  `frontend-scaffold`

```text
Front-end time.

**Task**
1. Set up Vite + React + TypeScript in `frontend/` (use `npm create vite@latest` scaffolding).
2. Add Tailwind CSS (`tailwind.config.js`, `postcss.config.js`, `index.css` with Tailwind directives).
3. Replace `App.tsx` with a minimalist page containing:
   - `<h1>Invoice Converter</h1>`
   - A placeholder `<UploadArea/>` component (empty div for now).
4. Add Jest + React Testing Library config, plus a passing smoke test `renders headline`.

Commit: `feat(frontend): Vite+React scaffold with Tailwind`.
```

### Prompt 6  `upload-stub`

```text
Enhance the front-end.

**Task**
1. Install `react-dropzone`.
2. Create `frontend/src/components/UploadArea.tsx`:
   - Accepts PDF/JPG/PNG  1 MB, max 100 files.
   - Renders file names after selection.
   - NO network requests yet.
3. Update `App.tsx` to include `<UploadArea/>`.
4. Jest test: select two fake files, assert they appear in the DOM.

Commit: `feat(frontend): basic dropzone upload stub`.
```

---

## Core Infrastructure (P-3 to P-4)

### Prompt 7  `sse-endpoint`

```json
Back-end SSE foundation.

**Task**
1. Add `/progress/{job_id}` endpoint returning `text/event-stream`.
2. Use `async def event_generator(job_id)` that yields one event per second with:
   {"job_id": "...", "status": "processing", "percentage": 0}
   For now, stop after ten events and then send "completed".

3. Include Cache-Control: no-cache header.

Unit test using httpx.AsyncClient that reads the stream and counts ten events.

Commit: `feat(api): dummy SSE progress endpoint`.
```

### Prompt 8  `sse-frontend-hook`

```text
Front-end SSE hook.

**Task**
1. Create `frontend/src/hooks/useSse.ts`.
   - Accepts `url: string`.
   - Returns `{ data, error }` where `data` is latest JSON event.
2. Add a `ProgressBar` component that fills width based on `percentage`.
3. Update `UploadArea` to call `useSse("/progress/demo")` (hard-coded) after selecting files.
4. Show progress bar.

Jest test: mock `EventSource` to emit two events and assert bar width increases.

Commit: `feat(frontend): SSE hook and progress bar`.
```

### Prompt 9  `langgraph-skeleton`

```text
Introduce LangGraph.

**Task**
1. Add dep `langgraph`.
2. Create `backend/langgraph_nodes/{base.py, upload.py, extract.py, check_currency.py, convert.py, excel.py}`  each node defines `async def run(input: dict) -> dict` returning `input` unchanged.
3. `backend/langgraph_nodes/pipeline.py` builds `Graph()` linking nodes in the spec order.
4. Add unit test `test_pipeline_noop` that feeds sample dict and asserts identical output.

Commit: `feat(pipeline): LangGraph skeleton with pass-through nodes`.
```

---

## Business Logic (P-5 to P-7)

### Prompt 10  `azure-adapter`

```text
Real extraction.

**Task**
1. Install `azure-ai-documentintelligence` (pin minor version).
2. Create `backend/app/azure_adapter.py` exposing `async def extract_invoice(path: str) -> InvoiceData`.
   - `InvoiceData` dataclass with date, total, currency, vendor, filename.
3. Wrap SDK; map absent fields to `None`.
4. Unit tests with `pytest-vcr` cassette against a sample invoice PDF (place fixture in `tests/fixtures/`).

Commit: `feat(extraction): Azure Document Intelligence adapter`.
```

### Prompt 11  `currency-adapter`

```text
Frankfurter integration.

**Task**
1. Add `backend/app/currency.py`:
   - `async def get_rate(date: str, from_: str, to_: str) -> Decimal`.
   - 3-strike circuit breaker stored in module state.
2. Use `httpx.AsyncClient` with 2s timeout.
3. Unit tests:
   - Happy path (mocked response 1.2).
   - Failure increments counter; third failure raises `FrankfurterDown`.

Commit: `feat(currency): Frankfurter client with retry guard`.
```

### Prompt 12  `converter-node`

```text
Finish currency node.

**Task**
1. Implement `convert.py` node:
   - If `invoice.currency == target`, copy amount.
   - Else call `get_rate`, compute `Decimal(amount) * rate` rounded HALF_UP 2 dp.
2. Write unit tests covering same-currency shortcut and converted case.

Commit: `feat(node): currency converter logic`.
```

### Prompt 13  `excel-generator`

```text
Excel time.

**Task**
1. Implement `excel.py` node:
   - Accepts list of `InvoiceData`; create workbook with columns & styles per spec.
   - Save bytes to `BytesIO` and return `{"xlsx": bytes, "row_count": n}`.
2. Add helper `invoice_suffix`. Include placeholder ERROR rows.
3. Test using openpyxl to reopen bytes and assert header & row count.

Commit: `feat(node): openpyxl report generator`.
```

---

## Integration & UX (P-8 to P-9)

### Prompt 14  `pipeline-integration`

```text
Wire nodes end-to-end.

**Task**
1. Replace pass-through logic with calls to real adapters.
2. In `/process-invoices`:
   - Save upload to `uploads/`, create DB job row.
   - Kick off `asyncio.create_task(execute_pipeline(job_id, files, target_currency))`.
3. `execute_pipeline` streams progress to a queue consumed by SSE endpoint.
4. Update `/download/{job_id}` to read `exports/{job_id}.xlsx`.

Add integration test using three tiny invoices + `pytest-asyncio`.

Commit: `feat(api): full pipeline integration`.
```

### Prompt 15  `ui-currency-select`

```text
Frontend currency selector.

**Task**
1. Add `currencies.json` (ISO codes) under `public/`.
2. Build `<CurrencySelect>` using Headless-UI Listbox.
3. Integrate into `UploadArea`; default USD; send along in future POST.

Jest: render and pick EUR  expect callback.

Commit: `feat(frontend): currency dropdown`.
```

### Prompt 16  `ui-complete-flow`

```text
Hook UI to real API.

**Task**
1. Replace hard-coded SSE URL. After upload:
   - POST `FormData` with files + currency to `/process-invoices`.
   - Retrieve `{job_id}` and open `/progress/{job_id}` stream.
2. On `completed`, auto-download via `window.location = /download/{job_id}`.
3. Show inline file list with successes / failures.

E2E test (Playwright or Cypress) optional but stub spec included.

Commit: `feat(frontend): complete happy-path flow`.
```

---

## Production & Polish (P-10 to P-11)

### Prompt 17  `docker-prod`

```text
Containerization.

**Task**
1. Create `backend/Dockerfile` multi-stage:
   - Stage 1: `npm run build` in `/frontend`, copy `dist/` to `/app/static`.
   - Stage 2: slim Python base; COPY backend code + static; install deps; `CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "80"]`.
2. Root `docker-compose.yml` exposing port 8080.
3. Add README "Docker quick-start".

Commit: `feat(devops): production Docker image & compose`.
```

### Prompt 18  `edge-tests`

```text
Hardening.

**Task**
1. Add backend tests for:
   - Missing invoice date.
   - Azure extraction failure.
   - Third Frankfurter error abort.
2. Front-end Jest tests for:
   - 1 MB size limit rejection.
   - Progress bar shows 100% on completion.

Ensure coverage  80%. Update CI to fail if below.

Commit: `test(edge): increase coverage and confidence`.
```

### Prompt 19  `docs-polish`

```text
Final polish.

**Task**
1. Create `ARCHITECTURE.md` with diagram (ASCII ok).
2. Extend README with local dev instructions, env var table, and badges.
3. Add `CONTRIBUTING.md` explaining branching & TDD workflow.

Commit: `docs: polish and project overview`.
```

### Prompt 20  `release-v0.1.0`

```text
Tag the first release.

**Steps:**
1. Update `pyproject.toml` version  0.1.0.
2. Create `CHANGELOG.md` summarising all completed phases.
3. `git tag v0.1.0`.

Commit message: `chore(release): v0.1.0`.

Push tag to trigger GitHub release workflow (autogenerated zip).
```

---

*Generated from ChatGPT prompt plan - formatted for optimal Markdown display*
</file>

<file path="frontend/src/App.tsx">
import UploadArea from './components/UploadArea';

function App() {
  return (
    <div className="min-h-screen bg-gray-50 p-8">
      <div className="max-w-4xl mx-auto">
        <h1 className="text-3xl font-bold text-gray-900 mb-8">Invoice Converter</h1>
        <UploadArea />
      </div>
    </div>
  );
}

export default App;
</file>

<file path="frontend/src/setupTests.ts">
import '@testing-library/jest-dom';

// Mock EventSource globally for all tests
global.EventSource = jest.fn().mockImplementation(() => ({
  onmessage: null,
  onerror: null,
  readyState: 1,
  close: jest.fn(),
})) as unknown as typeof EventSource;
</file>

<file path="frontend/.gitignore">
# Logs
logs
*.log
npm-debug.log*
yarn-debug.log*
yarn-error.log*
pnpm-debug.log*
lerna-debug.log*

node_modules
dist
dist-ssr
*.local

# Test coverage
coverage

# Editor directories and files
.vscode/*
!.vscode/extensions.json
.idea
.DS_Store
*.suo
*.ntvs*
*.njsproj
*.sln
*.sw?
</file>

<file path="server.py">
#!/usr/bin/env python3
"""MCP Server entry point for Financial Tools MCP."""

import os
import sys

# Add the src directory to the Python path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), "src"))

from proto.mcp_tools.exchange_rate import mcp

if __name__ == "__main__":
    mcp.run()
</file>

<file path="spec.md">
# Invoice Converter - Technical Specification

## Table of Contents

1. [Project Goals](#1-project-goals)
2. [Tech Stack & Repository Layout](#2-tech-stack--repository-layout)
3. [Environment Variables](#3-environment-variables)
4. [Backend API Contract](#4-backend-api-contract)
5. [LangGraph Pipeline](#5-langgraph-pipeline)
6. [SQLite Schema](#6-sqlite-schema)
7. [Frontend Behaviour](#7-frontend-behaviour)
8. [Error Handling Summary](#8-error-handling-summary)
9. [Testing Plan](#9-testing-plan)
10. [Docker & Local Run](#10-docker--local-run)
11. [Open Tasks & Estimates](#11-open-tasks--estimates)

---

## 1. Project Goals

Build a localhost web app that lets a user upload up to 100 invoices (PDF / JPEG / PNG  1 MB each), extracts key fields via Azure Document Intelligence, converts totals to a user-chosen currency via the Frankfurter API, and returns a styled .xlsx workbook. Progress is streamed back to the browser with Server-Sent Events (SSE); a single batch is processed at a time.

## 2. Tech Stack & Repository Layout

### Technology Choices

| Area | Choice |
|------|--------|
| **Frontend** | React + Vite, Tailwind CSS, Headless UI (dropdown), React Dropzone (uploads), Axios + EventSource |
| **Backend** | Python 3.11, FastAPI + Uvicorn, LangGraph orchestration |
| **Data Extraction** | Azure Document Intelligence pre-built invoice model |
| **Currency API** | Frankfurter (no caching) |
| **Workbook** | openpyxl |
| **Database** | SQLite (job + file tables) via sqlalchemy |
| **Testing** | pytest, pytest-mock, hypothesis (property tests), Coverage  80%  Frontend: Jest + React Testing Library |
| **CI** | GitHub Actions  lint (ruff), format (black), run all tests, fail < 80% coverage |
| **Containers** | Single multi-stage Docker image (React build  served by FastAPI); .env mounted via docker-compose |

### Repository Structure

```
invoice-converter/
 backend/
   app/                  # FastAPI src
   langgraph_nodes/
   tests/
   Dockerfile
 frontend/
   src/
   public/
   vite.config.ts
 docker-compose.yml
 README.md                # lists required env vars
 progress_quips.json      # fun progress messages
```

## 3. Environment Variables

*Documented in README*

```env
AZURE_DOCUMENT_INTELLIGENCE_ENDPOINT
AZURE_DOCUMENT_INTELLIGENCE_API_KEY
AZURE_DOCUMENT_INTELLIGENCE_RESOURCE_ID   # optional quota check
```

## 4. Backend API Contract

### API Endpoints

| Verb | Path | Purpose |
|------|------|---------|
| **POST** | `/process-invoices` | Starts a batch. Multipart form fields:<br> `files`: up to 100 files (PDF/JPG/PNG,  1 MB each)<br> `target_currency`: ISO-4217, default USD<br>Returns `{"job_id": "<uuid>"}` immediately. |
| **GET** | `/progress/{job_id}` | SSE stream (`text/event-stream`). Emits JSON objects in the schema below. |
| **GET** | `/download/{job_id}` | Downloads the finished workbook `invoice_report_<timestamp>.xlsx`. |

### SSE Payload Schema

```json
{
  "job_id": "abc123",
  "status": "processing" | "completed" | "error",
  "current_step": "uploading" | "extracting" | "currency_check" | "currency_conversion" | "excel_generation",
  "processed": 3,
  "total": 5,
  "percentage": 60,
  "current_file": {
    "name": "invoice_003.pdf",
    "original_currency": "EUR",
    "target_currency": "USD",
    "status": "processing" | "converting" | "completed" | "failed"
  },
  "message": "Converting EUR to USD for invoice_003.pdf",
  "completed_files": [
    {"name": "invoice_001.pdf", "status": "success"},
    {"name": "invoice_002.pdf", "status": "success"}
  ],
  "error": null | { "file": "...", "message": "..." }
}
```

## 5. LangGraph Pipeline

### Sequential Processing Flow

1. **File Upload Node**  stores paths & target currency in the DB.

2. **Invoice Extractor Node**  Azure DI, returns InvoiceData dataclass.

3. **Currency Check Node**  decide if conversion is required.

4. **Exchange Rate Node**  `GET https://api.frankfurter.app/<date>?from=<original>&to=<target>`
   - Abort batch after 3 Frankfurter failures: mark remaining files ERROR, finalize workbook.

5. **Currency Converter Node**  `Decimal(amount) * rate`, half-up rounding to 2 dp.

6. **Excel Generator Node**  openpyxl workbook "Invoices Report", columns:

### Excel Output Format

| Date (DD/MM/YYYY) | Invoice Suffix | &lt;TARGET CUR&gt; Total Price | Foreign Currency Total Price | Foreign Currency Code | Exchange Rate (4 dp) | Vendor Name |
|-------------------|----------------|--------------------------------|------------------------------|----------------------|---------------------|-------------|

- Rows sorted by date ascending
- "no date found" rows last
- Ties keep upload order

### Invoice Suffix Extraction

- Strip non-digits, take last 4, left-pad zeros
- If no digits  `NO_INV_NUM`

### Placeholder Error Row

- Date=`"ERROR"`
- Invoice Suffix=`(filename)`
- All numeric/text cols `"N/A"`

## 6. SQLite Schema

```sql
CREATE TABLE jobs (
  job_id TEXT PRIMARY KEY,
  status TEXT,           -- processing/completed/error
  processed INTEGER,
  total INTEGER,
  created_at TIMESTAMP,
  updated_at TIMESTAMP
);

CREATE TABLE files (
  file_id INTEGER PRIMARY KEY AUTOINCREMENT,
  job_id  TEXT REFERENCES jobs(job_id),
  filename TEXT,
  status TEXT,           -- success/failed/unprocessed
  original_currency TEXT,
  target_currency TEXT,
  error_message TEXT
);
```

## 7. Frontend Behaviour

### Upload Page

- **React Dropzone** (accept `.pdf`, `.jpg`, `.jpeg`, `.png`; size  1 MB; max 100)
- **Currency selector**: ISO list from embedded JSON (defaults USD)
- **Click Process**  `POST /process-invoices`

### Progress Modal

- **Tailwind progress bar** fills via percentage
- **Random quips** pulled from `progress_quips.json`
- **Inline list of files** shows fail notices

### Completion

- **Auto-trigger** fetch `/download/{job_id}`
- **Fallback** "Download Again" button appears

## 8. Error Handling Summary

| Scenario | User-facing Effect | Excel Effect |
|----------|-------------------|--------------|
| **Azure DI fails on a file** | Quip + inline file error; SSE status=failed | Placeholder row |
| **Frankfurter fails < 3** | Same file error handling | Placeholder row |
| **3rd Frankfurter failure** | Batch aborts; progress stops at Excel generation | Remaining rows = ERROR |
| **Missing invoice date** | Date = "no date found"; skip conversion; leave Exchange Rate & Target Total blank | |
| **Target currency matches invoice currency** | No conversion; Exchange Rate blank | |

**Note**: Uploaded PDFs/images are removed immediately after processing; workbooks kept in `backend/exports/`.

## 9. Testing Plan

### Backend

- **Unit tests** per LangGraph node (pytest + hypothesis)
- **Integration test**: end-to-end batch with 3 mock invoices (2 OK, 1 error)
- **Edge cases**: missing date, missing total, NO_INV_NUM, 3 Frankfurter errors

### Frontend

**Component tests** (Jest) for:
- Currency dropdown
- File uploader limits
- Progress bar updates via mocked SSE

**Cypress or Playwright** smoke test optional.

## 10. Docker & Local Run

### Development

```bash
# Dev
cd backend && uvicorn app.main:app --reload  #  localhost:8000
cd frontend && npm run dev                   #  localhost:3000
```

### Production

```bash
# One-shot build & run
docker compose up --build    # mounts host .env
```

`docker-compose.yml` builds the React app in stage 1, copies `dist/` into the FastAPI image, then serves everything via Uvicorn at port 80.

## 11. Open Tasks & Estimates

| Task | Owner | Est. |
|------|-------|------|
| **Scaffold backend / frontend dirs** |  | 0.5 d |
| **Implement LangGraph nodes** |  | 1.5 d |
| **SSE & DB integration** |  | 1 d |
| **React UI (Tailwind)** |  | 1 d |
| **openpyxl Excel styling** |  | 0.5 d |
| **Tests (backend+frontend)** |  | 1 d |
| **GitHub Actions CI** |  | 0.5 d |
| **Dockerfile + compose** |  | 0.5 d |
| **Total** | | **6.5 d** |

---

*Generated from ChatGPT specification - formatted for optimal Markdown display*
</file>

<file path="frontend/jest.config.js">
export default {
  preset: 'ts-jest',
  testEnvironment: 'jsdom',
  setupFilesAfterEnv: ['<rootDir>/src/setupTests.ts'],
  moduleNameMapper: {
    '\\.(css|less|scss|sass)$': 'identity-obj-proxy',
  },
  transform: {
    '^.+\\.tsx?$': ['ts-jest', {
      tsconfig: {
        jsx: 'react-jsx',
      },
    }],
  },
  testMatch: [
    '<rootDir>/src/**/__tests__/**/*.(ts|tsx)',
    '<rootDir>/src/**/?(*.)(test|spec).(ts|tsx)'
  ],
  moduleFileExtensions: ['ts', 'tsx', 'js', 'jsx', 'json', 'node'],
};
</file>

<file path="frontend/package.json">
{
  "name": "frontend",
  "private": true,
  "version": "0.0.0",
  "type": "module",
  "scripts": {
    "dev": "vite",
    "build": "tsc -b && vite build",
    "lint": "eslint .",
    "preview": "vite preview",
    "test": "jest"
  },
  "dependencies": {
    "react": "^19.1.0",
    "react-dom": "^19.1.0",
    "react-dropzone": "^14.3.8"
  },
  "devDependencies": {
    "@eslint/js": "^9.30.1",
    "@testing-library/jest-dom": "^6.6.3",
    "@testing-library/react": "^16.3.0",
    "@testing-library/user-event": "^14.6.1",
    "@types/jest": "^30.0.0",
    "@types/react": "^19.1.8",
    "@types/react-dom": "^19.1.6",
    "@types/testing-library__jest-dom": "^5.14.9",
    "@vitejs/plugin-react": "^4.6.0",
    "autoprefixer": "^10.4.21",
    "eslint": "^9.30.1",
    "eslint-plugin-react-hooks": "^5.2.0",
    "eslint-plugin-react-refresh": "^0.4.20",
    "globals": "^16.3.0",
    "identity-obj-proxy": "^3.0.0",
    "jest": "^30.0.4",
    "jest-environment-jsdom": "^30.0.4",
    "postcss": "^8.5.6",
    "tailwindcss": "^4.1.11",
    "ts-jest": "^29.4.0",
    "typescript": "~5.8.3",
    "typescript-eslint": "^8.35.1",
    "vite": "^7.0.4"
  }
}
</file>

<file path=".gitignore">
# Editor directories and files
.vscode/
.idea/
*.swp
*.swo

# OS
.DS_Store
._*

# Project specific
uploads/
exports/
*.db
invoice.db
notes.txt
tmp/
temp/
</file>

<file path=".github/workflows/ci.yml">
name: CI

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main, develop]

jobs:
  backend-tests:
    runs-on: ubuntu-latest
    defaults:
      run:
        working-directory: ./backend

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python 3.11
        uses: actions/setup-python@v4
        with:
          python-version: "3.11"

      - name: Install Poetry
        uses: snok/install-poetry@v1
        with:
          version: latest
          virtualenvs-create: true
          virtualenvs-in-project: true

      - name: Load cached venv
        id: cached-poetry-dependencies
        uses: actions/cache@v3
        with:
          path: backend/.venv
          key: venv-${{ runner.os }}-${{ hashFiles('backend/poetry.lock') }}

      - name: Install dependencies
        if: steps.cached-poetry-dependencies.outputs.cache-hit != 'true'
        run: poetry install --no-root --without proto --no-interaction

      - name: Run tests with pytest (excluding proto)
        run: poetry run pytest -m "not proto" --cov --cov-report=xml

      - name: Check Python coverage threshold
        run: |
          COVERAGE=$(poetry run coverage report --show-missing | grep TOTAL | awk '{print $4}' | sed 's/%//')
          echo "Python Coverage: $COVERAGE%"
          if (( $(echo "$COVERAGE < 80" | bc -l) )); then
            echo "Error: Python coverage $COVERAGE% is below required 80%"
            exit 1
          fi

      - name: Run Ruff linting
        run: poetry run ruff check .

      - name: Run Black formatting check
        run: poetry run black --check .

      - name: Run isort import sorting check
        run: poetry run isort --check-only .

      - name: Upload backend coverage
        uses: actions/upload-artifact@v4
        with:
          name: backend-coverage
          path: backend/coverage.xml

  frontend-tests:
    runs-on: ubuntu-latest
    defaults:
      run:
        working-directory: ./frontend

    steps:
      - uses: actions/checkout@v4

      - name: Set up Node.js 20
        uses: actions/setup-node@v4
        with:
          node-version: "20"
          cache: 'npm'
          cache-dependency-path: frontend/package-lock.json

      - name: Install dependencies
        run: npm ci

      - name: Run ESLint
        run: npm run lint

      - name: Run tests with coverage
        run: npm test -- --watchAll=false --coverage

      - name: Check Jest coverage threshold
        run: |
          if [ ! -f coverage/lcov.info ]; then
            echo "Error: Coverage file not found"
            exit 1
          fi
          
          # Parse coverage from lcov.info and check if >= 80%
          COVERAGE=$(grep -o "LF:[0-9]*" coverage/lcov.info | cut -d: -f2 | awk '{total+=$1} END {print total}')
          HIT=$(grep -o "LH:[0-9]*" coverage/lcov.info | cut -d: -f2 | awk '{total+=$1} END {print total}')
          
          if [ "$COVERAGE" -gt 0 ]; then
            PERCENTAGE=$(echo "scale=1; $HIT * 100 / $COVERAGE" | bc -l)
            echo "Coverage: $PERCENTAGE%"
            if (( $(echo "$PERCENTAGE < 80" | bc -l) )); then
              echo "Error: Coverage $PERCENTAGE% is below required 80%"
              exit 1
            fi
          fi

      - name: Upload frontend coverage
        uses: actions/upload-artifact@v4
        with:
          name: frontend-coverage
          path: frontend/coverage/
</file>

<file path="README.md">
# Financial Tools MCP

A comprehensive financial tools suite featuring an MCP server for exchange rates and a standalone invoice data extractor.

## Components

### 1. Exchange Rate MCP Server
- **Multi-Currency Support**: `get_exchange_rate(date: str, from_currency: str = "USD", to_currency: str = "ILS")  dict`
- **Flexible Date Input**: Accepts dates in various formats (YYYY-MM-DD, MM/DD/YYYY, "July 10, 2025", etc.)
- **Reliable Data Source**: Uses the Frankfurter API for historical exchange rates
- **Error Handling**: Proper exception handling for invalid dates, network errors, and API failures

### 2. Invoice Data Extractor
- **Azure Document Intelligence**: Processes PDF invoices using Azure's prebuilt invoice model
- **Structured Data Extraction**: Extracts date, invoice suffix, price, currency, and company information
- **Usage Monitoring**: Optional quota tracking for Azure F0 tier (500 pages/month limit)
- **Environment Configuration**: Uses `.env` file for Azure credentials

## Installation

### Option 1: Local Installation
```bash
# Install with pip
pip install -e .

# Or with dev dependencies
pip install -e .[dev]
```

### Option 2: Development Container (Recommended)
```bash
# Using VS Code Dev Containers extension
# 1. Open project in VS Code
# 2. Press Ctrl+Shift+P
# 3. Select "Dev Containers: Reopen in Container"

# Or using Docker Compose
docker-compose -f .devcontainer/docker-compose.yml up -d
```

## Usage

### 1. MCP Server (Exchange Rates)

Start the MCP server:
```bash
python server.py
```

**As a Python Module:**
```python
from mcp_tools.exchange_rate import mcp

# Get the exchange rate function
get_exchange_rate_func = None
for tool in mcp._tool_manager._tools.values():
    if hasattr(tool, "fn") and tool.fn.__name__ == "get_exchange_rate":
        get_exchange_rate_func = tool.fn
        break

# Default: USD to ILS
result = get_exchange_rate_func("2025-07-10")
print(result)
#  {"amount": 1.0, "base": "USD", "date": "2025-07-10", "rates": {"ILS": 3.3064}}

# Custom currencies: EUR to GBP
result = get_exchange_rate_func("2025-07-10", "EUR", "GBP")
print(result)
#  {"amount": 1.0, "base": "EUR", "date": "2025-07-10", "rates": {"GBP": 0.8520}}
```

### 2. Invoice Data Extractor

**Setup Environment Variables (.env file):**
```bash
AZURE_DOCUMENT_INTELLIGENCE_ENDPOINT=https://your-resource.cognitiveservices.azure.com/
AZURE_DOCUMENT_INTELLIGENCE_API_KEY=your-api-key-here
AZURE_DOCUMENT_INTELLIGENCE_RESOURCE_ID=/subscriptions/.../resourceGroups/.../providers/Microsoft.CognitiveServices/accounts/your-resource  # Optional for quota monitoring
```

**Use as Python Module:**
```python
from src.extractors.invoice_extractor import extract_invoice_data_azure, check_usage_quota

# Check current usage (optional)
usage = check_usage_quota()
if usage:
    print(f"Pages used: {usage['used']}/{usage['total_limit']}")

# Extract invoice data
invoice_data = extract_invoice_data_azure("path/to/invoice.pdf")
if invoice_data:
    print(f"Date: {invoice_data.InvoiceDate.content}")
    print(f"Invoice ID: {invoice_data.InvoiceId.content}")
    print(f"Price: {invoice_data.InvoiceTotal.value_currency.amount} {invoice_data.InvoiceTotal.value_currency.currency_code}")
    print(f"Company: {invoice_data.VendorName.content or invoice_data.VendorAddressRecipient.content}")
```

**Direct Execution:**
```bash
python src/extractors/invoice_extractor.py  # Processes payment.pdf in current directory
```

## Project Structure

```
src/
 mcp_tools/           # MCP server components
    exchange_rate.py # Exchange rate tool using FastMCP framework
 extractors/          # Data extraction utilities
     invoice_extractor.py # Azure Document Intelligence integration

tests/
 test_get_exchange_rate.py      # Exchange rate tests
 test_azure_invoice_extractor.py # Invoice extractor tests
```

## Data Models

**Exchange Rate Response:**
- Returns dictionary with `amount`, `base`, `date`, and `rates` fields

**Invoice Data Structure:**
- `InvoiceData`: Main dataclass containing all extracted invoice fields
- `DefaultContent`: Text fields with `.content` attribute (InvoiceDate, InvoiceId, VendorName, VendorAddressRecipient)
- `InvoiceTotal`: Invoice total with `.value_currency` (ValueCurrency object) and `.content` attributes
- `ValueCurrency`: Monetary values with `.amount` (float) and `.currency_code` (string) attributes

## Error Handling

**Exchange Rate Tool:**
- Invalid date formats raise `ValueError`
- Network timeouts and API errors raise `Exception` with detailed error messages
- Non-200 HTTP responses include the status code and response body

**Invoice Extractor:**
- Missing Azure credentials return `None` with helpful error messages
- Azure API errors are caught and logged with graceful fallback
- File not found errors are handled with clear user feedback

## Development

```bash
# Install development dependencies
pip install -e ".[dev]"

# Format code
black .

# Lint code
ruff check .

# Run all tests
pytest

# Run specific test files
pytest tests/test_get_exchange_rate.py
pytest tests/test_azure_invoice_extractor.py

# Run specific test classes
pytest tests/test_azure_invoice_extractor.py::TestInvoiceData
```

## Requirements

- **Python**: >= 3.12
- **Core Dependencies**: `requests`, `python-dateutil`, `mcp`, `azure-ai-documentintelligence`, `python-dotenv`
- **Development**: `black`, `ruff`, `pytest`, `requests-mock`
- **Azure Setup**: Document Intelligence resource (F0 tier sufficient for testing)

## License

MIT License - see LICENSE file for details.
</file>

<file path="backend/pyproject.toml">
[build-system]
requires = ["poetry-core"]
build-backend = "poetry.core.masonry.api"

[tool.poetry]
name = "invoice-converter"
version = "0.1.0"
description = "Invoice processing and currency conversion web application"
authors = ["Invoice Converter Team <team@invoice-converter.com>"]
readme = "README.md"
packages = [{include = "app"}]

[tool.poetry.dependencies]
python = "^3.11"
fastapi = "^0.110.0"
uvicorn = {extras = ["standard"], version = "^0.30.0"}
alembic = "^1.13.0"
sqlalchemy = "^2.0.0"
langgraph = "^0.2.0"
azure-ai-documentintelligence = "~1.0.0"
python-dotenv = "^1.0.0"
httpx = "^0.27.0"
python-dateutil = "^2.8.2"

[tool.poetry.group.proto.dependencies]
# Heavy dependencies for exploratory/proto code
mcp = "^1.0.0"
requests = "^2.31.0"
python-dateutil = "^2.8.2"
openpyxl = "^3.1.0"
langchain = "^0.2.0"

[tool.poetry.group.dev.dependencies]
pytest = "^7.4.3"
pytest-cov = "^4.1.0"
pytest-mock = "^3.12.0"
pytest-asyncio = "^0.21.0"
pytest-vcr = "^1.0.2"
httpx = "^0.27.0"
respx = "^0.21.0"
black = "^23.11.0"
ruff = "^0.1.6"
isort = "^5.12.0"
pre-commit = "^3.5.0"

[tool.black]
line-length = 120
target-version = ['py311']
include = '\.pyi?$'
extend-exclude = '''
/(
  # directories
  \.eggs
  | \.git
  | \.hg
  | \.mypy_cache
  | \.tox
  | \.venv
  | build
  | dist
)/
'''

[tool.ruff]
line-length = 120
target-version = "py311"

[tool.ruff.lint]
select = [
    "E",  # pycodestyle errors
    "W",  # pycodestyle warnings
    "F",  # pyflakes
    "I",  # isort
    "B",  # flake8-bugbear
    "C4", # flake8-comprehensions
    "UP", # pyupgrade
]
ignore = [
    "E501",  # line too long, handled by black
    "B008",  # do not perform function calls in argument defaults
    "C901",  # too complex
]

[tool.ruff.lint.per-file-ignores]
"__init__.py" = ["F401"]

[tool.isort]
profile = "black"
line_length = 120
</file>

<file path="CLAUDE.md">
# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Commands

**Development Setup:**
```bash
# MCP Server (uses uv package manager)
pip install -e ".[dev]"  # Install with dev dependencies

# Backend Web App (uses Poetry)
cd backend && poetry install  # Install backend dependencies
cd backend && poetry install --with=proto  # Include prototype dependencies

# Frontend Web App
cd frontend && npm install  # Install frontend dependencies
```

**Code Quality:**
```bash
# Backend (from backend/ directory)
poetry run black .        # Format code (line-length 120)
poetry run ruff check .   # Lint code
poetry run pytest         # Run all tests
poetry run pytest tests/test_currency.py  # Run specific test file
poetry run pytest tests/test_azure_adapter.py::TestInvoiceData  # Run specific test class

# Frontend
cd frontend && npm run lint         # ESLint checking
cd frontend && npm test -- --watchAll=false --coverage  # Run tests with coverage
cd frontend && npm run build        # TypeScript compile and build
```

**Run Services:**
```bash
python server.py  # Start the MCP server

# Web application:
cd backend && poetry run uvicorn app.main:app --reload  # Backend dev server
cd frontend && npm run dev  # Frontend dev server

# Database migrations:
cd backend && poetry run alembic upgrade head  # Apply migrations
cd backend && poetry run alembic revision --autogenerate -m "description"  # Create migration
```

## Architecture

This is a financial tools suite with a monorepo structure containing an MCP server implementation and a React + FastAPI web application for invoice processing:

### Project Structure:
```
/financial-tools-mcp/
 server.py                    # MCP server entry point
 backend/                     # FastAPI web application
    app/                     # FastAPI application code
       main.py              # FastAPI app with health and SSE endpoints
       models.py            # SQLAlchemy Job and File models
       azure_adapter.py     # Azure Document Intelligence integration
       currency.py          # Frankfurter API client with circuit breaker
       db.py                # Database session management
    langgraph_nodes/         # LangGraph processing pipeline
    tests/                   # Backend unit tests
    pyproject.toml           # Poetry configuration
 frontend/                    # React web application
     src/                     # React components and logic
        components/          # Upload and progress components
        hooks/               # SSE integration hook
     package.json             # npm configuration
     __tests__/               # Frontend unit tests
```

### Current Implementation:

**Core Backend Services:**
1. **Currency Conversion**: Modern async Frankfurter API client (`app/currency.py`)
   - `async def get_rate(date: str, from_: str, to_: str) -> Decimal`
   - 3-strike circuit breaker with module-level state tracking
   - 2-second timeout with httpx AsyncClient
   - Comprehensive error handling and date normalization

2. **Azure Document Intelligence**: Invoice extraction (`app/azure_adapter.py`)
   - Hybrid dataclass architecture: robust internal structure + simple API format
   - `async def extract_invoice(path: str) -> InvoiceData | None`
   - `def to_simple_format(invoice: InvoiceData, filename: str) -> SimpleInvoiceData`
   - Full Azure SDK integration with proper async patterns

3. **FastAPI Backend**: REST API with real-time capabilities
   - Health checks and SSE endpoints implemented
   - SQLite database with Job/File models and Alembic migrations
   - Async request handling with proper type hints

**Frontend (React + TypeScript):**
- **File Upload**: Drag-and-drop interface supporting PDF/JPEG/PNG (1MB, max 100 files)
- **Real-time Progress**: Server-Sent Events integration with `useSse` hook
- **Progress Visualization**: Animated progress bars with percentage display
- **Type Safety**: Full TypeScript coverage with strict ESLint rules
- **Testing**: Jest + React Testing Library with 92%+ coverage

### Key Architectural Patterns:

**Hybrid Data Architecture (Azure Adapter):**
The Azure adapter uses a dual-format approach:
- **Internal**: Robust nested dataclasses (`InvoiceData`, `DefaultContent`, `ValueCurrency`)
- **API**: Simplified format (`SimpleInvoiceData`) for JSON serialization
- **Conversion**: Helper functions bridge between formats for different use cases

**Circuit Breaker Pattern (Currency Service):**
```python
# Module-level failure tracking
_failure_count = 0
_max_failures = 2  # Block on 3rd attempt

async def get_rate(date: str, from_: str, to_: str) -> Decimal:
    if _failure_count >= _max_failures:
        raise FrankfurterDown("API is down after consecutive failures")
    # ... API call with failure count management
```

**Server-Sent Events Architecture:**
```typescript
// Frontend: Type-safe SSE hook with generic data typing
const { data: progressData } = useSse<ProgressData>(
  selectedFiles.length > 0 ? '/progress/demo' : ''
);

// Backend: Async generator pattern for streaming
async def generate():
    for step in range(11):
        yield f"data: {json.dumps(progress_data)}\n\n"
        await asyncio.sleep(0.5)
```

### Data Models:

**Currency Service:**
- Uses `Decimal` for precise monetary calculations
- Date normalization with `dateutil.parser` for flexible input formats
- Circuit breaker exceptions: `FrankfurterDown` after 3 consecutive failures

**Invoice Data Models:**
- `InvoiceData`: Nested structure matching Azure response format
- `SimpleInvoiceData`: Flat structure for API responses with optional fields
- Conversion helpers maintain data integrity across format boundaries

**Database Models (SQLAlchemy 2.0):**
- `Job`: Processing job tracking with status and metadata
- `File`: Individual file tracking within jobs
- Async session management with proper transaction handling

### Testing Strategy:

**Backend Testing (39 tests):**
- **Currency Module**: httpx mocking with respx, circuit breaker verification
- **Azure Adapter**: Mock Azure SDK calls, data format conversion testing
- **FastAPI Endpoints**: TestClient with async support, database mocking
- **Integration**: Real API call recording with pytest-vcr (when available)

**Frontend Testing (92%+ coverage):**
- **Component Testing**: Jest + React Testing Library for UI components
- **Hook Testing**: EventSource mocking for SSE functionality
- **Type Safety**: Full TypeScript coverage with strict linting

### Environment Setup:

**Required Environment Variables:**
```bash
# Azure Document Intelligence (required for invoice extraction)
AZURE_DOCUMENT_INTELLIGENCE_ENDPOINT=https://your-resource.cognitiveservices.azure.com/
AZURE_DOCUMENT_INTELLIGENCE_API_KEY=your-api-key

# Optional: For usage monitoring
AZURE_DOCUMENT_INTELLIGENCE_RESOURCE_ID=your-resource-id
```

**Development Dependencies:**
- **Backend**: Poetry with Python 3.11+, async libraries (httpx, respx for testing)
- **Frontend**: Node.js with React 19, TypeScript, Tailwind CSS 4
- **Database**: SQLite with Alembic migrations
- **Code Quality**: ruff + black for Python, ESLint + Prettier for TypeScript

## Development Workflow

**Testing Patterns:**
```bash
# Run specific service tests
poetry run pytest tests/test_currency.py -v
poetry run pytest tests/test_azure_adapter.py::TestConversionHelpers -v

# Test coverage and quality
poetry run pytest --cov=app tests/
poetry run ruff check . && poetry run black . --check

# Frontend testing
cd frontend && npm test -- --coverage --watchAll=false
```

**Common Development Tasks:**
1. **Adding New Endpoints**: Follow FastAPI async patterns in `app/main.py`
2. **Database Changes**: Use Alembic autogenerate for schema migrations
3. **Frontend Components**: Implement with TypeScript + testing in parallel
4. **API Integration**: Use circuit breaker pattern for external service calls

**Architecture Decision Records:**
- **Currency Service**: Chose httpx over requests for async compatibility
- **Azure Integration**: Hybrid data format balances robustness vs. API simplicity  
- **SSE Implementation**: EventSource over WebSockets for simpler real-time updates
- **Testing Strategy**: Mock external APIs, focus on integration patterns

## Next Implementation Steps

**Immediate priorities:**
- LangGraph pipeline orchestration connecting Azure + Currency services
- File upload endpoints with job queue management
- Excel report generation with styled output
- Production deployment configuration

**Pipeline Integration:**
The `langgraph_nodes/` directory contains the processing pipeline structure that will orchestrate:
1. File upload and validation
2. Azure document extraction
3. Currency conversion
4. Excel report generation
5. Progress tracking via SSE

This system is designed for scalable invoice processing with real-time user feedback and robust error handling across all integration points.
</file>

</files>
