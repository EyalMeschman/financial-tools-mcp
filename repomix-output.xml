This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
alembic/
  versions/
    75399a20793f_add_converted_total_and_exchange_rate_.py
    cd3912ae6d70_initial_migration_job_and_file_tables.py
  env.py
  README
  script.py.mako
app/
  azure_adapter.py
  currency.py
  db.py
  main.py
  models.py
langgraph_nodes/
  base.py
  check_currency.py
  convert.py
  excel.py
  extract.py
  pipeline.py
  upload.py
tests/
  fixtures/
    README.md
  __init__.py
  test_azure_adapter.py
  test_convert_node.py
  test_currency.py
  test_db_models.py
  test_excel_node.py
  test_health.py
  test_integration.py
  test_pipeline.py
  test_placeholder.py
  test_sse_progress.py
.gitignore
alembic.ini
Dockerfile
poetry.toml
pyproject.toml
README.md
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="alembic/versions/75399a20793f_add_converted_total_and_exchange_rate_.py">
"""add converted_total and exchange_rate columns

Revision ID: 75399a20793f
Revises: cd3912ae6d70
Create Date: 2025-07-14 14:30:00.000000

"""
from collections.abc import Sequence

import sqlalchemy as sa

from alembic import op

# revision identifiers, used by Alembic.
revision: str = "75399a20793f"
down_revision: str | Sequence[str] | None = "cd3912ae6d70"
branch_labels: str | Sequence[str] | None = None
depends_on: str | Sequence[str] | None = None


def upgrade() -> None:
    """Add converted_total and exchange_rate columns to files table."""
    # Add new columns for currency conversion tracking
    op.add_column("files", sa.Column("converted_total", sa.Numeric(10, 2), nullable=True))
    op.add_column("files", sa.Column("exchange_rate", sa.Numeric(10, 6), nullable=True))

    # Add indexes for the new columns
    op.create_index(op.f("ix_files_converted_total"), "files", ["converted_total"], unique=False)
    op.create_index(op.f("ix_files_exchange_rate"), "files", ["exchange_rate"], unique=False)

    # Add index for status column
    op.create_index(op.f("ix_files_status"), "files", ["status"], unique=False)


def downgrade() -> None:
    """Remove converted_total and exchange_rate columns from files table."""
    # Remove indexes
    op.drop_index(op.f("ix_files_status"), table_name="files")
    op.drop_index(op.f("ix_files_exchange_rate"), table_name="files")
    op.drop_index(op.f("ix_files_converted_total"), table_name="files")

    # Remove columns
    op.drop_column("files", "exchange_rate")
    op.drop_column("files", "converted_total")
</file>

<file path="alembic/versions/cd3912ae6d70_initial_migration_job_and_file_tables.py">
"""Initial migration: Job and File tables

Revision ID: cd3912ae6d70
Revises:
Create Date: 2025-07-13 18:39:51.539338

"""
from collections.abc import Sequence

import sqlalchemy as sa

from alembic import op

# revision identifiers, used by Alembic.
revision: str = "cd3912ae6d70"
down_revision: str | Sequence[str] | None = None
branch_labels: str | Sequence[str] | None = None
depends_on: str | Sequence[str] | None = None


def upgrade() -> None:
    """Upgrade schema."""
    # ### commands auto generated by Alembic - please adjust! ###
    op.create_table(
        "jobs",
        sa.Column("job_id", sa.String(), nullable=False),
        sa.Column("status", sa.String(), nullable=False),
        sa.Column("processed", sa.Integer(), nullable=True),
        sa.Column("total", sa.Integer(), nullable=False),
        sa.Column("created_at", sa.DateTime(), nullable=True),
        sa.Column("updated_at", sa.DateTime(), nullable=True),
        sa.PrimaryKeyConstraint("job_id"),
    )
    op.create_table(
        "files",
        sa.Column("id", sa.Integer(), autoincrement=True, nullable=False),
        sa.Column("job_id", sa.String(), nullable=False),
        sa.Column("filename", sa.String(), nullable=False),
        sa.Column("status", sa.String(), nullable=False),
        sa.Column("original_currency", sa.String(), nullable=True),
        sa.Column("target_currency", sa.String(), nullable=True),
        sa.Column("error_message", sa.String(), nullable=True),
        sa.ForeignKeyConstraint(
            ["job_id"],
            ["jobs.job_id"],
        ),
        sa.PrimaryKeyConstraint("id"),
    )
    # ### end Alembic commands ###


def downgrade() -> None:
    """Downgrade schema."""
    # ### commands auto generated by Alembic - please adjust! ###
    op.drop_table("files")
    op.drop_table("jobs")
    # ### end Alembic commands ###
</file>

<file path="alembic/env.py">
import os
from logging.config import fileConfig

from sqlalchemy import engine_from_config, pool

from alembic import context

# Import our models
from app.models import Base

# this is the Alembic Config object, which provides
# access to the values within the .ini file in use.
config = context.config

# Interpret the config file for Python logging.
# This line sets up loggers basically.
if config.config_file_name is not None:
    fileConfig(config.config_file_name)

# add your model's MetaData object here
# for 'autogenerate' support
target_metadata = Base.metadata

# Set the database URL from environment if not in config
database_url = os.getenv("DATABASE_URL", "sqlite:///./invoice.db")
config.set_main_option("sqlalchemy.url", database_url)

# other values from the config, defined by the needs of env.py,
# can be acquired:
# my_important_option = config.get_main_option("my_important_option")
# ... etc.


def run_migrations_offline() -> None:
    """Run migrations in 'offline' mode.

    This configures the context with just a URL
    and not an Engine, though an Engine is acceptable
    here as well.  By skipping the Engine creation
    we don't even need a DBAPI to be available.

    Calls to context.execute() here emit the given string to the
    script output.

    """
    url = config.get_main_option("sqlalchemy.url")
    context.configure(
        url=url,
        target_metadata=target_metadata,
        literal_binds=True,
        dialect_opts={"paramstyle": "named"},
    )

    with context.begin_transaction():
        context.run_migrations()


def run_migrations_online() -> None:
    """Run migrations in 'online' mode.

    In this scenario we need to create an Engine
    and associate a connection with the context.

    """
    connectable = engine_from_config(
        config.get_section(config.config_ini_section, {}),
        prefix="sqlalchemy.",
        poolclass=pool.NullPool,
    )

    with connectable.connect() as connection:
        context.configure(connection=connection, target_metadata=target_metadata)

        with context.begin_transaction():
            context.run_migrations()


if context.is_offline_mode():
    run_migrations_offline()
else:
    run_migrations_online()
</file>

<file path="alembic/README">
Generic single-database configuration.
</file>

<file path="alembic/script.py.mako">
"""${message}

Revision ID: ${up_revision}
Revises: ${down_revision | comma,n}
Create Date: ${create_date}

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa
${imports if imports else ""}

# revision identifiers, used by Alembic.
revision: str = ${repr(up_revision)}
down_revision: Union[str, Sequence[str], None] = ${repr(down_revision)}
branch_labels: Union[str, Sequence[str], None] = ${repr(branch_labels)}
depends_on: Union[str, Sequence[str], None] = ${repr(depends_on)}


def upgrade() -> None:
    """Upgrade schema."""
    ${upgrades if upgrades else "pass"}


def downgrade() -> None:
    """Downgrade schema."""
    ${downgrades if downgrades else "pass"}
</file>

<file path="app/azure_adapter.py">
"""Azure Document Intelligence adapter for invoice extraction."""

import os
from dataclasses import dataclass
from pathlib import Path

from azure.ai.documentintelligence.aio import DocumentIntelligenceClient
from azure.core.credentials import AzureKeyCredential
from dotenv import load_dotenv

load_dotenv()

# Confidence threshold for accepting extracted data
CONFIDENCE_THRESHOLD = 0.8
LOW_CONFIDENCE_PLACEHOLDER = "CONFIDENCE_TOO_LOW"


# Robust internal dataclasses (preserving original structure)
@dataclass
class DefaultContent:
    """Text content with optional confidence."""

    content: str
    confidence: float


@dataclass
class ValueCurrency:
    """Monetary value with currency code."""

    amount: float
    currency_code: str


@dataclass
class InvoiceTotal:
    """Invoice total with structured and text content."""

    value_currency: ValueCurrency | None
    content: str
    confidence: float


@dataclass
class InvoiceData:
    """Complete invoice data structure matching Azure response."""

    InvoiceDate: DefaultContent | None
    InvoiceId: DefaultContent | None
    InvoiceTotal: InvoiceTotal | None
    VendorName: DefaultContent | None
    VendorAddressRecipient: DefaultContent | None


# Simple output format for web API
@dataclass
class SimpleInvoiceData:
    """Simplified invoice data structure for API responses."""

    date: str | None = None
    total: float | None = None
    currency: str | None = None
    vendor: str | None = None
    filename: str | None = None


async def extract_invoice(path: str) -> InvoiceData | None:
    """Extract invoice data using Azure Document Intelligence.

    Args:
        path: Path to the invoice file (PDF, JPEG, PNG)

    Returns:
        InvoiceData: Extracted invoice data or None if extraction fails
    """
    # Get credentials from environment
    endpoint = os.getenv("AZURE_DOCUMENT_INTELLIGENCE_ENDPOINT")
    api_key = os.getenv("AZURE_DOCUMENT_INTELLIGENCE_API_KEY")

    if not endpoint or not api_key:
        print("Error: Missing Azure Document Intelligence credentials in .env file")
        print("Required variables:")
        print("- AZURE_DOCUMENT_INTELLIGENCE_ENDPOINT")
        print("- AZURE_DOCUMENT_INTELLIGENCE_API_KEY")
        return None

    # Check if file exists
    file_path = Path(path)
    if not file_path.exists():
        print(f"Error: File '{path}' not found")
        return None

    try:
        # Initialize async client
        async with DocumentIntelligenceClient(endpoint=endpoint, credential=AzureKeyCredential(api_key)) as client:
            # Read file
            with open(path, "rb") as file:
                file_data = file.read()

            print(f"Processing {path} with Azure Document Intelligence...")

            # Analyze document using prebuilt invoice model
            poller = await client.begin_analyze_document("prebuilt-invoice", file_data, content_type="application/pdf")
            # poller = await client.begin_analyze_document("prebuilt-receipt", file_data, content_type="application/pdf")

            result = await poller.result()

            # Extract data from Azure response
            invoice_data = _extract_from_azure_response(result)

            return invoice_data

    except Exception as e:
        print(f"Error processing document: {e}")
        return None


def _extract_from_azure_response(azure_result) -> InvoiceData | None:
    """Extract structured data from Azure Document Intelligence response."""
    if not azure_result.documents:
        return None

    invoice = azure_result.documents[0]
    fields = invoice.fields

    # Extract content from Azure field objects with proper structure
    invoice_date = None
    if "InvoiceDate" in fields and fields["InvoiceDate"]:
        content = getattr(fields["InvoiceDate"], "content", "")
        confidence = getattr(fields["InvoiceDate"], "confidence", 0.0)
        if content:
            invoice_date = DefaultContent(content=LOW_CONFIDENCE_PLACEHOLDER, confidence=confidence)
            if confidence > CONFIDENCE_THRESHOLD:
                invoice_date = DefaultContent(content=content, confidence=confidence)

    invoice_id = None
    if "InvoiceId" in fields and fields["InvoiceId"]:
        content = getattr(fields["InvoiceId"], "content", "")
        confidence = getattr(fields["InvoiceId"], "confidence", 0.0)
        if content:
            invoice_id = DefaultContent(content=LOW_CONFIDENCE_PLACEHOLDER, confidence=confidence)
            if confidence > CONFIDENCE_THRESHOLD:
                invoice_id = DefaultContent(content=content, confidence=confidence)

    invoice_total = None
    if "InvoiceTotal" in fields and fields["InvoiceTotal"]:
        total_field: InvoiceTotal = fields["InvoiceTotal"]
        content = getattr(total_field, "content", "")
        confidence = getattr(total_field, "confidence", 0.0)
        if content or (hasattr(total_field, "value_currency") and total_field.value_currency):
            value_currency = ValueCurrency(amount=0.0, currency_code=LOW_CONFIDENCE_PLACEHOLDER)
            invoice_total = InvoiceTotal(
                value_currency=value_currency, content=LOW_CONFIDENCE_PLACEHOLDER, confidence=confidence
            )
            if confidence > CONFIDENCE_THRESHOLD:
                value_currency = None
                if hasattr(total_field, "value_currency") and total_field.value_currency:
                    amount = getattr(total_field.value_currency, "amount", 0.0)
                    currency_code = getattr(total_field.value_currency, "currency_code", "")
                    if amount or currency_code:
                        value_currency = ValueCurrency(amount=amount, currency_code=currency_code)
                invoice_total = InvoiceTotal(value_currency=value_currency, content=content, confidence=confidence)

    vendor_name = None
    if "VendorName" in fields and fields["VendorName"]:
        content = getattr(fields["VendorName"], "content", "")
        confidence = getattr(fields["VendorName"], "confidence", 0.0)
        if content:
            vendor_name = DefaultContent(content=LOW_CONFIDENCE_PLACEHOLDER, confidence=confidence)
            if confidence > CONFIDENCE_THRESHOLD:
                vendor_name = DefaultContent(content=content, confidence=confidence)

    vendor_address = None
    if not vendor_name and "VendorAddressRecipient" in fields and fields["VendorAddressRecipient"]:
        content = getattr(fields["VendorAddressRecipient"], "content", "")
        confidence = getattr(fields["VendorAddressRecipient"], "confidence", 0.0)
        if content:
            vendor_address = DefaultContent(content=LOW_CONFIDENCE_PLACEHOLDER, confidence=confidence)
            if confidence > CONFIDENCE_THRESHOLD:
                vendor_address = DefaultContent(content=content, confidence=confidence)

    return InvoiceData(
        InvoiceDate=invoice_date,
        InvoiceId=invoice_id,
        InvoiceTotal=invoice_total,
        VendorName=vendor_name,
        VendorAddressRecipient=vendor_address,
    )


# Conversion helpers between formats
def to_simple_format(invoice: InvoiceData, filename: str) -> SimpleInvoiceData:
    """Convert full InvoiceData to simplified format for API responses.

    Args:
        invoice: Full structured invoice data
        filename: Original filename

    Returns:
        SimpleInvoiceData: Simplified format for JSON serialization
    """
    # Extract date
    date = None
    if invoice.InvoiceDate and invoice.InvoiceDate.content:
        date = invoice.InvoiceDate.content

    # Extract total and currency
    total = None
    currency = None
    if invoice.InvoiceTotal and invoice.InvoiceTotal.value_currency:
        total = invoice.InvoiceTotal.value_currency.amount
        currency = invoice.InvoiceTotal.value_currency.currency_code

    # Extract vendor (prefer VendorName, fallback to VendorAddressRecipient)
    vendor = None
    if invoice.VendorName and invoice.VendorName.content:
        vendor = invoice.VendorName.content
    elif invoice.VendorAddressRecipient and invoice.VendorAddressRecipient.content:
        vendor = invoice.VendorAddressRecipient.content

    return SimpleInvoiceData(date=date, total=total, currency=currency, vendor=vendor, filename=filename)


def from_azure_response(azure_result) -> InvoiceData | None:
    """Create InvoiceData from Azure Document Intelligence response.

    This is an alias for _extract_from_azure_response for backward compatibility.
    """
    return _extract_from_azure_response(azure_result)


# Convenience functions for different use cases
async def extract_invoice_simple(path: str) -> SimpleInvoiceData | None:
    """Extract invoice data in simplified format for web API.

    Args:
        path: Path to the invoice file

    Returns:
        SimpleInvoiceData: Simplified format or None if extraction fails
    """
    full_data = await extract_invoice(path)
    if full_data:
        filename = Path(path).name
        return to_simple_format(full_data, filename)
    return None


# Synchronous wrapper for backward compatibility
def extract_invoice_sync(path: str) -> InvoiceData | None:
    """Synchronous wrapper for invoice extraction."""
    import asyncio

    try:
        loop = asyncio.get_event_loop()
    except RuntimeError:
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)

    return loop.run_until_complete(extract_invoice(path))
</file>

<file path="app/currency.py">
"""Currency conversion using Frankfurter API with circuit breaker."""

import asyncio
import os
from decimal import ROUND_HALF_UP, Decimal

import httpx
from dateutil import parser as date_parser


# Thread-safe circuit breaker state
class CircuitBreaker:
    """Thread-safe circuit breaker for API calls."""

    def __init__(self, max_failures: int = 2):
        self._failure_count = 0
        self._max_failures = max_failures
        self._lock = asyncio.Lock()

    async def is_open(self) -> bool:
        """Check if circuit breaker is open (blocking calls)."""
        async with self._lock:
            return self._failure_count >= self._max_failures

    async def record_success(self) -> None:
        """Record successful API call."""
        async with self._lock:
            self._failure_count = 0

    async def record_failure(self) -> None:
        """Record failed API call."""
        async with self._lock:
            self._failure_count += 1

    async def get_failure_count(self) -> int:
        """Get current failure count (for testing)."""
        async with self._lock:
            return self._failure_count


# Global circuit breaker instance
_circuit_breaker = CircuitBreaker()


class FrankfurterDown(Exception):
    """Exception raised when Frankfurter API is down after max failures."""

    pass


def _normalize_date(date_str: str) -> str:
    """Normalize a date string to YYYY-MM-DD format."""
    try:
        # Parse the date string using dateutil which handles many formats
        parsed_date = date_parser.parse(date_str)
        # Return in YYYY-MM-DD format
        return parsed_date.strftime("%Y-%m-%d")
    except (ValueError, TypeError) as e:
        raise ValueError(f"Invalid date format: {date_str}") from e


async def get_rate(date: str, from_: str, to_: str) -> Decimal:
    """Get exchange rate from Frankfurter API with circuit breaker.

    Args:
        date: Date in any common format (YYYY-MM-DD, MM/DD/YYYY, DD-MM-YYYY, etc.)
        from_: Source currency code (e.g., "USD")
        to_: Target currency code (e.g., "ILS")

    Returns:
        Decimal: Exchange rate from source to target currency, rounded to 2 decimal places using ROUND_HALF_UP

    Raises:
        ValueError: If the date format is invalid
        FrankfurterDown: If API is down after 3 consecutive failures
        Exception: If the API request fails
    """
    # Check circuit breaker
    if await _circuit_breaker.is_open():
        failure_count = await _circuit_breaker.get_failure_count()
        raise FrankfurterDown(f"Frankfurter API is down after {failure_count + 1} consecutive failures")

    # Normalize the date to YYYY-MM-DD format
    norm_date = _normalize_date(date)

    # Normalize currency codes to uppercase
    from_currency = from_.upper()
    to_currency = to_.upper()

    # Make API request with httpx
    url = f"https://api.frankfurter.app/{norm_date}?from={from_currency}&to={to_currency}"

    # Get timeout from environment or default to 2.0 seconds
    timeout = float(os.getenv("FRANKFURTER_TIMEOUT", "2.0"))

    try:
        async with httpx.AsyncClient(timeout=timeout) as client:
            response = await client.get(url)

        if response.status_code != 200:
            await _circuit_breaker.record_failure()
            raise Exception(
                f"[get_rate] Failed to fetch exchange rate for {norm_date} ({from_currency} to {to_currency}). "
                f"Code {response.status_code} {response.reason_phrase}\n{response.text}"
            )

        data = response.json()

        # Extract the exchange rate from the response
        if to_currency not in data.get("rates", {}):
            await _circuit_breaker.record_failure()
            raise Exception(f"[get_rate] Currency {to_currency} not found in response for {norm_date}")

        rate = data["rates"][to_currency]

        # Reset failure count on complete success
        await _circuit_breaker.record_success()

        # Apply ROUND_HALF_UP rounding to 2 decimal places as required by milestone
        decimal_rate = Decimal(str(rate))
        return decimal_rate.quantize(Decimal("0.01"), rounding=ROUND_HALF_UP)

    except httpx.TimeoutException as e:
        await _circuit_breaker.record_failure()
        raise Exception(f"[get_rate] Request timeout for date {norm_date}") from e
    except httpx.RequestError as e:
        await _circuit_breaker.record_failure()
        raise Exception(f"[get_rate] Network error for date {norm_date}: {e}") from e
    except (ValueError, KeyError) as e:
        await _circuit_breaker.record_failure()
        raise Exception(f"[get_rate] Invalid JSON response for date {norm_date}") from e


async def reset_circuit_breaker() -> None:
    """Reset the circuit breaker failure count (for testing)."""
    global _circuit_breaker
    _circuit_breaker = CircuitBreaker()


async def get_failure_count() -> int:
    """Get current failure count (for testing)."""
    return await _circuit_breaker.get_failure_count()
</file>

<file path="app/db.py">
"""Database session management."""

import os

from sqlalchemy import create_engine
from sqlalchemy.orm import Session, sessionmaker

from .models import Base

# Get database URL from environment, default to SQLite
DATABASE_URL = os.getenv("DATABASE_URL", "sqlite:///./invoice.db")

# Create engine
engine = create_engine(
    DATABASE_URL, connect_args={"check_same_thread": False} if DATABASE_URL.startswith("sqlite") else {}
)

# Create session factory
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)


def get_session() -> Session:
    """Get a database session."""
    session = SessionLocal()
    try:
        return session
    except Exception:
        session.close()
        raise


def get_db():
    """Dependency to get database session."""
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()


def create_tables():
    """Create all tables."""
    Base.metadata.create_all(bind=engine)
</file>

<file path="app/main.py">
"""Invoice Converter API main application."""

import asyncio
import json
import uuid
from collections.abc import AsyncGenerator
from pathlib import Path

from fastapi import Depends, FastAPI, File, Form, HTTPException, UploadFile
from fastapi.responses import FileResponse, StreamingResponse
from fastapi.staticfiles import StaticFiles
from sqlalchemy.orm import Session

from app.db import get_db
from app.models import File as FileModel
from app.models import Job
from langgraph_nodes.pipeline import get_compiled_pipeline

app = FastAPI(title="Invoice Converter API")

# Mount static files (frontend)
static_path = Path("static")
if static_path.exists():
    # Mount assets directory at /assets for CSS/JS files
    app.mount("/assets", StaticFiles(directory="static/assets"), name="assets")
    # Mount static directory for other files (vite.svg, currencies.json, etc.)
    app.mount("/static", StaticFiles(directory="static"), name="static")

# In-memory progress storage for SSE
progress_queues: dict[str, asyncio.Queue] = {}


async def execute_pipeline(job_id: str, files: list[dict], target_currency: str):
    """Execute the invoice processing pipeline with progress updates."""
    progress_queue = progress_queues.get(job_id)
    if not progress_queue:
        return

    try:
        # Add timeout for CI environments
        pipeline_timeout = 30  # 30 seconds max for pipeline execution
        # Send initial progress
        await progress_queue.put(
            {
                "job_id": job_id,
                "status": "processing",
                "current_step": "uploading",
                "processed": 0,
                "total": len(files),
                "percentage": 0,
                "message": "Starting pipeline execution",
            }
        )

        # Prepare pipeline input
        pipeline_input = {"job_id": job_id, "files": files, "target_currency": target_currency}

        # Get compiled pipeline
        pipeline = get_compiled_pipeline()

        # Execute pipeline
        await progress_queue.put(
            {
                "job_id": job_id,
                "status": "processing",
                "current_step": "extracting",
                "processed": 0,
                "total": len(files),
                "percentage": 25,
                "message": "Extracting invoice data",
            }
        )

        await asyncio.wait_for(pipeline.ainvoke(pipeline_input), timeout=pipeline_timeout)

        await progress_queue.put(
            {
                "job_id": job_id,
                "status": "processing",
                "current_step": "currency_conversion",
                "processed": len(files),
                "total": len(files),
                "percentage": 75,
                "message": "Converting currencies",
            }
        )

        # Send completion
        await progress_queue.put(
            {
                "job_id": job_id,
                "status": "completed",
                "current_step": "excel_generation",
                "processed": len(files),
                "total": len(files),
                "percentage": 100,
                "message": "Excel report generated successfully",
            }
        )

        # Update database - use proper session management
        db_session = next(get_db())
        try:
            job = db_session.query(Job).filter(Job.job_id == job_id).first()
            if job:
                job.status = "completed"
                job.processed = len(files)
                db_session.commit()
        finally:
            db_session.close()

    except asyncio.TimeoutError:
        # Send timeout error
        await progress_queue.put({"job_id": job_id, "status": "error", "message": "Pipeline execution timed out"})
    except Exception as e:
        # Send error
        await progress_queue.put(
            {"job_id": job_id, "status": "error", "message": f"Pipeline execution failed: {str(e)}"}
        )

        # Update database - use proper session management
        db_session = next(get_db())
        try:
            job = db_session.query(Job).filter(Job.job_id == job_id).first()
            if job:
                job.status = "error"
                db_session.commit()
        finally:
            db_session.close()


async def event_generator(job_id: str) -> AsyncGenerator[str, None]:
    """Generate SSE events for job progress."""
    # Get or create progress queue for this job
    progress_queue = progress_queues.get(job_id)
    if not progress_queue:
        progress_queue = asyncio.Queue()
        progress_queues[job_id] = progress_queue

    try:
        while True:
            # Wait for progress update
            try:
                progress_data = await asyncio.wait_for(progress_queue.get(), timeout=30.0)
                yield f"data: {json.dumps(progress_data)}\n\n"

                # Break if job is completed or failed
                if progress_data.get("status") in ["completed", "error"]:
                    break

            except asyncio.TimeoutError:
                # Send keepalive
                yield f"data: {json.dumps({'keepalive': True})}\n\n"

    finally:
        # Clean up
        if job_id in progress_queues:
            del progress_queues[job_id]


@app.post("/process-invoices")
async def process_invoices(
    files: list[UploadFile] = File(...), target_currency: str = Form("USD"), db: Session = Depends(get_db)
):
    """Process uploaded invoice files."""
    if len(files) > 100:
        raise HTTPException(status_code=400, detail="Maximum 100 files allowed")

    # Generate job ID
    job_id = str(uuid.uuid4())

    # Create uploads directory
    uploads_dir = Path("uploads")
    uploads_dir.mkdir(exist_ok=True)

    # Save files and prepare file list
    file_list = []
    for upload_file in files:
        # Check file size (1MB limit)
        if upload_file.size and upload_file.size > 1024 * 1024:
            raise HTTPException(status_code=400, detail=f"File {upload_file.filename} exceeds 1MB limit")

        # Read file data
        file_data = await upload_file.read()

        file_list.append({"filename": upload_file.filename, "file_data": file_data})

    # Create database job record
    job = Job(job_id=job_id, status="processing", processed=0, total=len(files))
    db.add(job)

    # Create file records
    for file_info in file_list:
        file_record = FileModel(
            job_id=job_id, filename=file_info["filename"], status="uploaded", target_currency=target_currency
        )
        db.add(file_record)

    db.commit()

    # Create progress queue before starting background task
    progress_queues[job_id] = asyncio.Queue()

    # Start pipeline execution in background
    asyncio.create_task(execute_pipeline(job_id, file_list, target_currency))

    return {"job_id": job_id}


@app.get("/progress/{job_id}")
async def get_progress(job_id: str):
    """SSE endpoint for job progress updates."""
    return StreamingResponse(
        event_generator(job_id),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
        },
    )


@app.get("/download/{job_id}")
async def download_report(job_id: str):
    """Download the generated Excel report."""
    exports_dir = Path("exports")
    export_path = exports_dir / f"{job_id}.xlsx"

    if not export_path.exists():
        raise HTTPException(status_code=404, detail="Report not found")

    return FileResponse(
        path=export_path,
        media_type="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
        filename=f"invoice_report_{job_id}.xlsx",
    )


@app.get("/health")
def health_check():
    """Health check endpoint."""
    return {"status": "ok"}


@app.get("/")
async def serve_frontend():
    """Serve the frontend application."""
    static_path = Path("static")
    index_path = static_path / "index.html"

    if index_path.exists():
        return FileResponse(index_path)
    else:
        return {"message": "Frontend not available. API is running at /health"}


@app.get("/vite.svg")
async def serve_vite_svg():
    """Serve the vite.svg file."""
    svg_path = Path("static/vite.svg")
    if svg_path.exists():
        return FileResponse(svg_path, media_type="image/svg+xml")
    else:
        return {"error": "vite.svg File not found"}


@app.get("/currencies.json")
async def serve_currencies():
    """Serve the currencies.json file."""
    json_path = Path("static/currencies.json")
    if json_path.exists():
        return FileResponse(json_path, media_type="application/json")
    else:
        return {"error": "currencies.json File not found"}
</file>

<file path="app/models.py">
"""SQLAlchemy models for invoice processing."""

from datetime import UTC, datetime

from sqlalchemy import DateTime, ForeignKey, Integer, Numeric, String
from sqlalchemy.orm import DeclarativeBase, Mapped, mapped_column, relationship


class Base(DeclarativeBase):
    """Base class for all database models."""

    pass


class Job(Base):
    """Job model for tracking batch invoice processing jobs."""

    __tablename__ = "jobs"

    job_id: Mapped[str] = mapped_column(String, primary_key=True)
    status: Mapped[str] = mapped_column(String, nullable=False)
    processed: Mapped[int] = mapped_column(Integer, default=0)
    total: Mapped[int] = mapped_column(Integer, nullable=False)
    created_at: Mapped[datetime] = mapped_column(DateTime, default=lambda: datetime.now(UTC))
    updated_at: Mapped[datetime] = mapped_column(
        DateTime, default=lambda: datetime.now(UTC), onupdate=lambda: datetime.now(UTC)
    )

    # Relationship to files
    files: Mapped[list["File"]] = relationship("File", back_populates="job")


class File(Base):
    """File model for tracking individual invoice files."""

    __tablename__ = "files"

    id: Mapped[int] = mapped_column(Integer, primary_key=True, autoincrement=True)
    job_id: Mapped[str] = mapped_column(String, ForeignKey("jobs.job_id"), nullable=False)
    filename: Mapped[str] = mapped_column(String, nullable=False)
    status: Mapped[str] = mapped_column(String, nullable=False, index=True)
    original_currency: Mapped[str | None] = mapped_column(String)
    target_currency: Mapped[str | None] = mapped_column(String)
    error_message: Mapped[str | None] = mapped_column(String)

    # New columns for currency conversion tracking
    converted_total: Mapped[float | None] = mapped_column(Numeric(10, 2), index=True)
    exchange_rate: Mapped[float | None] = mapped_column(Numeric(10, 6), index=True)

    # Relationship to job
    job: Mapped["Job"] = relationship("Job", back_populates="files")
</file>

<file path="langgraph_nodes/base.py">
"""Base node for LangGraph pipeline."""


async def run(input: dict) -> dict:
    """Base node that passes input through unchanged.

    Args:
        input: Input dictionary containing pipeline state

    Returns:
        dict: Same input dictionary unchanged
    """
    return input
</file>

<file path="langgraph_nodes/check_currency.py">
"""Check currency node for currency validation."""


async def run(input: dict) -> dict:
    """Check currency node that validates currency information.

    Args:
        input: Input dictionary containing pipeline state with 'invoices' list

    Returns:
        dict: Input dictionary with currency validation results
    """
    invoices = input.get("invoices", [])

    # Prepare data for currency conversion
    files = []

    for invoice in invoices:
        file_data = {"filename": getattr(invoice, "_filename", "unknown"), "status": "ready_for_conversion"}

        # Extract currency information
        if invoice.InvoiceTotal and invoice.InvoiceTotal.value_currency:
            file_data["src_currency"] = invoice.InvoiceTotal.value_currency.currency_code
            file_data["invoice_total"] = invoice.InvoiceTotal.value_currency.amount
        else:
            file_data["src_currency"] = None
            file_data["invoice_total"] = None
            file_data["status"] = "failed"
            file_data["error"] = "No currency information found in invoice"

        # Extract date information
        if invoice.InvoiceDate:
            file_data["invoice_date"] = invoice.InvoiceDate.content
        else:
            file_data["invoice_date"] = "2024-01-01"  # Default date if not found

        files.append(file_data)

    # Update input with currency check results
    result = input.copy()
    result["files"] = files

    return result
</file>

<file path="langgraph_nodes/convert.py">
"""Convert node for currency conversion."""

import os
from decimal import ROUND_HALF_UP, Decimal

from app.currency import get_rate


async def run(input: dict) -> dict:
    """Convert node that performs currency conversion.

    Args:
        input: Input dictionary containing pipeline state with structure:
            {
                "job_id": "abc123",
                "target_currency": "ILS",  # optional, defaults to ILS
                "files": [
                    {
                        "id": "file-1",
                        "invoice_date": "2025-07-01",
                        "src_currency": "USD",
                        "invoice_total": 145.67
                    }
                ]
            }

    Returns:
        dict: Input dictionary with converted_total and exchange_rate added to each file,
              or status="failed" and error message for failed conversions
    """
    # Get target currency from job payload or environment variable or default to ILS
    target_currency = input.get("target_currency") or os.getenv("DEFAULT_TARGET_CURRENCY", "ILS")

    files = input.get("files", [])
    invoices = input.get("invoices", [])

    for file_data in files:
        try:
            # Extract required fields
            invoice_date = file_data.get("invoice_date")
            src_currency = file_data.get("src_currency")
            invoice_total = file_data.get("invoice_total")

            # Validate required fields
            if not all([invoice_date, src_currency, invoice_total]):
                file_data["status"] = "failed"
                file_data["error"] = "Missing required fields: invoice_date, src_currency, or invoice_total"
                continue

            # Skip conversion if source and target currencies are the same
            if src_currency.upper() == target_currency.upper():
                file_data["exchange_rate"] = 1.0
                file_data["converted_total"] = float(invoice_total)
                continue

            # Get exchange rate from Frankfurter API
            rate = await get_rate(invoice_date, src_currency, target_currency)

            # Convert the total amount with ROUND_HALF_UP rounding
            original_amount = Decimal(str(invoice_total))
            converted_amount = (original_amount * rate).quantize(Decimal("0.01"), rounding=ROUND_HALF_UP)

            # Add conversion results to file data
            file_data["exchange_rate"] = float(rate)
            file_data["converted_total"] = float(converted_amount)

        except Exception as e:
            # Mark individual file as failed but continue processing others
            file_data["status"] = "failed"
            file_data["error"] = f"Currency conversion failed: {str(e)}"

    # Also apply conversion results to invoices if they exist
    if invoices:
        conversion_data = {}
        for file_data in files:
            filename = file_data.get("filename")
            if filename:
                conversion_data[filename] = {
                    "converted_total": file_data.get("converted_total"),
                    "exchange_rate": file_data.get("exchange_rate"),
                    "status": file_data.get("status", "success"),
                }

        # Apply conversion results to invoices
        for invoice in invoices:
            filename = getattr(invoice, "_filename", "unknown")
            if filename in conversion_data:
                conv_data = conversion_data[filename]
                invoice._converted_amount = conv_data["converted_total"]
                invoice._exchange_rate = conv_data["exchange_rate"]
                invoice._conversion_status = conv_data["status"]

    return input
</file>

<file path="langgraph_nodes/excel.py">
"""Excel node for report generation."""

import re
from io import BytesIO

from openpyxl import Workbook
from openpyxl.styles import Alignment, Font, PatternFill
from openpyxl.utils import get_column_letter

from app.azure_adapter import InvoiceData


def invoice_suffix(invoice: InvoiceData) -> str:
    """Extract invoice suffix from filename per spec.

    - Strip non-digits, take last 4, left-pad zeros
    - If no digits → SUFFIX_NOT_FOUND

    Args:
        invoice: Invoice data (unused in current implementation)
        filename: Original filename

    Returns:
        str: Invoice suffix (4 digits or SUFFIX_NOT_FOUND)
    """
    if not invoice.InvoiceId or not invoice.InvoiceId.content:
        return "SUFFIX_NOT_FOUND"

    # Strip non-digits and extract all digits
    digits = re.sub(r"\D", "", invoice.InvoiceId.content)

    if not digits:
        return "SUFFIX_NOT_FOUND"

    # Take last 4 digits, left-pad with zeros if needed
    last_four = digits[-4:]
    return last_four.zfill(4)


async def run(input: dict) -> dict:
    """Excel node that generates Excel reports from invoice data.

    Args:
        input: Input dictionary containing 'invoices' key with list of InvoiceData objects
              and 'target_currency' key

    Returns:
        dict: Dictionary with 'xlsx' (BytesIO bytes) and 'row_count' keys
    """
    invoices: list[InvoiceData] = input.get("invoices", [])
    target_currency: str = input.get("target_currency", "USD")

    # Create workbook and worksheet
    wb = Workbook()
    ws = wb.active
    ws.title = "Invoices Report"

    # Define headers per spec
    headers = [
        "Date (DD/MM/YYYY)",
        "Invoice Suffix",
        f"{target_currency} Total Price",
        "Foreign Currency Total Price",
        "Foreign Currency Code",
        "Exchange Rate (4 dp)",
        "Vendor Name",
    ]

    # Style headers
    header_font = Font(bold=True, color="FFFFFF")
    header_fill = PatternFill(start_color="366092", end_color="366092", fill_type="solid")
    header_alignment = Alignment(horizontal="center", vertical="center")

    # Write headers
    for col, header in enumerate(headers, 1):
        cell = ws.cell(row=1, column=col, value=header)
        cell.font = header_font
        cell.fill = header_fill
        cell.alignment = header_alignment

    # Write data rows
    row_num = 2
    filename = ""  # Initialize filename for empty invoices case
    for invoice in invoices:
        # Extract filename from invoice data (should be stored in invoice)
        filename = getattr(invoice, "_filename", "")

        # Extract data with error handling per spec
        date = invoice.InvoiceDate.content if invoice.InvoiceDate else "ERROR"
        suffix = invoice_suffix(invoice)
        vendor_name = invoice.VendorName.content if invoice.VendorName else "N/A"

        # Handle amounts and currency
        target_total = "N/A"
        foreign_total = ""
        foreign_currency = ""
        exchange_rate = ""

        if invoice.InvoiceTotal and invoice.InvoiceTotal.value_currency:
            invoice_currency = invoice.InvoiceTotal.value_currency.currency_code
            invoice_amount = invoice.InvoiceTotal.value_currency.amount

            if invoice_currency == target_currency:
                # Case 1: Same currency - use invoice amount directly, leave foreign fields empty
                target_total = invoice_amount
                foreign_total = ""
                foreign_currency = ""
                exchange_rate = ""
            else:
                # Case 2: Different currency - fill all foreign fields
                foreign_total = invoice_amount
                foreign_currency = invoice_currency

                # Use converted amounts if available
                target_total = getattr(invoice, "_converted_amount", "N/A")
                exchange_rate = getattr(invoice, "_exchange_rate", "N/A")

                # Format exchange rate to 4 decimal places if it's a number
                if isinstance(exchange_rate, int | float):
                    exchange_rate = f"{exchange_rate:.4f}"

        # Write row data
        row_data = [date, suffix, target_total, foreign_total, foreign_currency, exchange_rate, vendor_name]

        for col, value in enumerate(row_data, 1):
            ws.cell(row=row_num, column=col, value=value)

        row_num += 1

    # Add placeholder ERROR rows if no invoices provided
    if not invoices:
        error_row = ["ERROR", filename, "N/A", "N/A", "N/A", "N/A", "N/A"]
        for col, value in enumerate(error_row, 1):
            ws.cell(row=2, column=col, value=value)
        row_num = 3

    # Auto-adjust column widths
    for col in range(1, len(headers) + 1):
        column_letter = get_column_letter(col)
        ws.column_dimensions[column_letter].width = 15

    # Save to BytesIO
    excel_buffer = BytesIO()
    wb.save(excel_buffer)
    excel_buffer.seek(0)

    # Also save to exports directory if job_id is provided
    job_id = input.get("job_id")
    if job_id:
        from pathlib import Path

        exports_dir = Path("exports")
        exports_dir.mkdir(exist_ok=True)

        export_path = exports_dir / f"{job_id}.xlsx"
        wb.save(export_path)

    # Calculate row count (excluding header)
    data_row_count = max(1, len(invoices)) if invoices else 1

    return {"xlsx": excel_buffer.getvalue(), "row_count": data_row_count}
</file>

<file path="langgraph_nodes/extract.py">
"""Extract node for invoice data extraction."""

from app.azure_adapter import extract_invoice


async def run(input: dict) -> dict:
    """Extract node that extracts data from invoices using Azure Document Intelligence.

    Args:
        input: Input dictionary containing pipeline state with 'files' list

    Returns:
        dict: Input dictionary with extracted invoice data
    """
    files = input.get("files", [])
    invoices = []

    for file_info in files:
        if "file_path" in file_info:
            file_path = file_info["file_path"]
            filename = file_info["filename"]

            try:
                print(f"Starting Azure extraction for {filename} at {file_path}")  # Debug
                # Extract invoice data using Azure adapter
                invoice_data = await extract_invoice(file_path)
                print(f"Azure extraction result for {filename}: {invoice_data is not None}")  # Debug

                if invoice_data:
                    # Add filename to invoice data for reference
                    invoice_data._filename = filename
                    invoices.append(invoice_data)

                    # Update file status
                    file_info["status"] = "extracted"
                    print(f"Successfully extracted invoice data for {filename}")  # Debug
                    print(f"Invoice: {invoice_data}")  # Debug
                else:
                    # Extraction failed
                    file_info["status"] = "failed"
                    file_info["error_message"] = "Failed to extract invoice data"
                    print(f"Azure extraction returned None for {filename}")  # Debug

            except Exception as e:
                # Handle extraction errors
                print(f"Azure extraction failed for {filename}: {e}")  # Debug logging
                file_info["status"] = "failed"
                file_info["error_message"] = str(e)

    # Update input with extracted invoices
    result = input.copy()
    result["invoices"] = invoices

    return result
</file>

<file path="langgraph_nodes/pipeline.py">
"""LangGraph pipeline for invoice processing workflow."""

from langgraph.graph import Graph

from . import check_currency, convert, excel, extract, upload


def create_pipeline() -> Graph:
    """Create the invoice processing pipeline graph.

    Returns:
        Graph: LangGraph instance with nodes linked in processing order
    """
    # Create a new graph
    graph = Graph()

    # Add nodes to the graph
    graph.add_node("upload", upload.run)
    graph.add_node("extract", extract.run)
    graph.add_node("check_currency", check_currency.run)
    graph.add_node("convert", convert.run)
    graph.add_node("excel", excel.run)

    # Link nodes in the processing order
    graph.add_edge("upload", "extract")
    graph.add_edge("extract", "check_currency")
    graph.add_edge("check_currency", "convert")
    graph.add_edge("convert", "excel")

    # Set entry point
    graph.set_entry_point("upload")
    graph.set_finish_point("excel")

    return graph


def get_compiled_pipeline():
    """Get a compiled pipeline ready for execution.

    Returns:
        Compiled LangGraph pipeline
    """
    pipeline = create_pipeline()
    return pipeline.compile()
</file>

<file path="langgraph_nodes/upload.py">
"""Upload node for file processing."""

from pathlib import Path


async def run(input: dict) -> dict:
    """Upload node that processes file uploads.

    Args:
        input: Input dictionary containing pipeline state with 'files' list

    Returns:
        dict: Input dictionary with processed file paths
    """
    # Ensure uploads directory exists
    uploads_dir = Path("uploads")
    uploads_dir.mkdir(exist_ok=True)

    # Process files if they exist
    files = input.get("files", [])
    processed_files = []

    for file_info in files:
        # If file_info has file data, save it to uploads directory
        if "file_data" in file_info:
            filename = file_info["filename"]
            file_data = file_info["file_data"]

            # Save file to uploads directory
            file_path = uploads_dir / filename
            with open(file_path, "wb") as f:
                f.write(file_data)

            processed_files.append({"filename": filename, "file_path": str(file_path), "status": "uploaded"})
        else:
            # File info without data, just pass through
            processed_files.append(file_info)

    # Update input with processed files
    result = input.copy()
    result["files"] = processed_files

    return result
</file>

<file path="tests/fixtures/README.md">
# Test Fixtures

This directory contains test fixtures for Azure Document Intelligence testing.

## Adding Sample Invoices

To add a sample invoice for testing:

1. Place the PDF file in this directory
2. Update the test cases to reference the new file
3. Run tests to generate VCR cassettes

**Note:** Do not commit actual invoice files with sensitive information.
Use anonymized or dummy invoices only.
</file>

<file path="tests/__init__.py">
"""Test package for invoice converter."""
</file>

<file path="tests/test_azure_adapter.py">
"""Tests for Azure Document Intelligence adapter."""

import os
from unittest.mock import AsyncMock, Mock, patch

import pytest

from app.azure_adapter import (
    DefaultContent,
    InvoiceData,
    InvoiceTotal,
    SimpleInvoiceData,
    ValueCurrency,
    _extract_from_azure_response,
    extract_invoice,
    extract_invoice_simple,
    from_azure_response,
    to_simple_format,
)


class TestDataClasses:
    """Test cases for the data classes."""

    def test_invoice_data_creation(self):
        """Test InvoiceData creation with robust structure."""
        invoice_data = InvoiceData(
            InvoiceDate=DefaultContent("2025-01-15", 0.95),
            InvoiceId=DefaultContent("INV-12345", 0.90),
            InvoiceTotal=InvoiceTotal(
                value_currency=ValueCurrency(amount=914.50, currency_code="USD"), content="914.50", confidence=0.92
            ),
            VendorName=DefaultContent("Test Company Inc.", 0.88),
            VendorAddressRecipient=DefaultContent("123 Main St", 0.85),
        )

        assert invoice_data.InvoiceDate.content == "2025-01-15"
        assert invoice_data.InvoiceId.content == "INV-12345"
        assert invoice_data.InvoiceTotal.value_currency.amount == 914.50
        assert invoice_data.InvoiceTotal.value_currency.currency_code == "USD"
        assert invoice_data.VendorName.content == "Test Company Inc."

    def test_simple_invoice_data_creation(self):
        """Test SimpleInvoiceData creation."""
        simple_data = SimpleInvoiceData(
            date="2025-01-15", total=914.50, currency="USD", vendor="Test Company Inc.", filename="test_invoice.pdf"
        )

        assert simple_data.date == "2025-01-15"
        assert simple_data.total == 914.50
        assert simple_data.currency == "USD"
        assert simple_data.vendor == "Test Company Inc."
        assert simple_data.filename == "test_invoice.pdf"

    def test_invoice_data_with_none_values(self):
        """Test InvoiceData with None values."""
        invoice_data = InvoiceData(
            InvoiceDate=None, InvoiceId=None, InvoiceTotal=None, VendorName=None, VendorAddressRecipient=None
        )

        assert invoice_data.InvoiceDate is None
        assert invoice_data.InvoiceId is None
        assert invoice_data.InvoiceTotal is None
        assert invoice_data.VendorName is None
        assert invoice_data.VendorAddressRecipient is None


class TestExtractFromAzureResponse:
    """Test cases for Azure response extraction."""

    def test_extract_success(self):
        """Test successful extraction from Azure response."""
        # Mock Azure response structure
        mock_date_field = Mock()
        mock_date_field.content = "2025-01-15"
        mock_date_field.confidence = 0.95

        mock_total_field = Mock()
        mock_value_currency = Mock()
        mock_value_currency.amount = 914.50
        mock_value_currency.currency_code = "USD"
        mock_total_field.value_currency = mock_value_currency
        mock_total_field.content = "$914.50"
        mock_total_field.confidence = 0.92

        mock_vendor_field = Mock()
        mock_vendor_field.content = "Test Company Inc."
        mock_vendor_field.confidence = 0.88

        mock_fields = {
            "InvoiceDate": mock_date_field,
            "InvoiceTotal": mock_total_field,
            "VendorName": mock_vendor_field,
        }

        mock_document = Mock()
        mock_document.fields = mock_fields

        mock_result = Mock()
        mock_result.documents = [mock_document]

        # Test the function
        result = _extract_from_azure_response(mock_result)

        assert result is not None
        assert result.InvoiceDate.content == "2025-01-15"
        assert result.InvoiceTotal.value_currency.amount == 914.50
        assert result.InvoiceTotal.value_currency.currency_code == "USD"
        assert result.InvoiceTotal.content == "$914.50"
        assert result.VendorName.content == "Test Company Inc."

    def test_extract_no_documents(self):
        """Test handling when no documents in response."""
        mock_result = Mock()
        mock_result.documents = []

        result = _extract_from_azure_response(mock_result)
        assert result is None

    def test_extract_missing_fields(self):
        """Test handling of missing fields."""
        mock_fields = {}  # Empty fields

        mock_document = Mock()
        mock_document.fields = mock_fields

        mock_result = Mock()
        mock_result.documents = [mock_document]

        result = _extract_from_azure_response(mock_result)

        assert result is not None
        assert result.InvoiceDate is None
        assert result.InvoiceId is None
        assert result.InvoiceTotal is None
        assert result.VendorName is None
        assert result.VendorAddressRecipient is None

    def test_extract_vendor_fallback(self):
        """Test vendor extraction fallback to VendorAddressRecipient."""
        mock_address_field = Mock()
        mock_address_field.content = "ABC Corp, 123 Main St"
        mock_address_field.confidence = 0.85

        mock_fields = {
            "VendorAddressRecipient": mock_address_field,
        }

        mock_document = Mock()
        mock_document.fields = mock_fields

        mock_result = Mock()
        mock_result.documents = [mock_document]

        result = _extract_from_azure_response(mock_result)

        assert result is not None
        assert result.VendorAddressRecipient.content == "ABC Corp, 123 Main St"
        assert result.VendorName is None


class TestConversionHelpers:
    """Test cases for format conversion helpers."""

    def test_to_simple_format_complete(self):
        """Test conversion from full to simple format with all fields."""
        full_data = InvoiceData(
            InvoiceDate=DefaultContent("2025-01-15", 0.95),
            InvoiceId=DefaultContent("INV-12345", 0.90),
            InvoiceTotal=InvoiceTotal(
                value_currency=ValueCurrency(amount=914.50, currency_code="USD"), content="914.50", confidence=0.92
            ),
            VendorName=DefaultContent("Test Company Inc.", 0.88),
            VendorAddressRecipient=DefaultContent("123 Main St", 0.85),
        )

        result = to_simple_format(full_data, "test_invoice.pdf")

        assert result.date == "2025-01-15"
        assert result.total == 914.50
        assert result.currency == "USD"
        assert result.vendor == "Test Company Inc."  # Should prefer VendorName
        assert result.filename == "test_invoice.pdf"

    def test_to_simple_format_with_fallbacks(self):
        """Test conversion with vendor fallback to address."""
        full_data = InvoiceData(
            InvoiceDate=None,
            InvoiceId=None,
            InvoiceTotal=None,
            VendorName=None,
            VendorAddressRecipient=DefaultContent("ABC Corp, 123 Main St", 0.85),
        )

        result = to_simple_format(full_data, "test_invoice.pdf")

        assert result.date is None
        assert result.total is None
        assert result.currency is None
        assert result.vendor == "ABC Corp, 123 Main St"  # Should use address as fallback
        assert result.filename == "test_invoice.pdf"

    def test_to_simple_format_empty(self):
        """Test conversion with empty data."""
        full_data = InvoiceData(
            InvoiceDate=None, InvoiceId=None, InvoiceTotal=None, VendorName=None, VendorAddressRecipient=None
        )

        result = to_simple_format(full_data, "test_invoice.pdf")

        assert result.date is None
        assert result.total is None
        assert result.currency is None
        assert result.vendor is None
        assert result.filename == "test_invoice.pdf"

    def test_from_azure_response_alias(self):
        """Test from_azure_response is working as alias."""
        mock_result = Mock()
        mock_result.documents = []

        result1 = from_azure_response(mock_result)
        result2 = _extract_from_azure_response(mock_result)

        assert result1 == result2  # Both should return None


class TestExtractInvoice:
    """Test cases for the extract_invoice function."""

    @pytest.mark.asyncio
    async def test_missing_credentials(self):
        """Test handling of missing credentials."""
        with patch.dict(os.environ, {}, clear=True):
            result = await extract_invoice("test.pdf")
            assert result is None

    @pytest.mark.asyncio
    async def test_file_not_found(self):
        """Test handling when file doesn't exist."""
        with patch.dict(
            os.environ,
            {
                "AZURE_DOCUMENT_INTELLIGENCE_ENDPOINT": "https://test.cognitiveservices.azure.com/",
                "AZURE_DOCUMENT_INTELLIGENCE_API_KEY": "test-api-key",
            },
        ):
            result = await extract_invoice("nonexistent.pdf")
            assert result is None

    @pytest.mark.asyncio
    @patch("app.azure_adapter.DocumentIntelligenceClient")
    @patch("builtins.open")
    @patch("app.azure_adapter.Path")
    async def test_extract_success(self, mock_path, mock_open, mock_client_class):
        """Test successful invoice extraction."""
        with patch.dict(
            os.environ,
            {
                "AZURE_DOCUMENT_INTELLIGENCE_ENDPOINT": "https://test.cognitiveservices.azure.com/",
                "AZURE_DOCUMENT_INTELLIGENCE_API_KEY": "test-api-key",
            },
        ):
            # Setup file mocks
            mock_path_instance = Mock()
            mock_path_instance.exists.return_value = True
            mock_path_instance.name = "test_invoice.pdf"
            mock_path.return_value = mock_path_instance

            mock_file = Mock()
            mock_file.read.return_value = b"mock pdf data"
            mock_open.return_value.__enter__.return_value = mock_file

            # Setup Azure client mock
            mock_client = AsyncMock()
            mock_client_class.return_value.__aenter__.return_value = mock_client

            # Mock the begin_analyze_document call
            mock_poller = AsyncMock()
            mock_client.begin_analyze_document.return_value = mock_poller

            # Mock successful Azure response
            mock_date_field = Mock()
            mock_date_field.content = "2025-01-15"
            mock_date_field.confidence = 0.95

            mock_total_field = Mock()
            mock_value_currency = Mock()
            mock_value_currency.amount = 914.50
            mock_value_currency.currency_code = "USD"
            mock_total_field.value_currency = mock_value_currency
            mock_total_field.content = "$914.50"
            mock_total_field.confidence = 0.92

            mock_vendor_field = Mock()
            mock_vendor_field.content = "Test Company Inc."
            mock_vendor_field.confidence = 0.88

            mock_fields = {
                "InvoiceDate": mock_date_field,
                "InvoiceTotal": mock_total_field,
                "VendorName": mock_vendor_field,
            }

            mock_document = Mock()
            mock_document.fields = mock_fields

            mock_result = Mock()
            mock_result.documents = [mock_document]
            mock_poller.result.return_value = mock_result

            # Test the function
            result = await extract_invoice("test_invoice.pdf")

            assert result is not None
            assert result.InvoiceDate.content == "2025-01-15"
            assert result.InvoiceTotal.value_currency.amount == 914.50
            assert result.InvoiceTotal.value_currency.currency_code == "USD"
            assert result.VendorName.content == "Test Company Inc."

            # Verify API was called correctly
            mock_client.begin_analyze_document.assert_called_once_with(
                "prebuilt-invoice", b"mock pdf data", content_type="application/pdf"
            )

    @pytest.mark.asyncio
    @patch("app.azure_adapter.DocumentIntelligenceClient")
    @patch("builtins.open")
    @patch("app.azure_adapter.Path")
    async def test_azure_exception(self, mock_path, mock_open, mock_client_class):
        """Test handling of Azure client exceptions."""
        with patch.dict(
            os.environ,
            {
                "AZURE_DOCUMENT_INTELLIGENCE_ENDPOINT": "https://test.cognitiveservices.azure.com/",
                "AZURE_DOCUMENT_INTELLIGENCE_API_KEY": "test-api-key",
            },
        ):
            # Setup file mocks
            mock_path_instance = Mock()
            mock_path_instance.exists.return_value = True
            mock_path_instance.name = "test_invoice.pdf"
            mock_path.return_value = mock_path_instance

            mock_file = Mock()
            mock_file.read.return_value = b"mock pdf data"
            mock_open.return_value.__enter__.return_value = mock_file

            # Setup Azure client mock to raise exception
            mock_client = AsyncMock()
            mock_client.begin_analyze_document.side_effect = Exception("Azure API error")
            mock_client_class.return_value.__aenter__.return_value = mock_client

            result = await extract_invoice("test_invoice.pdf")
            assert result is None

    @pytest.mark.asyncio
    @patch("app.azure_adapter.extract_invoice")
    async def test_extract_invoice_simple_success(self, mock_extract):
        """Test successful simple extraction."""
        # Mock full extraction result
        full_data = InvoiceData(
            InvoiceDate=DefaultContent("2025-01-15", 0.95),
            InvoiceId=DefaultContent("INV-12345", 0.90),
            InvoiceTotal=InvoiceTotal(
                value_currency=ValueCurrency(amount=914.50, currency_code="USD"), content="914.50", confidence=0.92
            ),
            VendorName=DefaultContent("Test Company Inc.", 0.88),
            VendorAddressRecipient=None,
        )
        mock_extract.return_value = full_data

        # Test simple extraction
        result = await extract_invoice_simple("test_invoice.pdf")

        assert result is not None
        assert result.date == "2025-01-15"
        assert result.total == 914.50
        assert result.currency == "USD"
        assert result.vendor == "Test Company Inc."
        assert result.filename == "test_invoice.pdf"

    @pytest.mark.asyncio
    @patch("app.azure_adapter.extract_invoice")
    async def test_extract_invoice_simple_failure(self, mock_extract):
        """Test simple extraction when full extraction fails."""
        mock_extract.return_value = None

        result = await extract_invoice_simple("test_invoice.pdf")
        assert result is None


# VCR cassette tests would go here when we have real Azure API calls to record
# @pytest.mark.vcr()
# async def test_extract_invoice_real_api():
#     """Test with real Azure API using VCR cassette."""
#     # This would be used with a real PDF file and Azure credentials
#     # The first run would record the HTTP interactions
#     # Subsequent runs would replay them
#     pass
</file>

<file path="tests/test_convert_node.py">
"""Tests for the convert node."""

import os
from decimal import ROUND_HALF_UP, Decimal
from unittest.mock import patch

import httpx
import pytest
import respx

from app.currency import reset_circuit_breaker
from langgraph_nodes.convert import run


class TestConvertNode:
    """Test cases for the convert node currency conversion."""

    @pytest.fixture(autouse=True, scope="function")
    async def reset_breaker(self):
        """Reset circuit breaker before and after each test."""
        # Reset before test
        await reset_circuit_breaker()
        yield
        # Reset after test to clean up
        await reset_circuit_breaker()

    @pytest.mark.asyncio
    async def test_happy_path_conversion(self):
        """Test successful currency conversion with ILS default."""
        input_data = {
            "job_id": "test-job-123",
            "files": [{"id": "file-1", "invoice_date": "2025-07-01", "src_currency": "USD", "invoice_total": 100.00}],
        }

        expected_response = {"amount": 1.0, "base": "USD", "date": "2025-07-01", "rates": {"ILS": 3.65}}

        with respx.mock:
            respx.get("https://api.frankfurter.app/2025-07-01?from=USD&to=ILS").mock(
                return_value=httpx.Response(200, json=expected_response)
            )

            result = await run(input_data)

            assert result["job_id"] == "test-job-123"
            assert len(result["files"]) == 1

            file_result = result["files"][0]
            assert file_result["exchange_rate"] == 3.65
            assert file_result["converted_total"] == 365.00
            assert "status" not in file_result or file_result["status"] != "failed"

    @pytest.mark.asyncio
    async def test_rounding_edge_cases(self):
        """Test ROUND_HALF_UP rounding for edge cases like 0.005 → 0.01."""
        input_data = {
            "job_id": "test-job-123",
            "files": [
                {
                    "id": "file-1",
                    "invoice_date": "2025-07-01",
                    "src_currency": "USD",
                    "invoice_total": 0.005,  # Edge case for rounding
                }
            ],
        }

        # Rate that will produce 0.005 after multiplication (0.005 * 1.0 = 0.005)
        expected_response = {"amount": 1.0, "base": "USD", "date": "2025-07-01", "rates": {"ILS": 1.0}}

        with respx.mock:
            respx.get("https://api.frankfurter.app/2025-07-01?from=USD&to=ILS").mock(
                return_value=httpx.Response(200, json=expected_response)
            )

            result = await run(input_data)

            file_result = result["files"][0]
            # 0.005 should round up to 0.01 with ROUND_HALF_UP
            assert file_result["converted_total"] == 0.01

    @pytest.mark.asyncio
    async def test_same_currency_no_conversion(self):
        """Test that same source and target currency returns rate 1.0."""
        input_data = {
            "job_id": "test-job-123",
            "target_currency": "USD",
            "files": [{"id": "file-1", "invoice_date": "2025-07-01", "src_currency": "USD", "invoice_total": 100.00}],
        }

        # No API call should be made
        result = await run(input_data)

        file_result = result["files"][0]
        assert file_result["exchange_rate"] == 1.0
        assert file_result["converted_total"] == 100.00

    @pytest.mark.asyncio
    async def test_per_job_target_currency_override(self):
        """Test per-job target currency override."""
        input_data = {
            "job_id": "test-job-123",
            "target_currency": "EUR",  # Override default ILS
            "files": [{"id": "file-1", "invoice_date": "2025-07-01", "src_currency": "USD", "invoice_total": 100.00}],
        }

        expected_response = {"amount": 1.0, "base": "USD", "date": "2025-07-01", "rates": {"EUR": 0.85}}

        with respx.mock:
            respx.get("https://api.frankfurter.app/2025-07-01?from=USD&to=EUR").mock(
                return_value=httpx.Response(200, json=expected_response)
            )

            result = await run(input_data)

            file_result = result["files"][0]
            assert file_result["exchange_rate"] == 0.85
            assert file_result["converted_total"] == 85.00

    @pytest.mark.asyncio
    async def test_environment_variable_fallback(self):
        """Test fallback to DEFAULT_TARGET_CURRENCY environment variable."""
        input_data = {
            "job_id": "test-job-123",
            # No target_currency in job payload
            "files": [{"id": "file-1", "invoice_date": "2025-07-01", "src_currency": "USD", "invoice_total": 100.00}],
        }

        expected_response = {"amount": 1.0, "base": "USD", "date": "2025-07-01", "rates": {"EUR": 0.85}}

        with patch.dict(os.environ, {"DEFAULT_TARGET_CURRENCY": "EUR"}):
            with respx.mock:
                respx.get("https://api.frankfurter.app/2025-07-01?from=USD&to=EUR").mock(
                    return_value=httpx.Response(200, json=expected_response)
                )

                result = await run(input_data)

                file_result = result["files"][0]
                assert file_result["exchange_rate"] == 0.85

    @pytest.mark.asyncio
    async def test_missing_required_fields(self):
        """Test handling of missing required fields."""
        input_data = {
            "job_id": "test-job-123",
            "files": [
                {
                    "id": "file-1",
                    # Missing invoice_date
                    "src_currency": "USD",
                    "invoice_total": 100.00,
                },
                {
                    "id": "file-2",
                    "invoice_date": "2025-07-01",
                    # Missing src_currency
                    "invoice_total": 100.00,
                },
                {
                    "id": "file-3",
                    "invoice_date": "2025-07-01",
                    "src_currency": "USD"
                    # Missing invoice_total
                },
            ],
        }

        result = await run(input_data)

        # All files should be marked as failed
        for file_result in result["files"]:
            assert file_result["status"] == "failed"
            assert "Missing required fields" in file_result["error"]

    @pytest.mark.asyncio
    async def test_api_failure_individual_file(self):
        """Test that API failure marks individual file as failed but continues processing others."""
        input_data = {
            "job_id": "test-job-123",
            "files": [
                {"id": "file-1", "invoice_date": "2025-07-01", "src_currency": "USD", "invoice_total": 100.00},
                {"id": "file-2", "invoice_date": "2025-07-01", "src_currency": "EUR", "invoice_total": 85.00},
            ],
        }

        with respx.mock:
            # First request fails
            respx.get("https://api.frankfurter.app/2025-07-01?from=USD&to=ILS").mock(
                return_value=httpx.Response(500, text="Internal Server Error")
            )

            # Second request succeeds
            respx.get("https://api.frankfurter.app/2025-07-01?from=EUR&to=ILS").mock(
                return_value=httpx.Response(
                    200, json={"amount": 1.0, "base": "EUR", "date": "2025-07-01", "rates": {"ILS": 4.0}}
                )
            )

            result = await run(input_data)

            # First file should be failed
            file1 = result["files"][0]
            assert file1["status"] == "failed"
            assert "Currency conversion failed" in file1["error"]

            # Second file should succeed
            file2 = result["files"][1]
            assert file2.get("status") != "failed"
            assert file2["exchange_rate"] == 4.0
            assert file2["converted_total"] == 340.00

    @pytest.mark.asyncio
    async def test_circuit_breaker_failure(self):
        """Test that circuit breaker failures are handled gracefully."""
        # Ensure clean state at start of test
        await reset_circuit_breaker()

        input_data = {
            "job_id": "test-job-123",
            "files": [{"id": "file-1", "invoice_date": "2025-07-01", "src_currency": "USD", "invoice_total": 100.00}],
        }

        with respx.mock:
            # Simulate multiple failures to trigger circuit breaker
            respx.get("https://api.frankfurter.app/2025-07-01?from=USD&to=ILS").mock(
                return_value=httpx.Response(500, text="Internal Server Error")
            )

            # First two failures
            for _ in range(2):
                result = await run(input_data.copy())
                assert result["files"][0]["status"] == "failed"

            # Third call should trigger circuit breaker
            result = await run(input_data.copy())
            file_result = result["files"][0]
            assert file_result["status"] == "failed"
            assert "Frankfurter API is down" in file_result["error"]

    @pytest.mark.asyncio
    async def test_circuit_breaker_recovery(self):
        """Test that circuit breaker resets after successful call."""
        from app.currency import get_failure_count

        # Ensure clean state at start of test
        await reset_circuit_breaker()

        # Use different dates to avoid mock URL conflicts
        fail_input = {
            "job_id": "test-job-123",
            "files": [{"id": "file-1", "invoice_date": "2025-07-01", "src_currency": "USD", "invoice_total": 100.00}],
        }

        success_input = {
            "job_id": "test-job-123",
            "files": [
                {
                    "id": "file-1",
                    "invoice_date": "2025-07-02",  # Different date
                    "src_currency": "USD",
                    "invoice_total": 100.00,
                }
            ],
        }

        # Verify circuit breaker starts clean
        assert await get_failure_count() == 0

        # First call fails
        with respx.mock(assert_all_called=False) as respx_mock:
            respx_mock.get("https://api.frankfurter.app/2025-07-01?from=USD&to=ILS").mock(
                return_value=httpx.Response(500, text="Internal Server Error")
            )

            result = await run(fail_input)
            assert result["files"][0]["status"] == "failed"

        # Check that failure was recorded
        failure_count = await get_failure_count()
        assert failure_count == 1

        # Second call succeeds - should reset circuit breaker
        with respx.mock(assert_all_called=False) as respx_mock:
            respx_mock.get("https://api.frankfurter.app/2025-07-02?from=USD&to=ILS").mock(
                return_value=httpx.Response(
                    200, json={"amount": 1.0, "base": "USD", "date": "2025-07-02", "rates": {"ILS": 3.65}}
                )
            )

            result = await run(success_input)
            file_result = result["files"][0]
            assert file_result.get("status") != "failed"
            assert file_result["exchange_rate"] == 3.65

        # Verify circuit breaker was reset to 0 on success
        assert await get_failure_count() == 0

    @pytest.mark.asyncio
    async def test_multiple_files_mixed_results(self):
        """Test processing multiple files with mixed success/failure results."""
        # Ensure clean state at start of test
        await reset_circuit_breaker()

        input_data = {
            "job_id": "test-job-123",
            "files": [
                {"id": "file-1", "invoice_date": "2025-07-01", "src_currency": "USD", "invoice_total": 100.00},
                {
                    "id": "file-2",
                    # Missing invoice_date - should fail
                    "src_currency": "EUR",
                    "invoice_total": 85.00,
                },
                {"id": "file-3", "invoice_date": "2025-07-01", "src_currency": "GBP", "invoice_total": 75.00},
            ],
        }

        with respx.mock(assert_all_called=False) as respx_mock:
            # USD conversion succeeds
            respx_mock.get("https://api.frankfurter.app/2025-07-01?from=USD&to=ILS").mock(
                return_value=httpx.Response(
                    200, json={"amount": 1.0, "base": "USD", "date": "2025-07-01", "rates": {"ILS": 3.65}}
                )
            )

            # GBP conversion succeeds
            respx_mock.get("https://api.frankfurter.app/2025-07-01?from=GBP&to=ILS").mock(
                return_value=httpx.Response(
                    200, json={"amount": 1.0, "base": "GBP", "date": "2025-07-01", "rates": {"ILS": 4.5}}
                )
            )

            result = await run(input_data)

            # File 1: Success
            file1 = result["files"][0]
            assert file1.get("status") != "failed"
            assert file1["exchange_rate"] == 3.65
            assert file1["converted_total"] == 365.00

            # File 2: Failed due to missing date
            file2 = result["files"][1]
            assert file2["status"] == "failed"
            assert "Missing required fields" in file2["error"]

            # File 3: Success
            file3 = result["files"][2]
            assert file3.get("status") != "failed"
            assert file3["exchange_rate"] == 4.5
            assert file3["converted_total"] == 337.50

    @pytest.mark.asyncio
    async def test_precision_and_rounding(self):
        """Test precise decimal calculations and ROUND_HALF_UP rounding."""
        # Ensure clean state at start of test
        await reset_circuit_breaker()

        input_data = {
            "job_id": "test-job-123",
            "files": [
                {
                    "id": "file-1",
                    "invoice_date": "2025-07-01",
                    "src_currency": "USD",
                    "invoice_total": 123.456,  # Will test precision
                }
            ],
        }

        # Rate that will produce fractional result requiring rounding
        expected_response = {"amount": 1.0, "base": "USD", "date": "2025-07-01", "rates": {"ILS": 3.333}}

        with respx.mock(assert_all_called=False) as respx_mock:
            respx_mock.get("https://api.frankfurter.app/2025-07-01?from=USD&to=ILS").mock(
                return_value=httpx.Response(200, json=expected_response)
            )

            result = await run(input_data)

            file_result = result["files"][0]
            # Rate 3.333 gets rounded to 3.33 by currency service (ROUND_HALF_UP)
            # Then 123.456 * 3.33 = 411.10848, which rounds to 411.11 (ROUND_HALF_UP)
            rounded_rate = Decimal("3.333").quantize(Decimal("0.01"), rounding=ROUND_HALF_UP)
            expected_converted = (Decimal("123.456") * rounded_rate).quantize(Decimal("0.01"), rounding=ROUND_HALF_UP)
            assert file_result["converted_total"] == float(expected_converted)
            assert file_result["exchange_rate"] == float(rounded_rate)
</file>

<file path="tests/test_currency.py">
"""Tests for the currency module."""

from decimal import Decimal

import httpx
import pytest
import respx

from app.currency import FrankfurterDown, _normalize_date, get_failure_count, get_rate, reset_circuit_breaker


class TestGetRate:
    """Test cases for the get_rate function."""

    @pytest.fixture(autouse=True, scope="function")
    async def setup(self):
        """Reset circuit breaker before each test."""
        await reset_circuit_breaker()
        yield
        await reset_circuit_breaker()

    @pytest.mark.asyncio
    async def test_happy_path(self):
        """Test successful exchange rate retrieval with mocked response 1.2."""
        expected_response = {"amount": 1.0, "base": "USD", "date": "2025-07-10", "rates": {"EUR": 1.2}}

        with respx.mock:
            respx.get("https://api.frankfurter.app/2025-07-10?from=USD&to=EUR").mock(
                return_value=httpx.Response(200, json=expected_response)
            )

            result = await get_rate("2025-07-10", "USD", "EUR")

            assert result == Decimal("1.20")  # Should be rounded to 2 decimal places
            assert await get_failure_count() == 0

    @pytest.mark.asyncio
    async def test_currency_normalization(self):
        """Test that currency codes are normalized to uppercase."""
        expected_response = {"amount": 1.0, "base": "USD", "date": "2025-07-10", "rates": {"EUR": 1.5}}

        with respx.mock:
            respx.get("https://api.frankfurter.app/2025-07-10?from=USD&to=EUR").mock(
                return_value=httpx.Response(200, json=expected_response)
            )

            result = await get_rate("2025-07-10", "usd", "eur")

            assert result == Decimal("1.5")

    @pytest.mark.asyncio
    async def test_date_formats(self):
        """Test various date format inputs."""
        expected_response = {"amount": 1.0, "base": "USD", "date": "2025-07-10", "rates": {"EUR": 1.2}}

        date_formats = ["2025-07-10", "07/10/2025", "10 Jul 2025"]

        for date_format in date_formats:
            with respx.mock:
                respx.get("https://api.frankfurter.app/2025-07-10?from=USD&to=EUR").mock(
                    return_value=httpx.Response(200, json=expected_response)
                )

                result = await get_rate(date_format, "USD", "EUR")
                assert result == Decimal("1.20")  # Should be rounded to 2 decimal places

    @pytest.mark.asyncio
    async def test_failure_increments_counter(self):
        """Test that API failures increment the failure counter."""
        with respx.mock:
            respx.get("https://api.frankfurter.app/2025-07-10?from=USD&to=EUR").mock(
                return_value=httpx.Response(500, text="Internal Server Error")
            )

            with pytest.raises(Exception, match="Failed to fetch exchange rate"):
                await get_rate("2025-07-10", "USD", "EUR")

            assert await get_failure_count() == 1

    @pytest.mark.asyncio
    async def test_timeout_increments_counter(self):
        """Test that timeouts increment the failure counter."""
        # Ensure clean state
        await reset_circuit_breaker()

        with respx.mock:
            respx.get("https://api.frankfurter.app/2025-07-10?from=USD&to=EUR").mock(
                side_effect=httpx.TimeoutException("Timeout")
            )

            with pytest.raises(Exception, match="Request timeout"):
                await get_rate("2025-07-10", "USD", "EUR")

            assert await get_failure_count() == 1

    @pytest.mark.asyncio
    async def test_network_error_increments_counter(self):
        """Test that network errors increment the failure counter."""
        # Ensure clean state
        await reset_circuit_breaker()

        with respx.mock:
            respx.get("https://api.frankfurter.app/2025-07-10?from=USD&to=EUR").mock(
                side_effect=httpx.ConnectError("Connection failed")
            )

            with pytest.raises(Exception, match="Network error"):
                await get_rate("2025-07-10", "USD", "EUR")

            assert await get_failure_count() == 1

    @pytest.mark.asyncio
    async def test_third_failure_raises_frankfurter_down(self):
        """Test that the third consecutive failure raises FrankfurterDown."""
        # Ensure clean state
        await reset_circuit_breaker()

        # First two failures
        for i in range(2):
            with respx.mock:
                respx.get("https://api.frankfurter.app/2025-07-10?from=USD&to=EUR").mock(
                    return_value=httpx.Response(500, text="Internal Server Error")
                )

                with pytest.raises(Exception, match="Failed to fetch exchange rate"):
                    await get_rate("2025-07-10", "USD", "EUR")

                assert await get_failure_count() == i + 1

        # Third failure should raise FrankfurterDown without making a request
        with pytest.raises(FrankfurterDown, match="Frankfurter API is down after 3 consecutive failures"):
            await get_rate("2025-07-10", "USD", "EUR")

        assert await get_failure_count() == 2  # Should stay at 2 since circuit breaker prevented the 3rd request

    @pytest.mark.asyncio
    async def test_success_resets_failure_count(self):
        """Test that successful requests reset the failure counter."""
        # Ensure clean state
        await reset_circuit_breaker()

        # First failure
        with respx.mock:
            respx.get("https://api.frankfurter.app/2025-07-10?from=USD&to=EUR").mock(
                return_value=httpx.Response(500, text="Internal Server Error")
            )

            with pytest.raises(Exception, match="Failed to fetch exchange rate"):
                await get_rate("2025-07-10", "USD", "EUR")

            assert await get_failure_count() == 1

        # Successful request should reset counter
        expected_response = {"amount": 1.0, "base": "USD", "date": "2025-07-10", "rates": {"EUR": 1.2}}

        with respx.mock:
            respx.get("https://api.frankfurter.app/2025-07-10?from=USD&to=EUR").mock(
                return_value=httpx.Response(200, json=expected_response)
            )

            result = await get_rate("2025-07-10", "USD", "EUR")

            assert result == Decimal("1.20")  # Should be rounded to 2 decimal places
            assert await get_failure_count() == 0

    @pytest.mark.asyncio
    async def test_invalid_json_response(self):
        """Test handling of invalid JSON responses."""
        # Ensure clean state
        await reset_circuit_breaker()

        with respx.mock:
            respx.get("https://api.frankfurter.app/2025-07-10?from=USD&to=EUR").mock(
                return_value=httpx.Response(200, text="Not JSON")
            )

            with pytest.raises(Exception, match="Invalid JSON response"):
                await get_rate("2025-07-10", "USD", "EUR")

            assert await get_failure_count() == 1

    @pytest.mark.asyncio
    async def test_missing_currency_in_response(self):
        """Test handling when target currency is missing from response."""
        # Ensure clean state
        await reset_circuit_breaker()

        incomplete_response = {"amount": 1.0, "base": "USD", "date": "2025-07-10", "rates": {}}

        with respx.mock:
            respx.get("https://api.frankfurter.app/2025-07-10?from=USD&to=EUR").mock(
                return_value=httpx.Response(200, json=incomplete_response)
            )

            with pytest.raises(Exception, match="Currency EUR not found in response"):
                await get_rate("2025-07-10", "USD", "EUR")

            assert await get_failure_count() == 1

    @pytest.mark.asyncio
    async def test_invalid_date_format(self):
        """Test that invalid dates raise ValueError."""
        # Ensure clean state
        await reset_circuit_breaker()

        with pytest.raises(ValueError, match="Invalid date format"):
            # This should fail in date normalization before making any request
            await get_rate("invalid-date-32-13-2025", "USD", "EUR")


class TestDateNormalization:
    """Test cases for date normalization helper function."""

    def test_normalize_date_formats(self):
        """Test various date format normalization."""
        assert _normalize_date("2025-07-10") == "2025-07-10"
        assert _normalize_date("07/10/2025") == "2025-07-10"
        assert _normalize_date("July 10, 2025") == "2025-07-10"
        assert _normalize_date("10 Jul 2025") == "2025-07-10"

    def test_normalize_date_invalid(self):
        """Test that invalid dates raise ValueError."""
        with pytest.raises(ValueError, match="Invalid date format"):
            _normalize_date("invalid-date")

        with pytest.raises(ValueError, match="Invalid date format"):
            _normalize_date("32-13-2025")


class TestCircuitBreakerHelpers:
    """Test cases for circuit breaker helper functions."""

    @pytest.mark.asyncio
    async def test_reset_circuit_breaker(self):
        """Test circuit breaker reset functionality."""
        # Reset circuit breaker and verify it's at 0
        await reset_circuit_breaker()
        assert await get_failure_count() == 0

        # Simulate some failures by manually calling record_failure
        from app.currency import _circuit_breaker

        await _circuit_breaker.record_failure()
        await _circuit_breaker.record_failure()

        assert await get_failure_count() == 2

        # Reset and verify it's back to 0
        await reset_circuit_breaker()
        assert await get_failure_count() == 0

    @pytest.mark.asyncio
    async def test_get_failure_count(self):
        """Test failure count getter."""
        await reset_circuit_breaker()
        assert await get_failure_count() == 0

        # Manually increment count to test getter
        from app.currency import _circuit_breaker

        await _circuit_breaker.record_failure()
        await _circuit_breaker.record_failure()
        await _circuit_breaker.record_failure()

        assert await get_failure_count() == 3
</file>

<file path="tests/test_db_models.py">
"""Tests for database models."""

import os
import tempfile
from datetime import datetime

import pytest
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker

from app.models import Base, File, Job


@pytest.fixture
def temp_db_session():
    """Create a temporary database session for testing."""
    # Create a temporary file for the SQLite database
    db_fd, db_path = tempfile.mkstemp(suffix=".db")
    os.close(db_fd)

    try:
        # Create engine and session
        engine = create_engine(f"sqlite:///{db_path}", connect_args={"check_same_thread": False})
        Base.metadata.create_all(bind=engine)

        SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)
        session = SessionLocal()

        yield session

        session.close()
    finally:
        # Clean up the temporary database file
        if os.path.exists(db_path):
            os.unlink(db_path)


def test_job_creation(temp_db_session):
    """Test creating and querying a Job."""
    # Create a job
    job = Job(job_id="test-job-123", status="pending", processed=0, total=5)

    temp_db_session.add(job)
    temp_db_session.commit()

    # Query back the job
    retrieved_job = temp_db_session.query(Job).filter(Job.job_id == "test-job-123").first()

    assert retrieved_job is not None
    assert retrieved_job.job_id == "test-job-123"
    assert retrieved_job.status == "pending"
    assert retrieved_job.processed == 0
    assert retrieved_job.total == 5
    assert isinstance(retrieved_job.created_at, datetime)
    assert isinstance(retrieved_job.updated_at, datetime)


def test_file_creation(temp_db_session):
    """Test creating and querying a File."""
    # Create a job first
    job = Job(job_id="test-job-456", status="processing", processed=1, total=3)
    temp_db_session.add(job)
    temp_db_session.commit()

    # Create a file associated with the job
    file = File(
        job_id="test-job-456",
        filename="invoice1.pdf",
        status="completed",
        original_currency="USD",
        target_currency="EUR",
    )

    temp_db_session.add(file)
    temp_db_session.commit()

    # Query back the file
    retrieved_file = temp_db_session.query(File).filter(File.filename == "invoice1.pdf").first()

    assert retrieved_file is not None
    assert retrieved_file.job_id == "test-job-456"
    assert retrieved_file.filename == "invoice1.pdf"
    assert retrieved_file.status == "completed"
    assert retrieved_file.original_currency == "USD"
    assert retrieved_file.target_currency == "EUR"
    assert retrieved_file.error_message is None


def test_job_file_relationship(temp_db_session):
    """Test the relationship between Job and File models."""
    # Create a job
    job = Job(job_id="test-job-789", status="completed", processed=2, total=2)
    temp_db_session.add(job)
    temp_db_session.commit()

    # Create files associated with the job
    file1 = File(
        job_id="test-job-789",
        filename="invoice1.pdf",
        status="completed",
        original_currency="USD",
        target_currency="EUR",
    )

    file2 = File(
        job_id="test-job-789",
        filename="invoice2.pdf",
        status="failed",
        original_currency="GBP",
        target_currency="EUR",
        error_message="Invalid PDF format",
    )

    temp_db_session.add_all([file1, file2])
    temp_db_session.commit()

    # Query job and check relationship
    retrieved_job = temp_db_session.query(Job).filter(Job.job_id == "test-job-789").first()
    assert len(retrieved_job.files) == 2

    # Check file relationship back to job
    retrieved_file = temp_db_session.query(File).filter(File.filename == "invoice1.pdf").first()
    assert retrieved_file.job.job_id == "test-job-789"
    assert retrieved_file.job.status == "completed"
</file>

<file path="tests/test_excel_node.py">
"""Tests for the excel node."""

from io import BytesIO

import pytest
from openpyxl import load_workbook

from app.azure_adapter import DefaultContent, InvoiceData, InvoiceTotal, ValueCurrency
from langgraph_nodes import excel


class TestExcelNode:
    """Test cases for the excel node."""

    def test_module_imports(self):
        """Test that the excel module imports successfully."""
        assert excel is not None

    def test_invoice_suffix_helper(self):
        """Test the invoice_suffix helper function per spec."""
        # Test with digits - take last 4, left-pad zeros
        mock_invoice = InvoiceData(
            InvoiceId=DefaultContent(content="INV-123", confidence=0.95),
            InvoiceDate=None,
            VendorName=None,
            VendorAddressRecipient=None,
            InvoiceTotal=None,
        )
        assert excel.invoice_suffix(mock_invoice) == "0123"

        mock_invoice.InvoiceId.content = "456789"
        assert excel.invoice_suffix(mock_invoice) == "6789"

        mock_invoice.InvoiceId.content = "INV-2024-001"
        assert excel.invoice_suffix(mock_invoice) == "4001"

        # Test with no digits
        mock_invoice.InvoiceId.content = "INV-NO-DIGITS"
        assert excel.invoice_suffix(mock_invoice) == "SUFFIX_NOT_FOUND"

        # Test empty/None InvoiceId
        mock_invoice.InvoiceId.content = ""
        assert excel.invoice_suffix(mock_invoice) == "SUFFIX_NOT_FOUND"

        mock_invoice.InvoiceId = None
        assert excel.invoice_suffix(mock_invoice) == "SUFFIX_NOT_FOUND"

    @pytest.mark.asyncio
    async def test_run_with_empty_invoices(self):
        """Test run function with empty invoice list."""
        result = await excel.run({"target_currency": "USD"})

        # Check return structure
        assert isinstance(result, dict)
        assert "xlsx" in result
        assert "row_count" in result
        assert isinstance(result["xlsx"], bytes)
        assert result["row_count"] == 1  # One ERROR row

        # Validate Excel content
        excel_data = BytesIO(result["xlsx"])
        wb = load_workbook(excel_data)
        ws = wb.active

        # Check headers per spec
        expected_headers = [
            "Date (DD/MM/YYYY)",
            "Invoice Suffix",
            "USD Total Price",
            "Foreign Currency Total Price",
            "Foreign Currency Code",
            "Exchange Rate (4 dp)",
            "Vendor Name",
        ]
        for col, expected_header in enumerate(expected_headers, 1):
            assert ws.cell(row=1, column=col).value == expected_header

        # Check ERROR row per spec
        expected_error_row = ["ERROR", "", "N/A", "N/A", "N/A", "N/A", "N/A"]
        for col, expected_value in enumerate(expected_error_row, 1):
            cell_value = ws.cell(row=2, column=col).value
            # Empty string becomes None in Excel cells
            if expected_value == "" and cell_value is None:
                continue
            assert cell_value == expected_value

    @pytest.mark.asyncio
    async def test_run_with_complete_invoice(self):
        """Test run function with a complete invoice."""
        # Create test invoice data
        invoice = InvoiceData(
            InvoiceId=DefaultContent(content="INV-001", confidence=0.95),
            InvoiceDate=DefaultContent(content="2024-01-15", confidence=0.95),
            VendorName=DefaultContent(content="Acme Corp", confidence=0.95),
            VendorAddressRecipient=DefaultContent(content="123 Business St", confidence=0.95),
            InvoiceTotal=InvoiceTotal(
                value_currency=ValueCurrency(amount=1234.56, currency_code="EUR"), content="1,234.56", confidence=0.95
            ),
        )

        # Add filename attribute for testing
        invoice._filename = "test_invoice_001.pdf"

        result = await excel.run({"invoices": [invoice], "target_currency": "USD"})

        # Check return structure
        assert isinstance(result, dict)
        assert "xlsx" in result
        assert "row_count" in result
        assert isinstance(result["xlsx"], bytes)
        assert result["row_count"] == 1

        # Validate Excel content
        excel_data = BytesIO(result["xlsx"])
        wb = load_workbook(excel_data)
        ws = wb.active

        # Check data row per spec (different currency: EUR vs USD)
        assert ws.cell(row=2, column=1).value == "2024-01-15"  # Date
        assert ws.cell(row=2, column=2).value == "0001"  # Invoice Suffix (from InvoiceId)
        assert ws.cell(row=2, column=3).value == "N/A"  # USD Total Price (not converted yet)
        assert ws.cell(row=2, column=4).value == 1234.56  # Foreign Currency Total Price
        assert ws.cell(row=2, column=5).value == "EUR"  # Foreign Currency Code
        assert ws.cell(row=2, column=6).value == "N/A"  # Exchange Rate (not converted yet)
        assert ws.cell(row=2, column=7).value == "Acme Corp"  # Vendor Name

    @pytest.mark.asyncio
    async def test_run_with_partial_invoice(self):
        """Test run function with partial invoice data (missing fields)."""
        # Create invoice with missing fields
        invoice = InvoiceData(
            InvoiceId=DefaultContent(content="INV-002", confidence=0.95),
            InvoiceDate=None,  # Missing date
            VendorName=DefaultContent(content="Test Vendor", confidence=0.95),
            VendorAddressRecipient=None,  # Missing address
            InvoiceTotal=None,  # Missing total
        )

        # Add filename with no digits
        invoice._filename = "test_invoice.pdf"

        result = await excel.run({"invoices": [invoice], "target_currency": "GBP"})

        # Validate Excel content
        excel_data = BytesIO(result["xlsx"])
        wb = load_workbook(excel_data)
        ws = wb.active

        # Check data row with ERROR/N/A placeholders per spec
        assert ws.cell(row=2, column=1).value == "ERROR"  # Date (missing)
        assert ws.cell(row=2, column=2).value == "0002"  # Invoice Suffix (from InvoiceId)
        assert ws.cell(row=2, column=3).value == "N/A"  # GBP Total Price (missing)
        assert ws.cell(row=2, column=4).value in ["", None]  # Foreign Currency Total Price (missing)
        assert ws.cell(row=2, column=5).value in ["", None]  # Foreign Currency Code (missing)
        assert ws.cell(row=2, column=6).value in ["", None]  # Exchange Rate (missing)
        assert ws.cell(row=2, column=7).value == "Test Vendor"  # Vendor Name

    @pytest.mark.asyncio
    async def test_run_with_multiple_invoices(self):
        """Test run function with multiple invoices."""
        # Create multiple test invoices
        invoice1 = InvoiceData(
            InvoiceId=DefaultContent(content="INV-001", confidence=0.95),
            InvoiceDate=DefaultContent(content="2024-01-15", confidence=0.95),
            VendorName=DefaultContent(content="Vendor A", confidence=0.95),
            VendorAddressRecipient=DefaultContent(content="Address A", confidence=0.95),
            InvoiceTotal=InvoiceTotal(
                value_currency=ValueCurrency(amount=100.0, currency_code="USD"), content="100.00", confidence=0.95
            ),
        )
        invoice1._filename = "invoice_123.pdf"

        invoice2 = InvoiceData(
            InvoiceId=DefaultContent(content="INV-002", confidence=0.95),
            InvoiceDate=DefaultContent(content="2024-01-16", confidence=0.95),
            VendorName=DefaultContent(content="Vendor B", confidence=0.95),
            VendorAddressRecipient=DefaultContent(content="Address B", confidence=0.95),
            InvoiceTotal=InvoiceTotal(
                value_currency=ValueCurrency(amount=200.0, currency_code="EUR"), content="200.00", confidence=0.95
            ),
        )
        invoice2._filename = "invoice_456.pdf"

        result = await excel.run({"invoices": [invoice1, invoice2], "target_currency": "CAD"})

        # Check return structure
        assert result["row_count"] == 2

        # Validate Excel content
        excel_data = BytesIO(result["xlsx"])
        wb = load_workbook(excel_data)
        ws = wb.active

        # Check first invoice
        assert ws.cell(row=2, column=1).value == "2024-01-15"  # Date
        assert ws.cell(row=2, column=2).value == "0001"  # Invoice Suffix
        assert ws.cell(row=2, column=5).value == "USD"  # Foreign Currency Code
        assert ws.cell(row=2, column=7).value == "Vendor A"  # Vendor Name

        # Check second invoice
        assert ws.cell(row=3, column=1).value == "2024-01-16"  # Date
        assert ws.cell(row=3, column=2).value == "0002"  # Invoice Suffix
        assert ws.cell(row=3, column=5).value == "EUR"  # Foreign Currency Code
        assert ws.cell(row=3, column=7).value == "Vendor B"  # Vendor Name

    @pytest.mark.asyncio
    async def test_excel_formatting(self):
        """Test that Excel file has proper formatting."""
        invoice = InvoiceData(
            InvoiceId=DefaultContent(content="INV-001", confidence=0.95),
            InvoiceDate=DefaultContent(content="2024-01-15", confidence=0.95),
            VendorName=DefaultContent(content="Test Vendor", confidence=0.95),
            VendorAddressRecipient=DefaultContent(content="Test Address", confidence=0.95),
            InvoiceTotal=InvoiceTotal(
                value_currency=ValueCurrency(amount=100.0, currency_code="USD"), content="100.00", confidence=0.95
            ),
        )
        invoice._filename = "test_123.pdf"

        result = await excel.run({"invoices": [invoice], "target_currency": "EUR"})

        # Validate Excel formatting
        excel_data = BytesIO(result["xlsx"])
        wb = load_workbook(excel_data)
        ws = wb.active

        # Check that worksheet has correct title per spec
        assert ws.title == "Invoices Report"

        # Check header formatting (basic checks)
        header_cell = ws.cell(row=1, column=1)
        assert header_cell.value == "Date (DD/MM/YYYY)"
        assert header_cell.font.bold is True
        assert header_cell.font.color.rgb == "00FFFFFF"
</file>

<file path="tests/test_health.py">
"""Tests for the health endpoint."""

from fastapi.testclient import TestClient

from app.main import app

client = TestClient(app)


def test_health_endpoint():
    """Test that health endpoint returns 200 and correct JSON."""
    response = client.get("/health")
    assert response.status_code == 200
    assert response.json() == {"status": "ok"}
</file>

<file path="tests/test_integration.py">
"""Integration tests for the full invoice processing pipeline."""

from unittest.mock import patch

import pytest
from fastapi.testclient import TestClient
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker

from app.db import get_db
from app.main import app
from app.models import Base


# Override the database for testing
@pytest.fixture
def test_db():
    """Create a test database."""
    # Use file-based SQLite for better thread safety
    engine = create_engine("sqlite:///test.db", connect_args={"check_same_thread": False})
    Base.metadata.drop_all(bind=engine)
    Base.metadata.create_all(bind=engine)
    TestingSessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)

    def override_get_db():
        db = TestingSessionLocal()
        try:
            yield db
        finally:
            db.close()

    app.dependency_overrides[get_db] = override_get_db
    yield TestingSessionLocal
    app.dependency_overrides.clear()

    # Clean up test database
    import os

    if os.path.exists("test.db"):
        os.remove("test.db")


@pytest.fixture
def client(test_db):
    """Create a test client."""
    with TestClient(app) as c:
        yield c


@pytest.fixture
def mock_azure_extract():
    """Mock Azure Document Intelligence extraction."""
    from app.azure_adapter import DefaultContent, InvoiceData, InvoiceTotal, ValueCurrency

    # Create mock invoice data for three different invoices
    mock_invoices = [
        InvoiceData(
            InvoiceId=DefaultContent(content="INV-001", confidence=0.95),
            InvoiceDate=DefaultContent(content="2024-01-15", confidence=0.92),
            VendorName=DefaultContent(content="Acme Corp", confidence=0.88),
            VendorAddressRecipient=DefaultContent(content="123 Business St", confidence=0.85),
            InvoiceTotal=InvoiceTotal(
                value_currency=ValueCurrency(amount=1000.0, currency_code="EUR"), content="1,000.00", confidence=0.90
            ),
        ),
        InvoiceData(
            InvoiceId=DefaultContent(content="INV-002", confidence=0.93),
            InvoiceDate=DefaultContent(content="2024-01-16", confidence=0.91),
            VendorName=DefaultContent(content="Tech Solutions", confidence=0.87),
            VendorAddressRecipient=DefaultContent(content="456 Tech Ave", confidence=0.84),
            InvoiceTotal=InvoiceTotal(
                value_currency=ValueCurrency(amount=500.0, currency_code="GBP"), content="£500.00", confidence=0.89
            ),
        ),
        InvoiceData(
            InvoiceId=DefaultContent(content="INV-003", confidence=0.94),
            InvoiceDate=DefaultContent(content="2024-01-17", confidence=0.90),
            VendorName=DefaultContent(content="Service Provider", confidence=0.86),
            VendorAddressRecipient=DefaultContent(content="789 Service Blvd", confidence=0.83),
            InvoiceTotal=InvoiceTotal(
                value_currency=ValueCurrency(amount=750.0, currency_code="USD"), content="750.00", confidence=0.91
            ),
        ),
    ]

    async def mock_extract(file_path):
        # Return different mock data based on filename
        if "invoice1" in file_path:
            return mock_invoices[0]
        elif "invoice2" in file_path:
            return mock_invoices[1]
        elif "invoice3" in file_path:
            return mock_invoices[2]
        else:
            return None

    with patch("app.azure_adapter.extract_invoice", side_effect=mock_extract):
        yield mock_extract


@pytest.fixture
def mock_currency_rate():
    """Mock currency rate API."""
    from decimal import Decimal

    async def mock_get_rate(date, from_currency, to_currency):
        # Return mock exchange rates
        rates = {
            ("EUR", "USD"): Decimal("1.1000"),
            ("GBP", "USD"): Decimal("1.2500"),
            ("USD", "USD"): Decimal("1.0000"),
        }
        return rates.get((from_currency, to_currency), Decimal("1.0000"))

    with patch("app.currency.get_rate", side_effect=mock_get_rate):
        yield mock_get_rate


def create_tiny_pdf():
    """Create a tiny PDF file for testing."""
    # This is a minimal PDF file structure
    pdf_content = b"""%PDF-1.4
1 0 obj
<<
/Type /Catalog
/Pages 2 0 R
>>
endobj
2 0 obj
<<
/Type /Pages
/Kids [3 0 R]
/Count 1
>>
endobj
3 0 obj
<<
/Type /Page
/Parent 2 0 R
/MediaBox [0 0 612 792]
/Contents 4 0 R
>>
endobj
4 0 obj
<<
/Length 44
>>
stream
BT
/F1 12 Tf
100 700 Td
(Test Invoice) Tj
ET
endstream
endobj
xref
0 5
0000000000 65535 f
0000000009 00000 n
0000000058 00000 n
0000000115 00000 n
0000000201 00000 n
trailer
<<
/Size 5
/Root 1 0 R
>>
startxref
295
%%EOF"""
    return pdf_content


def test_full_pipeline_integration(client, test_db, mock_azure_extract, mock_currency_rate):
    """Test the full invoice processing pipeline with three tiny invoices."""

    # Create three tiny PDF files
    pdf_data = create_tiny_pdf()

    files = [
        ("files", ("invoice1.pdf", pdf_data, "application/pdf")),
        ("files", ("invoice2.pdf", pdf_data, "application/pdf")),
        ("files", ("invoice3.pdf", pdf_data, "application/pdf")),
    ]

    # Submit files for processing
    response = client.post("/process-invoices", files=files, data={"target_currency": "USD"})

    assert response.status_code == 200
    job_data = response.json()
    assert "job_id" in job_data
    job_id = job_data["job_id"]

    # Test that job was created successfully
    assert len(job_id) > 0

    # The background pipeline task will run with mocks, but we don't wait for it
    # This tests the full API contract without hanging


def test_pipeline_basic_flow(client, test_db, mock_azure_extract, mock_currency_rate):
    """Test basic pipeline flow without SSE streaming."""

    # Create test files
    pdf_data = create_tiny_pdf()

    files = [("files", ("test_invoice_123.pdf", pdf_data, "application/pdf"))]

    # Submit for processing
    response = client.post("/process-invoices", files=files, data={"target_currency": "USD"})

    assert response.status_code == 200
    job_data = response.json()
    job_id = job_data["job_id"]

    # Just test that we get a valid job ID
    assert job_id is not None
    assert len(job_id) > 0


def test_error_handling(client):
    """Test error handling in the pipeline."""

    # Test with oversized file
    large_content = b"x" * (2 * 1024 * 1024)  # 2MB file

    files = [("files", ("large_file.pdf", large_content, "application/pdf"))]

    response = client.post("/process-invoices", files=files, data={"target_currency": "USD"})

    assert response.status_code == 400
    assert "exceeds 1MB limit" in response.json()["detail"]


def test_too_many_files(client):
    """Test handling of too many files."""

    pdf_data = create_tiny_pdf()

    # Create 101 files (exceeds limit of 100)
    files = [("files", (f"invoice_{i}.pdf", pdf_data, "application/pdf")) for i in range(101)]

    response = client.post("/process-invoices", files=files, data={"target_currency": "USD"})

    assert response.status_code == 400
    assert "Maximum 100 files allowed" in response.json()["detail"]


def test_download_nonexistent_job(client):
    """Test downloading report for non-existent job."""

    response = client.get("/download/non-existent-job-id")
    assert response.status_code == 404
    assert "Report not found" in response.json()["detail"]
</file>

<file path="tests/test_pipeline.py">
"""Tests for LangGraph pipeline."""

import pytest

from langgraph_nodes.pipeline import get_compiled_pipeline


@pytest.mark.asyncio
async def test_pipeline_execution():
    """Test that pipeline executes and produces Excel output."""
    # Sample input data
    sample_input = {
        "job_id": "test-job-123",
        "files": [
            {"filename": "invoice1.pdf", "status": "uploaded"},
            {"filename": "invoice2.pdf", "status": "uploaded"},
        ],
        "target_currency": "USD",
        "metadata": {"user_id": "user123", "timestamp": "2025-01-01T00:00:00Z"},
    }

    # Get compiled pipeline
    pipeline = get_compiled_pipeline()

    # Execute pipeline
    result = await pipeline.ainvoke(sample_input)

    # Assert the pipeline produces Excel output
    assert "xlsx" in result
    assert "row_count" in result
    assert isinstance(result["xlsx"], bytes)
    assert isinstance(result["row_count"], int)
    assert result["row_count"] >= 1  # At least one row (could be ERROR row)

    # Check if original input data is preserved (it may not be, which is okay)
    # The Excel node is the final output, so the result structure depends on its implementation
</file>

<file path="tests/test_placeholder.py">
"""Placeholder test to verify pytest configuration."""


def test_placeholder():
    """Basic test to ensure pytest is working."""
    assert 1 == 1
</file>

<file path="tests/test_sse_progress.py">
"""Tests for SSE progress endpoint."""


import pytest
from httpx import ASGITransport, AsyncClient

from app.main import app


@pytest.mark.skip(reason="SSE test can hang in CI - requires active progress queue")
@pytest.mark.asyncio
async def test_progress_endpoint_returns_sse_events():
    """Test that progress endpoint returns correct SSE events."""
    job_id = "test-job-123"

    async with AsyncClient(transport=ASGITransport(app=app), base_url="http://test") as client:
        async with client.stream("GET", f"/progress/{job_id}") as response:
            assert response.status_code == 200
            assert response.headers["content-type"] == "text/event-stream; charset=utf-8"
            assert response.headers["cache-control"] == "no-cache"


@pytest.mark.skip(reason="SSE endpoint hangs without active progress queue - skip in CI")
def test_progress_endpoint_basic():
    """Test basic progress endpoint response structure."""
    from fastapi.testclient import TestClient

    client = TestClient(app)
    job_id = "test-job-123"

    # This test hangs because SSE endpoint waits for progress queue
    # Skip in CI to prevent hanging
    with client.stream("GET", f"/progress/{job_id}") as response:
        assert response.status_code == 200
        assert "text/event-stream" in response.headers["content-type"]
        assert response.headers["cache-control"] == "no-cache"


def test_health_endpoint():
    """Test health endpoint as a simple API test."""
    from fastapi.testclient import TestClient

    client = TestClient(app)
    response = client.get("/health")

    assert response.status_code == 200
    assert response.json() == {"status": "ok"}
</file>

<file path=".gitignore">
# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
share/python-wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST

# PyInstaller
*.manifest
*.spec

# Installer logs
pip-log.txt
pip-delete-this-directory.txt

# Unit test / coverage reports
htmlcov/
.tox/
.nox/
.coverage
.coverage.*
.cache
nosetests.xml
coverage.xml
*.cover
*.py,cover
.hypothesis/
.pytest_cache/
cover/

# Translations
*.mo
*.pot

# Django stuff:
*.log
local_settings.py
db.sqlite3
db.sqlite3-journal

# Flask stuff:
instance/
.webassets-cache

# Scrapy stuff:
.scrapy

# Sphinx documentation
docs/_build/

# PyBuilder
.pybuilder/
target/

# Jupyter Notebook
.ipynb_checkpoints

# IPython
profile_default/
ipython_config.py

# pyenv
.python-version

# pipenv
Pipfile.lock

# pdm
.pdm.toml

# PEP 582
__pypackages__/

# Celery stuff
celerybeat-schedule
celerybeat.pid

# SageMath parsed files
*.sage.py

# Environments
.env
.venv
env/
venv/
ENV/
env.bak/
venv.bak/

# Spyder project settings
.spyderproject
.spyproject

# Rope project settings
.ropeproject

# mkdocs documentation
/site

# mypy
.mypy_cache/
.dmypy.json
dmypy.json

# Pyre type checker
.pyre/

# pytype static type analyzer
.pytype/

# Cython debug symbols
cython_debug/

# Ruff cache
.ruff_cache/
</file>

<file path="alembic.ini">
# A generic, single database configuration.

[alembic]
# path to migration scripts.
# this is typically a path given in POSIX (e.g. forward slashes)
# format, relative to the token %(here)s which refers to the location of this
# ini file
script_location = %(here)s/alembic

# template used to generate migration file names; The default value is %%(rev)s_%%(slug)s
# Uncomment the line below if you want the files to be prepended with date and time
# see https://alembic.sqlalchemy.org/en/latest/tutorial.html#editing-the-ini-file
# for all available tokens
# file_template = %%(year)d_%%(month).2d_%%(day).2d_%%(hour).2d%%(minute).2d-%%(rev)s_%%(slug)s

# sys.path path, will be prepended to sys.path if present.
# defaults to the current working directory.  for multiple paths, the path separator
# is defined by "path_separator" below.
prepend_sys_path = .


# timezone to use when rendering the date within the migration file
# as well as the filename.
# If specified, requires the python>=3.9 or backports.zoneinfo library and tzdata library.
# Any required deps can installed by adding `alembic[tz]` to the pip requirements
# string value is passed to ZoneInfo()
# leave blank for localtime
# timezone =

# max length of characters to apply to the "slug" field
# truncate_slug_length = 40

# set to 'true' to run the environment during
# the 'revision' command, regardless of autogenerate
# revision_environment = false

# set to 'true' to allow .pyc and .pyo files without
# a source .py file to be detected as revisions in the
# versions/ directory
# sourceless = false

# version location specification; This defaults
# to <script_location>/versions.  When using multiple version
# directories, initial revisions must be specified with --version-path.
# The path separator used here should be the separator specified by "path_separator"
# below.
# version_locations = %(here)s/bar:%(here)s/bat:%(here)s/alembic/versions

# path_separator; This indicates what character is used to split lists of file
# paths, including version_locations and prepend_sys_path within configparser
# files such as alembic.ini.
# The default rendered in new alembic.ini files is "os", which uses os.pathsep
# to provide os-dependent path splitting.
#
# Note that in order to support legacy alembic.ini files, this default does NOT
# take place if path_separator is not present in alembic.ini.  If this
# option is omitted entirely, fallback logic is as follows:
#
# 1. Parsing of the version_locations option falls back to using the legacy
#    "version_path_separator" key, which if absent then falls back to the legacy
#    behavior of splitting on spaces and/or commas.
# 2. Parsing of the prepend_sys_path option falls back to the legacy
#    behavior of splitting on spaces, commas, or colons.
#
# Valid values for path_separator are:
#
# path_separator = :
# path_separator = ;
# path_separator = space
# path_separator = newline
#
# Use os.pathsep. Default configuration used for new projects.
path_separator = os

# set to 'true' to search source files recursively
# in each "version_locations" directory
# new in Alembic version 1.10
# recursive_version_locations = false

# the output encoding used when revision files
# are written from script.py.mako
# output_encoding = utf-8

# database URL.  This is consumed by the user-maintained env.py script only.
# other means of configuring database URLs may be customized within the env.py
# file.
# sqlalchemy.url = driver://user:pass@localhost/dbname


[post_write_hooks]
# post_write_hooks defines scripts or Python functions that are run
# on newly generated revision scripts.  See the documentation for further
# detail and examples

# format using "black" - use the console_scripts runner, against the "black" entrypoint
# hooks = black
# black.type = console_scripts
# black.entrypoint = black
# black.options = -l 79 REVISION_SCRIPT_FILENAME

# lint with attempts to fix using "ruff" - use the module runner, against the "ruff" module
# hooks = ruff
# ruff.type = module
# ruff.module = ruff
# ruff.options = check --fix REVISION_SCRIPT_FILENAME

# Alternatively, use the exec runner to execute a binary found on your PATH
# hooks = ruff
# ruff.type = exec
# ruff.executable = ruff
# ruff.options = check --fix REVISION_SCRIPT_FILENAME

# Logging configuration.  This is also consumed by the user-maintained
# env.py script only.
[loggers]
keys = root,sqlalchemy,alembic

[handlers]
keys = console

[formatters]
keys = generic

[logger_root]
level = WARNING
handlers = console
qualname =

[logger_sqlalchemy]
level = WARNING
handlers =
qualname = sqlalchemy.engine

[logger_alembic]
level = INFO
handlers =
qualname = alembic

[handler_console]
class = StreamHandler
args = (sys.stderr,)
level = NOTSET
formatter = generic

[formatter_generic]
format = %(levelname)-5.5s [%(name)s] %(message)s
datefmt = %H:%M:%S
</file>

<file path="Dockerfile">
# Stage 1: Build frontend
FROM node:18-alpine AS frontend-builder

# Set working directory
WORKDIR /app

# Copy frontend package files
COPY frontend/package*.json ./
RUN npm ci

# Copy frontend source
COPY frontend/ ./

# Build frontend
RUN npm run build

# Stage 2: Python runtime
FROM python:3.11-slim

# Set working directory
WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Copy backend requirements
COPY backend/pyproject.toml backend/poetry.lock* backend/poetry.toml ./

# Install Poetry and dependencies
RUN pip install poetry && \
    poetry config virtualenvs.create false && \
    poetry install --without dev,proto --no-interaction --no-ansi --no-root

# Copy backend application code
COPY backend/app/ ./app/
COPY backend/langgraph_nodes/ ./langgraph_nodes/
COPY backend/alembic/ ./alembic/
COPY backend/alembic.ini ./

# Copy built frontend static files
COPY --from=frontend-builder /app/dist/ ./static/

# Create necessary directories
RUN mkdir -p uploads exports

# Expose port
EXPOSE 80

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:80/health || exit 1

# Run database migrations and start server
CMD ["sh", "-c", "alembic upgrade head && uvicorn app.main:app --host 0.0.0.0 --port 80"]
</file>

<file path="poetry.toml">
[virtualenvs]
in-project = true
</file>

<file path="pyproject.toml">
[build-system]
requires = ["poetry-core"]
build-backend = "poetry.core.masonry.api"

[tool.poetry]
name = "invoice-converter"
version = "0.1.0"
description = "Invoice processing and currency conversion web application"
authors = ["Invoice Converter Team <team@invoice-converter.com>"]
readme = "README.md"
packages = [{include = "app"}]

[tool.poetry.dependencies]
python = "^3.11"
fastapi = "^0.110.0"
uvicorn = {extras = ["standard"], version = "^0.30.0"}
alembic = "^1.13.0"
sqlalchemy = "^2.0.0"
langgraph = "^0.2.0"
azure-ai-documentintelligence = "~1.0.0"
python-dotenv = "^1.0.0"
httpx = "^0.27.0"
python-dateutil = "^2.8.2"
openpyxl = "^3.1.0"
python-multipart = "^0.0.6"
aiohttp = "^3.8.0"

[tool.poetry.group.proto.dependencies]
# Heavy dependencies for exploratory/proto code
requests = "^2.31.0"
python-dateutil = "^2.8.2"
langchain = "^0.2.0"

[tool.poetry.group.dev.dependencies]
pytest = "^7.4.3"
pytest-cov = "^4.1.0"
pytest-mock = "^3.12.0"
pytest-asyncio = "^0.21.0"
pytest-vcr = "^1.0.2"
httpx = "^0.27.0"
respx = "^0.21.0"
black = "^23.11.0"
ruff = "^0.1.6"
isort = "^5.12.0"
pre-commit = "^3.5.0"
debugpy = "^1.8.15"

[tool.black]
line-length = 120
target-version = ['py311']
include = '\.pyi?$'
extend-exclude = '''
/(
  # directories
  \.eggs
  | \.git
  | \.hg
  | \.mypy_cache
  | \.tox
  | \.venv
  | build
  | dist
)/
'''

[tool.ruff]
line-length = 120
target-version = "py311"

[tool.ruff.lint]
select = [
    "E",  # pycodestyle errors
    "W",  # pycodestyle warnings
    "F",  # pyflakes
    "I",  # isort
    "B",  # flake8-bugbear
    "C4", # flake8-comprehensions
    "UP", # pyupgrade
]
ignore = [
    "E501",  # line too long, handled by black
    "B008",  # do not perform function calls in argument defaults
    "C901",  # too complex
]

[tool.ruff.lint.per-file-ignores]
"__init__.py" = ["F401"]

[tool.isort]
profile = "black"
line_length = 120
</file>

<file path="README.md">
# Invoice Converter Backend

FastAPI backend for the Invoice Converter application.

## Features

- Invoice data extraction using Azure Document Intelligence
- Currency conversion via Frankfurter API
- LangGraph pipeline orchestration
- Real-time progress tracking with Server-Sent Events
- Excel report generation

## Development

```bash
# Install dependencies
poetry install

# Run tests
poetry run pytest

# Start development server
poetry run uvicorn app.main:app --reload
```
</file>

</files>
